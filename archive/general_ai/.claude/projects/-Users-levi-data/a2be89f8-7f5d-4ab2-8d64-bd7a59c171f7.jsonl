{"parentUuid":null,"isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","type":"system","subtype":"local_command","content":"<command-name>/model</command-name>\n            <command-message>model</command-message>\n            <command-args></command-args>","level":"info","timestamp":"2025-11-06T14:04:53.455Z","uuid":"1c17ab94-9994-4a44-8aa1-27c526e8fdb6","isMeta":false}
{"parentUuid":"1c17ab94-9994-4a44-8aa1-27c526e8fdb6","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","type":"system","subtype":"local_command","content":"<local-command-stdout>Kept model as \u001b[1mSonnet\u001b[22m</local-command-stdout>","level":"info","timestamp":"2025-11-06T14:04:53.455Z","uuid":"419783d8-b496-48cf-9a28-10d0ddbbf159","isMeta":false}
{"type":"file-history-snapshot","messageId":"b6acb37d-a1c1-4f53-a13b-05cd1c45c8f6","snapshot":{"messageId":"b6acb37d-a1c1-4f53-a13b-05cd1c45c8f6","trackedFileBackups":{},"timestamp":"2025-11-06T14:05:19.982Z"},"isSnapshotUpdate":false}
{"parentUuid":"419783d8-b496-48cf-9a28-10d0ddbbf159","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","type":"user","message":{"role":"user","content":"how does claude 4.5 sonnet and haiku compare to Qwen Vl 32B"},"uuid":"b6acb37d-a1c1-4f53-a13b-05cd1c45c8f6","timestamp":"2025-11-06T14:05:19.977Z","thinkingMetadata":{"level":"high","disabled":false,"triggers":[]}}
{"parentUuid":"b6acb37d-a1c1-4f53-a13b-05cd1c45c8f6","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Hcc1jQH4ynLFhrUAkR9XRo","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"The user is asking about how Claude 4.5 Sonnet and Haiku compare to Qwen VL 32B. This is a question about AI models and their capabilities.\n\nLet me think about what I know:\n- I am Claude Sonnet 4.5 (model ID: claude-sonnet-4-5-20250929) according to my context\n- There is also Claude Haiku mentioned as part of the Claude family\n- Qwen VL 32B is presumably a vision-language model from the Qwen series by Alibaba\n\nHowever, my knowledge cutoff is January 2025, and I should be careful about making direct comparisons without current information. The user might be asking about:\n1. Performance benchmarks\n2. Capabilities (vision, language, reasoning)\n3. Cost/speed tradeoffs\n4. Use cases\n\nSince this is asking about current model comparisons and capabilities, I should search for recent information to give an accurate answer. I should use web search to get current, accurate information about these models and their comparative performance.\n\nThis is not a coding task, so I don't need to use TodoWrite. I should just search for information and provide a helpful comparison.","signature":"EtoJCkYICRgCKkB0GFlqrztUmjAykVp7AURnK7QBtEnQ2q+oxka6KetSxkVjiweYZFCeNeQjUPOI46WPNPPbzYqXdU7EcvqHjOqiEgxgIvBsz0K8//nf3QoaDGhtpZXwKjPak8/JSyIwc5zY1GItFNIkw3y74cE6szivFiahA31zTJKeTm6wThBQnPtYyTO8QP5c9z9G2RzNKsEIDxTr9gmdt/NA0/p6DrnebNua4laW3ctp6fxrBeh+p2uwhm4GiQc2W0mcfr/Mu2TgV7P+DtncE6j7L5U3P8anXPP6qx3Z/YXKY+rnbWgRUlJzhTDHXGkTMxanqBYMRbVuhK59J5AkhvW5mbAzDdKeL/dV8Qnf+I3d+J0mSJ2BFEsdf0Erw2B/safJgAk4INbQMqV/SLpsp5nuK+PK271wl3GCNSwFYUX+QPIRN7/8GIJb1p2f4HWobuQhPqBcY2y8RDiVmnciDUgP1UtXiN3Gh1egAvqSPJoSX2/PGb6L0TfOe5pv5CV2pR7FcAN4hBwt6Xg86O/ren+PphkHC9ZsMAsGem8tJNFV4+8ThH56c4vf0zATCTNVit4u2qpK2Q4o7+rxEJazlrF9J1eOCFFab1G3oMFHCwrqdMuZ2iGzVK1YopCLgP9a2XASA9ea31+OorJyG/u0nVHY6rCkxEOdjGbPTzCYdq3NnBZm6shOdScf2+nT6oKoyDe1u9M8g+Cg2QqeAPi77i+8bBqdfFCK0c+PArvXR80zFz753qg9M8qXDz41wU5OIU2B3wFvD5QhezNJ7ve1sKD2Z18RV0RHreaRsIsIMxgA7Y7nQJsp2qWJgxjH3RpOWKzz5lTE/s/WNFioXmBjDb2LXdqajwC+nNR9Ad7XtZVWEkindSVuumIg9K3O/z/wKVN4yFj9U6DOQGUR0NuJAJIybb50if2L+sT7I2T52oMIaa8oZ7qhT745X8VNBlWW3y2MiHAb8tnn7B63hVwA5//0HDYQxBAuYJ6XDFNKP4M8Mkv8b/jq3ZT2fYG5baZFsY7SH44qnxrmIMHEinK0SJoc/9HPwlNW1FMWvKtrqiqqoyf5/UxauvB9kNzH2pYtaBiOR2/2GmlCqiag2hNJULnzpYLpYSs2HS65JImN6YI++q2X3eTpBltwxyYN+uU03+PZ6JnlacMnVaFhmEo+yNIGweihHWRZUapgCy1a9WJbxJ9U8Nsju5AhzrPpBpyqH+m86QI9h0QTAqAToF0v3sanzySZMcAKJLHWP+1YCrMfPYkEw75JRDYoRTPMmg+IqPM+hdaTzkn1zVYRuuI9uzy1Y8llRtmmJSBcYIE3L9BRd7rmccTyP7rV2Jv6COMzCtrxuRGSOqnqr1SQ9+9wUcUohYQR5oOMO0jdK/SsIcDBdm52H892dk9GO1ieeUNkM2NImnMsyTUcD1gcQByjvO16gFmHR0JFLgr2pgXnpHMTuvmtYQp6t/UczaB+3TEhK/1akUhX4jBwyywfADcyKkDl5+OX1nXPL2VDrffi++XKP93gndmOCJLPW+RGq06mcm9WGTHQAabekrijGGqq5XwWUypnx8CyGqybNYyYA6kXTryqI1lKZdQao6uBd5iqEOGxuuTjSz9D/qg/XxVTPs7ugAFXD/PGKuXNH1+LgWw5PEwsTvvC95EYGAE="}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":33336,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":33336,"ephemeral_1h_input_tokens":0},"output_tokens":7,"service_tier":"standard"}},"requestId":"req_011CUrmDhd6pQ9Mf4Hnnjg5i","type":"assistant","uuid":"c5f37137-3cdf-47bb-a6db-e85af33a51f9","timestamp":"2025-11-06T14:05:29.520Z"}
{"parentUuid":"c5f37137-3cdf-47bb-a6db-e85af33a51f9","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Hcc1jQH4ynLFhrUAkR9XRo","type":"message","role":"assistant","content":[{"type":"text","text":"I'll search for current information about these models to give you an accurate comparison."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":33336,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":33336,"ephemeral_1h_input_tokens":0},"output_tokens":7,"service_tier":"standard"}},"requestId":"req_011CUrmDhd6pQ9Mf4Hnnjg5i","type":"assistant","uuid":"91388a05-31d4-4614-9cec-c61c5ed765ce","timestamp":"2025-11-06T14:05:30.031Z"}
{"parentUuid":"91388a05-31d4-4614-9cec-c61c5ed765ce","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Hcc1jQH4ynLFhrUAkR9XRo","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01TuwnmdHz1Zncmt1DoWfKY4","name":"mcp__plugin_exa-mcp-server_exa__web_search_exa","input":{"query":"Claude 4.5 Sonnet Haiku vs Qwen VL 32B comparison benchmarks performance 2025","numResults":5}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":33336,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":33336,"ephemeral_1h_input_tokens":0},"output_tokens":7,"service_tier":"standard"}},"requestId":"req_011CUrmDhd6pQ9Mf4Hnnjg5i","type":"assistant","uuid":"83f5d082-4c77-4e8b-be37-1f6979396508","timestamp":"2025-11-06T14:05:30.924Z"}
{"parentUuid":"83f5d082-4c77-4e8b-be37-1f6979396508","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Hcc1jQH4ynLFhrUAkR9XRo","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01KB6idZtzjo9Poud6Vq5292","name":"mcp__plugin_exa-mcp-server_exa__web_search_exa","input":{"query":"Qwen VL 32B model capabilities specifications vision language","numResults":5}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":33336,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":33336,"ephemeral_1h_input_tokens":0},"output_tokens":7,"service_tier":"standard"}},"requestId":"req_011CUrmDhd6pQ9Mf4Hnnjg5i","type":"assistant","uuid":"b68c6d98-31d9-4578-af56-71d4fee60b35","timestamp":"2025-11-06T14:05:31.569Z"}
{"parentUuid":"b68c6d98-31d9-4578-af56-71d4fee60b35","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01TuwnmdHz1Zncmt1DoWfKY4","type":"tool_result","content":[{"type":"text","text":"{\n  \"requestId\": \"25cb48c0ca9e6bda48a6e74fe72800ca\",\n  \"autoDate\": \"2025-01-01T00:00:00.000Z\",\n  \"resolvedSearchType\": \"neural\",\n  \"results\": [\n    {\n      \"id\": \"https://llm-stats.com/models/compare/claude-haiku-4-5-20251015-vs-qwen2.5-vl-32b\",\n      \"title\": \"Claude Haiku 4.5 vs Qwen2.5 VL 32B Instruct - LLM Stats\",\n      \"url\": \"https://llm-stats.com/models/compare/claude-haiku-4-5-20251015-vs-qwen2.5-vl-32b\",\n      \"publishedDate\": \"2025-11-06T14:05:35.000Z\",\n      \"author\": \"LLM Stats\",\n      \"text\": \"llm-stats.com The leading AI leaderboard featuring LLM benchmarks and LLM arenas for objective model evaluation. Contact\",\n      \"image\": \"https://llm-stats.com/api/og/compare/claude-haiku-4-5-20251015/qwen2.5-vl-32b\",\n      \"favicon\": \"https://llm-stats.com/favicon.ico\"\n    },\n    {\n      \"id\": \"https://blog.galaxy.ai/compare/claude-haiku-4-5-vs-qwen2-5-vl-32b-instruct\",\n      \"title\": \"Claude Haiku 4.5 vs Qwen2.5 VL 32B Instruct (Comparative Analysis)\",\n      \"url\": \"https://blog.galaxy.ai/compare/claude-haiku-4-5-vs-qwen2-5-vl-32b-instruct\",\n      \"publishedDate\": \"2025-10-15T00:00:00.000Z\",\n      \"author\": \"\",\n      \"text\": \"# Claude Haiku 4.5 vs Qwen2.5 VL 32B Instruct (Comparative Analysis)\\n\\nCompareClaude Haiku 4.5Qwen2.5 VL 32B Instruct\\n\\nComparative Analysis: [Claude Haiku 4.5](https://blog.galaxy.ai/model/claude-haiku-4-5) vs. [Qwen2.5 VL 32B Instruct](https://blog.galaxy.ai/model/qwen2-5-vl-32b-instruct)\\n\\nWant to try out these models side by side? [Try Galaxy.ai for free](https://galaxy.ai/?ref=blog)\\n\\n## Overview\\n\\nQwen2.5 VL 32B Instruct was released 6 months before Claude Haiku 4.5.\\n\\n| Claude Haiku 4.5 | Qwen2.5 VL 32B Instruct |\\n| --- | --- |\\n| Model Provider The organization behind this AI's development | Anthropic | Qwen |\\n| Input Context Window Maximum input tokens this model can process at once | 200K tokens | 16.4K tokens |\\n| Output Token Limit Maximum output tokens this model can generate at once | 64K tokens | 16.4K tokens |\\n| Release Date When this model first became publicly available | October 15, 2025 23 days ago October 15th, 2025 | March 24, 2025 7 months ago March 24th, 2025 |\\n\\n## Capabilities & Features\\n\\nCompare supported features, modalities, and advanced capabilities\\n\\n| Claude Haiku 4.5 | Qwen2.5 VL 32B Instruct |\\n| --- | --- |\\n| Input Types Supported input formats | ðŸ–¼ï¸ImageðŸ“Text | ðŸ“TextðŸ–¼ï¸Image |\\n| Output Types Supported output formats | ðŸ“Text | ðŸ“Text |\\n| Tokenizer Text encoding system | Claude | Qwen |\\n| Key Features Advanced capabilities | âœ“Function CallingStructured Outputâœ“Reasoning Modeâœ“Content Moderation | Function Callingâœ“Structured OutputReasoning ModeContent Moderation |\\n| Open Source Model availability | Proprietary | [Available on HuggingFace â†’](https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct) |\\n\\n## Pricing\\n\\nClaude Haiku 4.5 is roughly 20.0x more expensive compared to Qwen2.5 VL 32B Instruct for input tokens and roughly 22.7x more expensive for output tokens.\\n\\n| Claude Haiku 4.5 | Qwen2.5 VL 32B Instruct |\\n| --- | --- |\\n| Input Token Cost Cost per million input tokens | $1.00 per million tokens | $0.05 per million tokens |\\n| Output Token Cost \",\n      \"image\": \"https://galaxy.ai/api/og?title=Claude%20Haiku%204.5%20vs%20Qwen2.5%20VL%2032B%20Instruct%20(Comparative%20Analysis)\",\n      \"favicon\": \"https://blog.galaxy.ai/favicon.ico\"\n    },\n    {\n      \"id\": \"https://skywork.ai/blog/llm/top-10-open-llms-2025-november-ranking-analysis/\",\n      \"title\": \"Top 10 Open LLMs 2025 November Ranking & Analysis - Skywork.ai\",\n      \"url\": \"https://skywork.ai/blog/llm/top-10-open-llms-2025-november-ranking-analysis/\",\n      \"publishedDate\": \"2025-11-03T05:51:52.000Z\",\n      \"author\": \"Camille\",\n      \"text\": \"[Skip to content](https://skywork.ai/skywork.ai#content)\\n\\nHi, Camilleâ€™s here! On October 28, 2025, I fell into a small rabbit hole. I was halfway through drafting an email sequence when a friend pinged me: â€œHave you tried the latest Qwen? Itâ€™s ridiculous.â€ I paused the email, spun up a notebook, and decided to run my own open LLM ranking for 2025. Not sponsored, just honest results from a week of testing (Oct 28â€“Nov 1, 2025), a couple of late coffees, and way too many prompt variations.\\n\\n## Open LLM Ranking 2025: Methodology Explained\\n\\nI didnâ€™t want this to be another hand-wavy list. So I used a repeatable setup:\\n\\n- Dates: Oct 28â€“Nov 1, 2025 (screenshots and logs saved: I note versions below)\\n- Hardware: 8x A100 80GB (for 70B-ish models) and a single A100 for 7â€“8B tests\\n- Runtime: vLLM 0.5.x with tensor parallel: bf16 where supported\\n- Datasets/Scoring: EleutherAI lm-evaluation-harness v0.4.x: MT-Bench via FastChat: GSM8K strict: HumanEval pass@1\\n- Decoding: temperature 0 for benchmarks: 0.2â€“0.7 for qualitative prompts\\n- Context: 8K default: I also sanity-checked 128K claims with synthetic retrieval prompts\\n- Safety: default model guardrails left on: jailbreak tests excluded (different article)\\n\\nI weighted tasks that actually show up in real work: reasoning (MMLU/GSM8K), coding (HumanEval), and instruction-following (MT-Bench). I also added a â€œediting workflowâ€ sanity test: rewrite, summarize, and fact-check a messy paragraph with citations.\\n\\nNote: open here means weights-available models with permissive or research-friendly licenses. I linked to official docs or model cards where relevant.\\n\\n## Top 3 Open LLMs Deep Dive\\n\\n### Alibaba Qwen Series Performance\\n\\nI went in curious, came out kind of impressed. The Qwen 2.5 line (especially the 72B/â€Maxâ€-class instruct) felt steady under pressure. On Oct 30, I ran GSM8K with chain-of-thought disabled and a simple â€œshow stepsâ€ style. It solved more word problems cleanly than I expected and didnâ€™t hallucinate units as often. \",\n      \"image\": \"https://blog-cdn.skywork.ai/wp-content/uploads/2025/11/image-1762148931.png\"\n    },\n    {\n      \"id\": \"https://medium.com/@leucopsis/claude-haiku-4-5-review-4ac12a103275\",\n      \"title\": \"Claude Haiku 4.5 Review - Medium\",\n      \"url\": \"https://medium.com/@leucopsis/claude-haiku-4-5-review-4ac12a103275\",\n      \"publishedDate\": \"2025-10-18T06:43:50.000Z\",\n      \"author\": \"Barnacle Goose\",\n      \"text\": \"[Sitemap](https://medium.com/sitemap/sitemap.xml)\\n\\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4ac12a103275&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40leucopsis%2Fclaude-haiku-4-5-review-4ac12a103275&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n\\n[Search](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40leucopsis%2Fclaude-haiku-4-5-review-4ac12a103275&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n# **Claude Haiku 4.5 Review**\\n\\n[Barnacle Goose](https://medium.com/@leucopsis?source=post_page---byline--4ac12a103275---------------------------------------)\\n\\n10 min read\\n\\nÂ·\\n\\n4 days ago\\n\\n--\\n\\nListen\\n\\nShare\\n\\nReleased on October 15, 2025, [Claude Haiku 4.5](https://haiku.gptchatly.com) brings much of last springâ€™s flagship capability into a package that is dramatically faster and markedly cheaper to operate - enough to reshape how teams design AI systems for production workloads. At a high level, [Haiku 4.5](https://haiku.gptchatly.com/) is a pragmatic engineerâ€™s tool: quick on the draw, and steady at multi-step software tasks.\\n\\nBelow, I break down what the model is, what it does well, how it behaves in practice, where it trails competitors, and why its launch matters for anyone \",\n      \"image\": \"https://miro.medium.com/v2/resize:fit:1200/1*NSakn54QrlR6bY3ESEanTQ.png\",\n      \"favicon\": \"https://miro.medium.com/v2/5d8de952517e8160e40ef9841c781cdc14a5db313057fa3c3de41c6f5b494b19\"\n    },\n    {\n      \"id\": \"https://sourceforge.net/software/compare/Claude-Sonnet-4.5-vs-Qwen2.5-VL/\",\n      \"title\": \"Claude Sonnet 4.5 vs. Qwen2.5-VL Comparison - SourceForge\",\n      \"url\": \"https://sourceforge.net/software/compare/Claude-Sonnet-4.5-vs-Qwen2.5-VL/\",\n      \"publishedDate\": \"2025-01-01T00:00:00.000Z\",\n      \"author\": \"\",\n      \"text\": \"- [Join/Login](https://sourceforge.net/auth/)\\n- [Business Software](https://sourceforge.net/software/)\\n- [Open Source Software](https://sourceforge.net/directory/)\\n- [For Vendors](https://sourceforge.net/software/vendors/)\\n- [Blog](https://sourceforge.net/blog/)\\n- [About](https://sourceforge.net/about)\\n- More\\n - [Articles](https://sourceforge.net/articles/)\\n - [SourceForge Podcast](https://sourceforge.net/articles/category/sourceforge-podcast/)\\n - [Support / Documentation](https://docs.sourceforge.net/)\\n - [Subscribe to our Newsletter](https://sourceforge.net/user/newsletters)\\n - [Support Request](https://sourceforge.net/support)\\n\\n# Claude Sonnet 4.5 vs. Qwen2.5-VL Comparison Chart\\n\\n| ## [Claude Sonnet 4.5](https://sourceforge.net/software/product/Claude-Sonnet-4.5/) Anthropic | ## [Qwen2.5-VL](https://sourceforge.net/software/product/Qwen2.5-VL/) Alibaba | + | + |\\n| --- | --- | --- | --- |\\n| [Learn More](https://sourceforge.net/software/product/Claude-Sonnet-4.5/) [Update Features](https://sourceforge.net/sourceforge.net#claim_claude-sonnet-4-5.s) | [Learn More](https://sourceforge.net/software/product/Qwen2.5-VL/) [Update Features](https://sourceforge.net/sourceforge.net#claim_qwen2-5-vl.s) | Add To Compare | Add To Compare |\\n| --- | --- | --- | --- |\\n\\n| | |\\n| --- | --- |\\n| ### Related Products - [Claude Code](https://sourceforge.net/software/product/Claude-Code/) Claude Code is an AI-powered coding assistant that brings Anthropicâ€™s Claude directly to the developerâ€™s terminal. It enables engineers to search, understand, and modify million-line codebases instantly using natural language. By integrating with your existing command-line tools, version control systems, and CI/CD pipelines, Claude Code fits seamlessly into any development workflow. Developers can triage issues, refactor code, and generate pull requests without ever leaving the terminal. With its deep contextual understanding, it performs multi-file edits and real-time code analysis while preserving accu\",\n      \"favicon\": \"https://a.fsdn.com/con/img/sandiego/logo-180x180.png\"\n    }\n  ],\n  \"requestTags\": {\n    \"flagsEnabledInPosthog\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exa21,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache\",\n    \"flagsEnabledInRequest\": \"\",\n    \"enabledFlags\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exa21,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache\",\n    \"initialSearchType\": \"auto\",\n    \"didIncludeText\": \"no\",\n    \"didExcludeText\": \"no\",\n    \"didIncludeDomains\": \"no\",\n    \"didExcludeDomains\": \"no\",\n    \"didIncludeUrls\": \"no\",\n    \"didExcludeUrls\": \"no\",\n    \"moderation\": \"no\",\n    \"didUseDateFilters\": \"no\",\n    \"category\": \"none\",\n    \"didUseAutoprompt\": \"no\",\n    \"hasUserLocation\": \"no\",\n    \"didRequestContentText\": true,\n    \"didRequestContentHighlights\": false,\n    \"didRequestContentSummary\": false,\n    \"livecrawl\": \"preferred\",\n    \"requestedLinks\": \"no\",\n    \"willPostRerankNeural\": \"yes\",\n    \"willPostModerate\": false,\n    \"searchTypeBranch\": \"auto\",\n    \"autoBranch\": {\n      \"neuralSearch\": {\n        \"sierra\": {\n          \"query\": \"Claude 4.5 Sonnet Haiku vs Qwen VL 32B comparison benchmarks performance 2025\",\n          \"flags\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache,use_exa21\",\n          \"filters\": {\n            \"moderation\": \"blacklisted\",\n            \"moderationConfig\": {\n              \"llamaguardS1\": false,\n              \"llamaguardS3\": false,\n              \"llamaguardS4\": false,\n              \"llamaguardS12\": false,\n              \"domainBlacklisted\": false,\n              \"domainBlacklistedMedia\": false\n            }\n          },\n          \"queryDateString\": \"2025-11-06\",\n          \"numResultsRequested\": 35,\n          \"engine\": \"default\",\n          \"isTweetCategory\": false,\n          \"sierraTime\": 476.9025600000168,\n          \"numResultsReturned\": 35\n        },\n        \"fetchContents\": {\n          \"contentExpansion\": {\n            \"failedContents\": []\n          },\n          \"contentsResultsCount\": 35,\n          \"filteredResultsCount\": 0\n        }\n      },\n      \"keywordSearch\": {\n        \"query\": \"Claude 4.5 Sonnet Haiku vs Qwen VL 32B comparison benchmarks performance 2025\",\n        \"modifiedQuery\": \"Claude 4.5 Sonnet Haiku vs Qwen VL 32B comparison benchmarks performance 2025\",\n        \"slicedAmount\": 0,\n        \"searchData\": {\n          \"q\": \"Claude 4.5 Sonnet Haiku vs Qwen VL 32B comparison benchmarks performance 2025\",\n          \"num\": 10,\n          \"safe\": \"active\"\n        },\n        \"opts\": {\n          \"snippetMode\": \"snippet\",\n          \"addAnswerBox\": true,\n          \"addKnowledgeGraph\": true,\n          \"addPeopleAlsoAsk\": false,\n          \"allowEmptyAnswerLinks\": false,\n          \"providers\": [\n            {\n              \"provider\": \"serper\",\n              \"startAfterMs\": 0,\n              \"timeoutMs\": 10000\n            }\n          ],\n          \"timeoutMs\": 10000,\n          \"teamId\": \"cm60snyw00011cfcxf2vbwp8n\"\n        },\n        \"serperSource\": \"regular\",\n        \"peopleAlsoAsk\": [\n          \"When to use Claude Haiku vs sonnet?\"\n        ],\n        \"success\": true,\n        \"numResultsRequested\": 5,\n        \"numResultsReturned\": 10,\n        \"fetchContents\": {\n          \"contentExpansion\": {\n            \"failedContents\": [\n              \"https://blog.galaxy.ai/compare/claude-haiku-4-5-vs-qwen3-vl-32b-instruct\",\n              \"https://slashdot.org/software/comparison/Claude-Sonnet-4.5-vs-Qwen2.5-VL/\",\n              \"https://artificialanalysis.ai/models/comparisons/claude-4-5-sonnet-thinking-vs-qwen2.5-32b-instruct\"\n            ]\n          },\n          \"contentsResultsCount\": 7,\n          \"filteredResultsCount\": 0\n        }\n      },\n      \"neitherFailed\": true,\n      \"autoJoin\": true,\n      \"autoJoinRerank\": {\n        \"reranker\": {\n          \"withModeration\": false,\n          \"preserveUnranked\": true,\n          \"strategy\": {\n            \"strategy\": \"modal_sranker_qwen_trt\",\n            \"default\": true\n          },\n          \"strategy_decided\": \"modal_sranker_qwen_trt\",\n          \"strategy_reason\": \"explicit_strategy\",\n          \"isQdf\": false\n        }\n      },\n      \"autoJoinResults\": \"35\"\n    },\n    \"decidedAutoType\": \"neural\",\n    \"resolvedSearchType\": \"neural\",\n    \"autoType\": \"neural\",\n    \"decidedResultsLength\": 5,\n    \"totalFilterCount\": \"12\",\n    \"contextExpansionNeeded\": \"not-needed\",\n    \"finalResultsCount\": 5,\n    \"searchTime\": 6873.6,\n    \"path\": \"/search\",\n    \"method\": \"POST\",\n    \"teamId\": \"cm60snyw00011cfcxf2vbwp8n\"\n  },\n  \"effectiveFilters\": {\n    \"includeDomains\": [],\n    \"excludeDomains\": [],\n    \"includeUrls\": [],\n    \"excludeUrls\": [],\n    \"includeText\": [],\n    \"excludeText\": []\n  },\n  \"searchTime\": 6873.6,\n  \"costDollars\": {\n    \"total\": 0.01,\n    \"search\": {\n      \"neural\": 0.005\n    },\n    \"contents\": {\n      \"text\": 0.005\n    }\n  }\n}"}]}]},"uuid":"de29ab3e-cebe-4cdb-88e8-e6a59776eb51","timestamp":"2025-11-06T14:05:39.426Z","toolUseResult":[{"type":"text","text":"{\n  \"requestId\": \"25cb48c0ca9e6bda48a6e74fe72800ca\",\n  \"autoDate\": \"2025-01-01T00:00:00.000Z\",\n  \"resolvedSearchType\": \"neural\",\n  \"results\": [\n    {\n      \"id\": \"https://llm-stats.com/models/compare/claude-haiku-4-5-20251015-vs-qwen2.5-vl-32b\",\n      \"title\": \"Claude Haiku 4.5 vs Qwen2.5 VL 32B Instruct - LLM Stats\",\n      \"url\": \"https://llm-stats.com/models/compare/claude-haiku-4-5-20251015-vs-qwen2.5-vl-32b\",\n      \"publishedDate\": \"2025-11-06T14:05:35.000Z\",\n      \"author\": \"LLM Stats\",\n      \"text\": \"llm-stats.com The leading AI leaderboard featuring LLM benchmarks and LLM arenas for objective model evaluation. Contact\",\n      \"image\": \"https://llm-stats.com/api/og/compare/claude-haiku-4-5-20251015/qwen2.5-vl-32b\",\n      \"favicon\": \"https://llm-stats.com/favicon.ico\"\n    },\n    {\n      \"id\": \"https://blog.galaxy.ai/compare/claude-haiku-4-5-vs-qwen2-5-vl-32b-instruct\",\n      \"title\": \"Claude Haiku 4.5 vs Qwen2.5 VL 32B Instruct (Comparative Analysis)\",\n      \"url\": \"https://blog.galaxy.ai/compare/claude-haiku-4-5-vs-qwen2-5-vl-32b-instruct\",\n      \"publishedDate\": \"2025-10-15T00:00:00.000Z\",\n      \"author\": \"\",\n      \"text\": \"# Claude Haiku 4.5 vs Qwen2.5 VL 32B Instruct (Comparative Analysis)\\n\\nCompareClaude Haiku 4.5Qwen2.5 VL 32B Instruct\\n\\nComparative Analysis: [Claude Haiku 4.5](https://blog.galaxy.ai/model/claude-haiku-4-5) vs. [Qwen2.5 VL 32B Instruct](https://blog.galaxy.ai/model/qwen2-5-vl-32b-instruct)\\n\\nWant to try out these models side by side? [Try Galaxy.ai for free](https://galaxy.ai/?ref=blog)\\n\\n## Overview\\n\\nQwen2.5 VL 32B Instruct was released 6 months before Claude Haiku 4.5.\\n\\n| Claude Haiku 4.5 | Qwen2.5 VL 32B Instruct |\\n| --- | --- |\\n| Model Provider The organization behind this AI's development | Anthropic | Qwen |\\n| Input Context Window Maximum input tokens this model can process at once | 200K tokens | 16.4K tokens |\\n| Output Token Limit Maximum output tokens this model can generate at once | 64K tokens | 16.4K tokens |\\n| Release Date When this model first became publicly available | October 15, 2025 23 days ago October 15th, 2025 | March 24, 2025 7 months ago March 24th, 2025 |\\n\\n## Capabilities & Features\\n\\nCompare supported features, modalities, and advanced capabilities\\n\\n| Claude Haiku 4.5 | Qwen2.5 VL 32B Instruct |\\n| --- | --- |\\n| Input Types Supported input formats | ðŸ–¼ï¸ImageðŸ“Text | ðŸ“TextðŸ–¼ï¸Image |\\n| Output Types Supported output formats | ðŸ“Text | ðŸ“Text |\\n| Tokenizer Text encoding system | Claude | Qwen |\\n| Key Features Advanced capabilities | âœ“Function CallingStructured Outputâœ“Reasoning Modeâœ“Content Moderation | Function Callingâœ“Structured OutputReasoning ModeContent Moderation |\\n| Open Source Model availability | Proprietary | [Available on HuggingFace â†’](https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct) |\\n\\n## Pricing\\n\\nClaude Haiku 4.5 is roughly 20.0x more expensive compared to Qwen2.5 VL 32B Instruct for input tokens and roughly 22.7x more expensive for output tokens.\\n\\n| Claude Haiku 4.5 | Qwen2.5 VL 32B Instruct |\\n| --- | --- |\\n| Input Token Cost Cost per million input tokens | $1.00 per million tokens | $0.05 per million tokens |\\n| Output Token Cost \",\n      \"image\": \"https://galaxy.ai/api/og?title=Claude%20Haiku%204.5%20vs%20Qwen2.5%20VL%2032B%20Instruct%20(Comparative%20Analysis)\",\n      \"favicon\": \"https://blog.galaxy.ai/favicon.ico\"\n    },\n    {\n      \"id\": \"https://skywork.ai/blog/llm/top-10-open-llms-2025-november-ranking-analysis/\",\n      \"title\": \"Top 10 Open LLMs 2025 November Ranking & Analysis - Skywork.ai\",\n      \"url\": \"https://skywork.ai/blog/llm/top-10-open-llms-2025-november-ranking-analysis/\",\n      \"publishedDate\": \"2025-11-03T05:51:52.000Z\",\n      \"author\": \"Camille\",\n      \"text\": \"[Skip to content](https://skywork.ai/skywork.ai#content)\\n\\nHi, Camilleâ€™s here! On October 28, 2025, I fell into a small rabbit hole. I was halfway through drafting an email sequence when a friend pinged me: â€œHave you tried the latest Qwen? Itâ€™s ridiculous.â€ I paused the email, spun up a notebook, and decided to run my own open LLM ranking for 2025. Not sponsored, just honest results from a week of testing (Oct 28â€“Nov 1, 2025), a couple of late coffees, and way too many prompt variations.\\n\\n## Open LLM Ranking 2025: Methodology Explained\\n\\nI didnâ€™t want this to be another hand-wavy list. So I used a repeatable setup:\\n\\n- Dates: Oct 28â€“Nov 1, 2025 (screenshots and logs saved: I note versions below)\\n- Hardware: 8x A100 80GB (for 70B-ish models) and a single A100 for 7â€“8B tests\\n- Runtime: vLLM 0.5.x with tensor parallel: bf16 where supported\\n- Datasets/Scoring: EleutherAI lm-evaluation-harness v0.4.x: MT-Bench via FastChat: GSM8K strict: HumanEval pass@1\\n- Decoding: temperature 0 for benchmarks: 0.2â€“0.7 for qualitative prompts\\n- Context: 8K default: I also sanity-checked 128K claims with synthetic retrieval prompts\\n- Safety: default model guardrails left on: jailbreak tests excluded (different article)\\n\\nI weighted tasks that actually show up in real work: reasoning (MMLU/GSM8K), coding (HumanEval), and instruction-following (MT-Bench). I also added a â€œediting workflowâ€ sanity test: rewrite, summarize, and fact-check a messy paragraph with citations.\\n\\nNote: open here means weights-available models with permissive or research-friendly licenses. I linked to official docs or model cards where relevant.\\n\\n## Top 3 Open LLMs Deep Dive\\n\\n### Alibaba Qwen Series Performance\\n\\nI went in curious, came out kind of impressed. The Qwen 2.5 line (especially the 72B/â€Maxâ€-class instruct) felt steady under pressure. On Oct 30, I ran GSM8K with chain-of-thought disabled and a simple â€œshow stepsâ€ style. It solved more word problems cleanly than I expected and didnâ€™t hallucinate units as often. \",\n      \"image\": \"https://blog-cdn.skywork.ai/wp-content/uploads/2025/11/image-1762148931.png\"\n    },\n    {\n      \"id\": \"https://medium.com/@leucopsis/claude-haiku-4-5-review-4ac12a103275\",\n      \"title\": \"Claude Haiku 4.5 Review - Medium\",\n      \"url\": \"https://medium.com/@leucopsis/claude-haiku-4-5-review-4ac12a103275\",\n      \"publishedDate\": \"2025-10-18T06:43:50.000Z\",\n      \"author\": \"Barnacle Goose\",\n      \"text\": \"[Sitemap](https://medium.com/sitemap/sitemap.xml)\\n\\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F4ac12a103275&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40leucopsis%2Fclaude-haiku-4-5-review-4ac12a103275&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n\\n[Search](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40leucopsis%2Fclaude-haiku-4-5-review-4ac12a103275&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n# **Claude Haiku 4.5 Review**\\n\\n[Barnacle Goose](https://medium.com/@leucopsis?source=post_page---byline--4ac12a103275---------------------------------------)\\n\\n10 min read\\n\\nÂ·\\n\\n4 days ago\\n\\n--\\n\\nListen\\n\\nShare\\n\\nReleased on October 15, 2025, [Claude Haiku 4.5](https://haiku.gptchatly.com) brings much of last springâ€™s flagship capability into a package that is dramatically faster and markedly cheaper to operate - enough to reshape how teams design AI systems for production workloads. At a high level, [Haiku 4.5](https://haiku.gptchatly.com/) is a pragmatic engineerâ€™s tool: quick on the draw, and steady at multi-step software tasks.\\n\\nBelow, I break down what the model is, what it does well, how it behaves in practice, where it trails competitors, and why its launch matters for anyone \",\n      \"image\": \"https://miro.medium.com/v2/resize:fit:1200/1*NSakn54QrlR6bY3ESEanTQ.png\",\n      \"favicon\": \"https://miro.medium.com/v2/5d8de952517e8160e40ef9841c781cdc14a5db313057fa3c3de41c6f5b494b19\"\n    },\n    {\n      \"id\": \"https://sourceforge.net/software/compare/Claude-Sonnet-4.5-vs-Qwen2.5-VL/\",\n      \"title\": \"Claude Sonnet 4.5 vs. Qwen2.5-VL Comparison - SourceForge\",\n      \"url\": \"https://sourceforge.net/software/compare/Claude-Sonnet-4.5-vs-Qwen2.5-VL/\",\n      \"publishedDate\": \"2025-01-01T00:00:00.000Z\",\n      \"author\": \"\",\n      \"text\": \"- [Join/Login](https://sourceforge.net/auth/)\\n- [Business Software](https://sourceforge.net/software/)\\n- [Open Source Software](https://sourceforge.net/directory/)\\n- [For Vendors](https://sourceforge.net/software/vendors/)\\n- [Blog](https://sourceforge.net/blog/)\\n- [About](https://sourceforge.net/about)\\n- More\\n - [Articles](https://sourceforge.net/articles/)\\n - [SourceForge Podcast](https://sourceforge.net/articles/category/sourceforge-podcast/)\\n - [Support / Documentation](https://docs.sourceforge.net/)\\n - [Subscribe to our Newsletter](https://sourceforge.net/user/newsletters)\\n - [Support Request](https://sourceforge.net/support)\\n\\n# Claude Sonnet 4.5 vs. Qwen2.5-VL Comparison Chart\\n\\n| ## [Claude Sonnet 4.5](https://sourceforge.net/software/product/Claude-Sonnet-4.5/) Anthropic | ## [Qwen2.5-VL](https://sourceforge.net/software/product/Qwen2.5-VL/) Alibaba | + | + |\\n| --- | --- | --- | --- |\\n| [Learn More](https://sourceforge.net/software/product/Claude-Sonnet-4.5/) [Update Features](https://sourceforge.net/sourceforge.net#claim_claude-sonnet-4-5.s) | [Learn More](https://sourceforge.net/software/product/Qwen2.5-VL/) [Update Features](https://sourceforge.net/sourceforge.net#claim_qwen2-5-vl.s) | Add To Compare | Add To Compare |\\n| --- | --- | --- | --- |\\n\\n| | |\\n| --- | --- |\\n| ### Related Products - [Claude Code](https://sourceforge.net/software/product/Claude-Code/) Claude Code is an AI-powered coding assistant that brings Anthropicâ€™s Claude directly to the developerâ€™s terminal. It enables engineers to search, understand, and modify million-line codebases instantly using natural language. By integrating with your existing command-line tools, version control systems, and CI/CD pipelines, Claude Code fits seamlessly into any development workflow. Developers can triage issues, refactor code, and generate pull requests without ever leaving the terminal. With its deep contextual understanding, it performs multi-file edits and real-time code analysis while preserving accu\",\n      \"favicon\": \"https://a.fsdn.com/con/img/sandiego/logo-180x180.png\"\n    }\n  ],\n  \"requestTags\": {\n    \"flagsEnabledInPosthog\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exa21,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache\",\n    \"flagsEnabledInRequest\": \"\",\n    \"enabledFlags\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exa21,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache\",\n    \"initialSearchType\": \"auto\",\n    \"didIncludeText\": \"no\",\n    \"didExcludeText\": \"no\",\n    \"didIncludeDomains\": \"no\",\n    \"didExcludeDomains\": \"no\",\n    \"didIncludeUrls\": \"no\",\n    \"didExcludeUrls\": \"no\",\n    \"moderation\": \"no\",\n    \"didUseDateFilters\": \"no\",\n    \"category\": \"none\",\n    \"didUseAutoprompt\": \"no\",\n    \"hasUserLocation\": \"no\",\n    \"didRequestContentText\": true,\n    \"didRequestContentHighlights\": false,\n    \"didRequestContentSummary\": false,\n    \"livecrawl\": \"preferred\",\n    \"requestedLinks\": \"no\",\n    \"willPostRerankNeural\": \"yes\",\n    \"willPostModerate\": false,\n    \"searchTypeBranch\": \"auto\",\n    \"autoBranch\": {\n      \"neuralSearch\": {\n        \"sierra\": {\n          \"query\": \"Claude 4.5 Sonnet Haiku vs Qwen VL 32B comparison benchmarks performance 2025\",\n          \"flags\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache,use_exa21\",\n          \"filters\": {\n            \"moderation\": \"blacklisted\",\n            \"moderationConfig\": {\n              \"llamaguardS1\": false,\n              \"llamaguardS3\": false,\n              \"llamaguardS4\": false,\n              \"llamaguardS12\": false,\n              \"domainBlacklisted\": false,\n              \"domainBlacklistedMedia\": false\n            }\n          },\n          \"queryDateString\": \"2025-11-06\",\n          \"numResultsRequested\": 35,\n          \"engine\": \"default\",\n          \"isTweetCategory\": false,\n          \"sierraTime\": 476.9025600000168,\n          \"numResultsReturned\": 35\n        },\n        \"fetchContents\": {\n          \"contentExpansion\": {\n            \"failedContents\": []\n          },\n          \"contentsResultsCount\": 35,\n          \"filteredResultsCount\": 0\n        }\n      },\n      \"keywordSearch\": {\n        \"query\": \"Claude 4.5 Sonnet Haiku vs Qwen VL 32B comparison benchmarks performance 2025\",\n        \"modifiedQuery\": \"Claude 4.5 Sonnet Haiku vs Qwen VL 32B comparison benchmarks performance 2025\",\n        \"slicedAmount\": 0,\n        \"searchData\": {\n          \"q\": \"Claude 4.5 Sonnet Haiku vs Qwen VL 32B comparison benchmarks performance 2025\",\n          \"num\": 10,\n          \"safe\": \"active\"\n        },\n        \"opts\": {\n          \"snippetMode\": \"snippet\",\n          \"addAnswerBox\": true,\n          \"addKnowledgeGraph\": true,\n          \"addPeopleAlsoAsk\": false,\n          \"allowEmptyAnswerLinks\": false,\n          \"providers\": [\n            {\n              \"provider\": \"serper\",\n              \"startAfterMs\": 0,\n              \"timeoutMs\": 10000\n            }\n          ],\n          \"timeoutMs\": 10000,\n          \"teamId\": \"cm60snyw00011cfcxf2vbwp8n\"\n        },\n        \"serperSource\": \"regular\",\n        \"peopleAlsoAsk\": [\n          \"When to use Claude Haiku vs sonnet?\"\n        ],\n        \"success\": true,\n        \"numResultsRequested\": 5,\n        \"numResultsReturned\": 10,\n        \"fetchContents\": {\n          \"contentExpansion\": {\n            \"failedContents\": [\n              \"https://blog.galaxy.ai/compare/claude-haiku-4-5-vs-qwen3-vl-32b-instruct\",\n              \"https://slashdot.org/software/comparison/Claude-Sonnet-4.5-vs-Qwen2.5-VL/\",\n              \"https://artificialanalysis.ai/models/comparisons/claude-4-5-sonnet-thinking-vs-qwen2.5-32b-instruct\"\n            ]\n          },\n          \"contentsResultsCount\": 7,\n          \"filteredResultsCount\": 0\n        }\n      },\n      \"neitherFailed\": true,\n      \"autoJoin\": true,\n      \"autoJoinRerank\": {\n        \"reranker\": {\n          \"withModeration\": false,\n          \"preserveUnranked\": true,\n          \"strategy\": {\n            \"strategy\": \"modal_sranker_qwen_trt\",\n            \"default\": true\n          },\n          \"strategy_decided\": \"modal_sranker_qwen_trt\",\n          \"strategy_reason\": \"explicit_strategy\",\n          \"isQdf\": false\n        }\n      },\n      \"autoJoinResults\": \"35\"\n    },\n    \"decidedAutoType\": \"neural\",\n    \"resolvedSearchType\": \"neural\",\n    \"autoType\": \"neural\",\n    \"decidedResultsLength\": 5,\n    \"totalFilterCount\": \"12\",\n    \"contextExpansionNeeded\": \"not-needed\",\n    \"finalResultsCount\": 5,\n    \"searchTime\": 6873.6,\n    \"path\": \"/search\",\n    \"method\": \"POST\",\n    \"teamId\": \"cm60snyw00011cfcxf2vbwp8n\"\n  },\n  \"effectiveFilters\": {\n    \"includeDomains\": [],\n    \"excludeDomains\": [],\n    \"includeUrls\": [],\n    \"excludeUrls\": [],\n    \"includeText\": [],\n    \"excludeText\": []\n  },\n  \"searchTime\": 6873.6,\n  \"costDollars\": {\n    \"total\": 0.01,\n    \"search\": {\n      \"neural\": 0.005\n    },\n    \"contents\": {\n      \"text\": 0.005\n    }\n  }\n}"}]}
{"parentUuid":"de29ab3e-cebe-4cdb-88e8-e6a59776eb51","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01KB6idZtzjo9Poud6Vq5292","type":"tool_result","content":[{"type":"text","text":"{\n  \"requestId\": \"b6bf184e95381cb1b94dd535b5a37d33\",\n  \"resolvedSearchType\": \"neural\",\n  \"results\": [\n    {\n      \"id\": \"https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct\",\n      \"title\": \"Qwen/Qwen3-VL-32B-Instruct - Hugging Face\",\n      \"url\": \"https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct\",\n      \"publishedDate\": \"2025-11-01T14:44:32.000Z\",\n      \"author\": \"\",\n      \"text\": \"# Qwen3-VL-32B-Instruct\\n\\nMeet Qwen3-VL â€” the most powerful vision-language model in the Qwen series to date.\\n\\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\\n\\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoningâ€‘enhanced Thinking editions for flexible, onâ€‘demand deployment.\\n\\n#### Key Enhancements:\\n\\n- **Visual Agent**: Operates PC/mobile GUIsâ€”recognizes elements, understands functions, invokes tools, completes tasks.\\n\\n- **Visual Coding Boost**: Generates Draw.io/HTML/CSS/JS from images/videos.\\n\\n- **Advanced Spatial Perception**: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\\n\\n- **Long Context & Video Understanding**: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\\n\\n- **Enhanced Multimodal Reasoning**: Excels in STEM/Mathâ€”causal analysis and logical, evidence-based answers.\\n\\n- **Upgraded Visual Recognition**: Broader, higher-quality pretraining is able to â€œrecognize everythingâ€â€”celebrities, anime, products, landmarks, flora/fauna, etc.\\n\\n- **Expanded OCR**: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\\n\\n- **Text Understanding on par with pure LLMs**: Seamless textâ€“vision fusion for lossless, unified comprehension.\\n\\n\\n#### Model Architecture Updates:\\n\\n1. **Interleaved-MRoPE**: Fullâ€‘frequency allocation over time, width, and height via robust positional embeddings, enhancing longâ€‘horizon video reasoning.\\n\\n2. **DeepStack**: Fuses multiâ€‘level ViT features to capture fineâ€‘grained details and sharpen imageâ€“text alignment.\\n\\n3. **Text\",\n      \"image\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/Qwen/Qwen3-VL-32B-Instruct.png\"\n    },\n    {\n      \"id\": \"https://ollama.com/library/qwen3-vl:32b\",\n      \"title\": \"qwen3-vl:32b - Ollama\",\n      \"url\": \"https://ollama.com/library/qwen3-vl:32b\",\n      \"publishedDate\": \"2025-10-29T23:24:00.000Z\",\n      \"author\": \"\",\n      \"text\": \"[qwen3-vl](https://ollama.com/library/qwen3-vl):32b\\n\\n61.9KÂ DownloadsUpdatedÂ 3 days ago\\n\\n## The most powerful vision-language model in the Qwen model family to date.\\n\\nCancel\\n\\nvisioncloud2b4b8b30b32b235b\\n\\nUpdated 4 days ago\\n\\n4 days ago\\n\\nff2e46876908 Â· 21GB Â·\\n\\n[model](https://ollama.com/library/qwen3-vl:32b/blobs/6e416d39200a)\\n\\narchqwen3vl\\n\\nÂ·\\n\\nparameters33.4B\\n\\nÂ·\\n\\nquantizationQ4\\\\_K\\\\_M\\n\\n21GB\\n\\n[license](https://ollama.com/library/qwen3-vl:32b/blobs/7339fa418c9a)\\n\\nApache License\\nVersion 2.0, January 2004\\nhttp://www.apache.org/licenses/\\nTERMS AND CONDITIONS FOR US\\n\\n11kB\\n\\n[params](https://ollama.com/library/qwen3-vl:32b/blobs/f6417cb1e269)\\n\\n{\\n\\\"temperature\\\": 1,\\n\\\"top\\\\_k\\\": 20,\\n\\\"top\\\\_p\\\": 0.95\\n}\\n\\n42B\\n\\n## Readme\\n\\n> Qwen3-VL models require [Ollama 0.12.7](https://github.com/ollama/ollama/releases)\\n\\nQwen3-VL is the most powerful vision-language model in the Qwen family to date.\\n\\nIn this generation, there are improvements to the model in many areas: its understanding and generating text, perceiving and reasoning about visual content, supporting longer context lengths, understanding spatial relationships and dynamic videos, or interacting with AI agents â€” Qwen3-VL shows clear and significant progress in every area.\\n\\n### Models\\n\\n**2B**\\n\\n```\\nollama run qwen3-vl:2b\\n\\n```\\n\\n**4B**\\n\\n```\\nollama run qwen3-vl:4b\\n\\n```\\n\\n**8B**\\n\\n```\\nollama run qwen3-vl:8b\\n\\n```\\n\\n**30B**\\n\\n```\\nollama run qwen3-vl:30b\\n\\n```\\n\\n**32B**\\n\\n```\\nollama run qwen3-vl:32b\\n\\n```\\n\\n**235B**\\n\\n```\\nollama run qwen3-vl:235b\\n\\n```\\n\\n```\\nollama run qwen3-vl:235b-cloud\\n\\n```\\n\\n### Key features\\n\\n**Visual Agent Capabilities**: Qwen3-VL can operate computer and mobile interfaces â€” recognize GUI elements, understand button functions, call tools, and complete tasks. It achieves top global performance on benchmarks like OS World, and using tools significantly improves its performance on fine-grained perception tasks.\\n\\n**Superior Text-Centric Performance**: Qwen3-VL employs early-stage joint pretraining of text and visual modalities, continuously stre\",\n      \"image\": \"https://ollama.com/public/og.png\",\n      \"favicon\": \"https://ollama.com/public/icon-32x32.png\"\n    },\n    {\n      \"id\": \"https://github.com/QwenLM/Qwen3-VL\",\n      \"title\": \"QwenLM/Qwen3-VL - GitHub\",\n      \"url\": \"https://github.com/QwenLM/Qwen3-VL\",\n      \"publishedDate\": \"2024-08-29T08:30:38.000Z\",\n      \"author\": \"QwenLM\",\n      \"text\": \"[Skip to content](https://github.com/github.com#start-of-content)\\n\\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\\n\\n[QwenLM](https://github.com/QwenLM)/ **[Qwen3-VL](https://github.com/QwenLM/Qwen3-VL)** Public\\n\\n- [Notifications](https://github.com/login?return_to=%2FQwenLM%2FQwen3-VL) You must be signed in to change notification settings\\n- [Fork\\\\\\n1.3k](https://github.com/login?return_to=%2FQwenLM%2FQwen3-VL)\\n- [Star\\\\\\n16k](https://github.com/login?return_to=%2FQwenLM%2FQwen3-VL)\\n\\n\\nQwen3-VL is the multimodal large language model series developed by Qwen team, Alibaba Cloud.\\n\\n### License\\n\\n[Apache-2.0 license](https://github.com/QwenLM/Qwen3-VL/blob/main/LICENSE)\\n\\n[16k\\\\\\nstars](https://github.com/QwenLM/Qwen3-VL/stargazers) [1.3k\\\\\\nforks](https://github.com/QwenLM/Qwen3-VL/forks) [Branches](https://github.com/QwenLM/Qwen3-VL/branches) [Tags](https://github.com/QwenLM/Qwen3-VL/tags) [Activity](https://github.com/QwenLM/Qwen3-VL/activity)\\n\\n[Star](https://github.com/login?return_to=%2FQwenLM%2FQwen3-VL)\\n\\n[Notifications](https://github.com/login?return_to=%2FQwenLM%2FQwen3-VL) You must be signed in to change notification settings\\n\\n# QwenLM/Qwen3-VL\\n\\nmain\\n\\n[Branches](https://github.com/QwenLM/Qwen3-VL/branches) [Tags](https://github.com/QwenLM/Qwen3-VL/tags)\\n\\nGo to file\\n\\nCode\\n\\nOpen more actions menu\\n\\n## Folders and files\\n\\n| Name | Name | Last commit message | Last commit date |\\n| --- | --- | --- | --- |\\n| ## Latest commit ## History [282 Commits](https://github.com/QwenLM/Qwen3-VL/commits/main/) |\\n| [cookbooks](https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks) | [cookbooks](https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks) |\\n| [docker](https://github.com/QwenLM/Qwen3-VL/tree/main/docker) | [docker](https://github.com/QwenLM/Qwen3-VL/tree/main/docker) |\\n| [evaluation/mmmu](https\",\n      \"image\": \"https://opengraph.githubassets.com/886fbf7133fa2349286b517fecbdb0f0ea75afc449bcf412401ae223f8bada69/QwenLM/Qwen3-VL\",\n      \"favicon\": \"https://github.com/fluidicon.png\"\n    },\n    {\n      \"id\": \"https://lmstudio.ai/models/qwen/qwen3-vl-32b\",\n      \"title\": \"qwen/qwen3-vl-32b\",\n      \"url\": \"https://lmstudio.ai/models/qwen/qwen3-vl-32b\",\n      \"publishedDate\": \"2025-10-21T18:36:19.000Z\",\n      \"author\": \"\",\n      \"text\": \"1.6K Downloads\\n\\nModel\\n\\n1.6K Downloads\\n\\nFork\\n\\n0\\n\\nStar\\n\\n1\\n\\n# qwen3-vl-32b\\n\\nPublic\\n\\nFork\\n\\n0\\n\\nStar\\n\\n1\\n\\n[Use Model in LM Studio](https://lmstudio.ai/deeplink?owner=qwen&name=qwen3-vl-32b)\\n\\n[Model](https://lmstudio.ai/models/qwen/qwen3-vl-32b) [Revisions](https://lmstudio.ai/qwen/qwen3-vl-32b/revisions)\\n\\nUse cases\\n\\nVision Input\\n\\nTrained for tool use\\n\\nMinimum system memory\\n\\n20GB\\n\\nTags\\n\\n32B\\n\\nqwen3\\\\_vl\\n\\nLast update\\n\\n[Updated 1 day ago](https://lmstudio.ai/qwen/qwen3-vl-32b/revisions) by [lmmy](https://lmstudio.ai/lmmy)\\n\\nREADME\\n\\nFormattedRaw\\n\\n## Qwen3 VL 32B\\n\\nThe latest generation vision-language model in the Qwen series with comprehensive upgrades to visual perception and spatial reasoning.\\n\\n### Key Features\\n\\n- **Visual Agent**: Operates PC and mobile GUIsâ€”recognizes elements, understands functions, and completes tasks\\n- **Visual Coding**: Generates Draw.io, HTML, CSS, and JavaScript from images and videos\\n- **Advanced Spatial Perception**: Provides 2D/3D grounding for spatial reasoning and embodied AI applications\\n- **Upgraded Recognition**: Recognizes celebrities, anime, products, landmarks, flora, fauna, and more\\n- **Expanded OCR**: Supports 32 languages with robust performance in low light, blur, and tilt conditions\\n- **Pure Text Performance**: Text understanding on par with pure LLMs through seamless text-vision fusion\\n\\n### Architecture Highlights\\n\\n- 33B parameters\\n- Interleaved-MRoPE for enhanced video reasoning\\n- DeepStack for fine-grained detail capture\\n- Text-Timestamp Alignment for precise event localization\\n- Context length: 256,000 tokens\\n- Vision-enabled multimodal model\\n\\n### Performance\\n\\nDelivers superior vision-language performance across diverse tasks including document analysis, visual question answering, and agentic interactions.\\n\\nParameters\\n\\nCustom configuration options included with this model\\n\\nRepeat Penalty\\n\\nDisabled\\n\\nTemperature\\n\\n0.7\\n\\nTop K Sampling\\n\\n20\\n\\nTop P Sampling\\n\\n0.8\\n\\nSources\\n\\nThe underlying model files this model uses\\n\\nBased on\\n\\nðŸ¤— [lmstudio-co\",\n      \"image\": \"https://files.lmstudio.ai/6-model-link-og.png\",\n      \"favicon\": \"https://lmstudio.ai/_next/static/media/android-chrome-192x192.3a60873f.png\"\n    },\n    {\n      \"id\": \"https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct\",\n      \"title\": \"Qwen/Qwen2.5-VL-32B-Instruct - Hugging Face\",\n      \"url\": \"https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct\",\n      \"publishedDate\": \"2025-07-21T16:41:59.000Z\",\n      \"author\": \"\",\n      \"text\": \"# Qwen2.5-VL-32B-Instruct\\n\\n## Latest Updates:\\n\\nIn addition to the original formula, we have further enhanced Qwen2.5-VL-32B's mathematical and problem-solving abilities through reinforcement learning. This has also significantly improved the model's subjective user experience, with response styles adjusted to better align with human preferences. Particularly for objective queries such as mathematics, logical reasoning, and knowledge-based Q&A, the level of detail in responses and the clarity of formatting have been noticeably enhanced.\\n\\n## Introduction\\n\\nIn the past five months since Qwen2-VLâ€™s release, numerous developers have built new models on the Qwen2-VL vision-language models, providing us with valuable feedback. During this period, we focused on building more useful vision-language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5-VL.\\n\\n#### Key Enhancements:\\n\\n- **Understand things visually**: Qwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images.\\n\\n- **Being agentic**: Qwen2.5-VL directly plays as a visual agent that can reason and dynamically direct tools, which is capable of computer use and phone use.\\n\\n- **Understanding long videos and capturing events**: Qwen2.5-VL can comprehend videos of over 1 hour, and this time it has a new ability of cpaturing event by pinpointing the relevant video segments.\\n\\n- **Capable of visual localization in different formats**: Qwen2.5-VL can accurately localize objects in an image by generating bounding boxes or points, and it can provide stable JSON outputs for coordinates and attributes.\\n\\n- **Generating structured outputs**: for data like scans of invoices, forms, tables, etc. Qwen2.5-VL supports structured outputs of their contents, benefiting usages in finance, commerce, etc.\\n\\n\\n#### Model Architecture Updates:\\n\\n- **Dynamic Resolution a\",\n      \"image\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/Qwen/Qwen2.5-VL-32B-Instruct.png\"\n    }\n  ],\n  \"requestTags\": {\n    \"flagsEnabledInPosthog\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exa21,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache\",\n    \"flagsEnabledInRequest\": \"\",\n    \"enabledFlags\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exa21,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache\",\n    \"initialSearchType\": \"auto\",\n    \"didIncludeText\": \"no\",\n    \"didExcludeText\": \"no\",\n    \"didIncludeDomains\": \"no\",\n    \"didExcludeDomains\": \"no\",\n    \"didIncludeUrls\": \"no\",\n    \"didExcludeUrls\": \"no\",\n    \"moderation\": \"no\",\n    \"didUseDateFilters\": \"no\",\n    \"category\": \"none\",\n    \"didUseAutoprompt\": \"no\",\n    \"hasUserLocation\": \"no\",\n    \"didRequestContentText\": true,\n    \"didRequestContentHighlights\": false,\n    \"didRequestContentSummary\": false,\n    \"livecrawl\": \"preferred\",\n    \"requestedLinks\": \"no\",\n    \"willPostRerankNeural\": \"yes\",\n    \"willPostModerate\": false,\n    \"searchTypeBranch\": \"auto\",\n    \"autoBranch\": {\n      \"neuralSearch\": {\n        \"sierra\": {\n          \"query\": \"Qwen VL 32B model capabilities specifications vision language\",\n          \"flags\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache,use_exa21\",\n          \"filters\": {\n            \"moderation\": \"blacklisted\",\n            \"moderationConfig\": {\n              \"llamaguardS1\": false,\n              \"llamaguardS3\": false,\n              \"llamaguardS4\": false,\n              \"llamaguardS12\": false,\n              \"domainBlacklisted\": false,\n              \"domainBlacklistedMedia\": false\n            }\n          },\n          \"queryDateString\": \"2025-11-06\",\n          \"numResultsRequested\": 35,\n          \"engine\": \"default\",\n          \"isTweetCategory\": false,\n          \"sierraTime\": 314.39156299998285,\n          \"numResultsReturned\": 35\n        },\n        \"fetchContents\": {\n          \"contentExpansion\": {\n            \"failedContents\": []\n          },\n          \"contentsResultsCount\": 35,\n          \"filteredResultsCount\": 0\n        }\n      },\n      \"keywordSearch\": {\n        \"query\": \"Qwen VL 32B model capabilities specifications vision language\",\n        \"modifiedQuery\": \"Qwen VL 32B model capabilities specifications vision language\",\n        \"slicedAmount\": 0,\n        \"searchData\": {\n          \"q\": \"Qwen VL 32B model capabilities specifications vision language\",\n          \"num\": 10,\n          \"safe\": \"active\"\n        },\n        \"opts\": {\n          \"snippetMode\": \"snippet\",\n          \"addAnswerBox\": true,\n          \"addKnowledgeGraph\": true,\n          \"addPeopleAlsoAsk\": false,\n          \"allowEmptyAnswerLinks\": false,\n          \"providers\": [\n            {\n              \"provider\": \"serper\",\n              \"startAfterMs\": 0,\n              \"timeoutMs\": 10000\n            }\n          ],\n          \"timeoutMs\": 10000,\n          \"teamId\": \"cm60snyw00011cfcxf2vbwp8n\"\n        },\n        \"serperSource\": \"regular\",\n        \"relatedQueries\": [\n          \"Qwen vl 32b model capabilities specifications vision language download\",\n          \"Qwen vl 32b model capabilities specifications vision language free\",\n          \"Qwen3 vl 32b instruct\",\n          \"Qwen2 5 VL-32B-Instruct\",\n          \"Qwen2 5 VL-32B-Instruct vLLM\",\n          \"Qwen3-32B Instruct\",\n          \"Qwen2 5 vl 32b instruct vram\",\n          \"Qwen2 5 vl 32b instruct github\"\n        ],\n        \"success\": true,\n        \"numResultsRequested\": 5,\n        \"numResultsReturned\": 10,\n        \"fetchContents\": {\n          \"contentExpansion\": {\n            \"failedContents\": [\n              \"https://modelscope.cn/models/Qwen/Qwen3-VL-32B-Instruct-GGUF\"\n            ]\n          },\n          \"contentsResultsCount\": 9,\n          \"filteredResultsCount\": 0\n        }\n      },\n      \"neitherFailed\": true,\n      \"autoJoin\": true,\n      \"autoJoinRerank\": {\n        \"reranker\": {\n          \"withModeration\": false,\n          \"preserveUnranked\": true,\n          \"strategy\": {\n            \"strategy\": \"modal_sranker_qwen_trt\",\n            \"default\": true\n          },\n          \"strategy_decided\": \"modal_sranker_qwen_trt\",\n          \"strategy_reason\": \"explicit_strategy\",\n          \"isQdf\": false\n        }\n      },\n      \"autoJoinResults\": \"35\"\n    },\n    \"decidedAutoType\": \"neural\",\n    \"resolvedSearchType\": \"neural\",\n    \"autoType\": \"neural\",\n    \"decidedResultsLength\": 5,\n    \"totalFilterCount\": \"8\",\n    \"contextExpansionNeeded\": \"not-needed\",\n    \"finalResultsCount\": 5,\n    \"searchTime\": 4681.1,\n    \"path\": \"/search\",\n    \"method\": \"POST\",\n    \"teamId\": \"cm60snyw00011cfcxf2vbwp8n\"\n  },\n  \"effectiveFilters\": {\n    \"includeDomains\": [],\n    \"excludeDomains\": [],\n    \"includeUrls\": [],\n    \"excludeUrls\": [],\n    \"includeText\": [],\n    \"excludeText\": []\n  },\n  \"searchTime\": 4681.1,\n  \"costDollars\": {\n    \"total\": 0.01,\n    \"search\": {\n      \"neural\": 0.005\n    },\n    \"contents\": {\n      \"text\": 0.005\n    }\n  }\n}"}]}]},"uuid":"18a8ad58-1ba3-4d08-be2c-8c387c770811","timestamp":"2025-11-06T14:05:44.969Z","toolUseResult":[{"type":"text","text":"{\n  \"requestId\": \"b6bf184e95381cb1b94dd535b5a37d33\",\n  \"resolvedSearchType\": \"neural\",\n  \"results\": [\n    {\n      \"id\": \"https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct\",\n      \"title\": \"Qwen/Qwen3-VL-32B-Instruct - Hugging Face\",\n      \"url\": \"https://huggingface.co/Qwen/Qwen3-VL-32B-Instruct\",\n      \"publishedDate\": \"2025-11-01T14:44:32.000Z\",\n      \"author\": \"\",\n      \"text\": \"# Qwen3-VL-32B-Instruct\\n\\nMeet Qwen3-VL â€” the most powerful vision-language model in the Qwen series to date.\\n\\nThis generation delivers comprehensive upgrades across the board: superior text understanding & generation, deeper visual perception & reasoning, extended context length, enhanced spatial and video dynamics comprehension, and stronger agent interaction capabilities.\\n\\nAvailable in Dense and MoE architectures that scale from edge to cloud, with Instruct and reasoningâ€‘enhanced Thinking editions for flexible, onâ€‘demand deployment.\\n\\n#### Key Enhancements:\\n\\n- **Visual Agent**: Operates PC/mobile GUIsâ€”recognizes elements, understands functions, invokes tools, completes tasks.\\n\\n- **Visual Coding Boost**: Generates Draw.io/HTML/CSS/JS from images/videos.\\n\\n- **Advanced Spatial Perception**: Judges object positions, viewpoints, and occlusions; provides stronger 2D grounding and enables 3D grounding for spatial reasoning and embodied AI.\\n\\n- **Long Context & Video Understanding**: Native 256K context, expandable to 1M; handles books and hours-long video with full recall and second-level indexing.\\n\\n- **Enhanced Multimodal Reasoning**: Excels in STEM/Mathâ€”causal analysis and logical, evidence-based answers.\\n\\n- **Upgraded Visual Recognition**: Broader, higher-quality pretraining is able to â€œrecognize everythingâ€â€”celebrities, anime, products, landmarks, flora/fauna, etc.\\n\\n- **Expanded OCR**: Supports 32 languages (up from 19); robust in low light, blur, and tilt; better with rare/ancient characters and jargon; improved long-document structure parsing.\\n\\n- **Text Understanding on par with pure LLMs**: Seamless textâ€“vision fusion for lossless, unified comprehension.\\n\\n\\n#### Model Architecture Updates:\\n\\n1. **Interleaved-MRoPE**: Fullâ€‘frequency allocation over time, width, and height via robust positional embeddings, enhancing longâ€‘horizon video reasoning.\\n\\n2. **DeepStack**: Fuses multiâ€‘level ViT features to capture fineâ€‘grained details and sharpen imageâ€“text alignment.\\n\\n3. **Text\",\n      \"image\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/Qwen/Qwen3-VL-32B-Instruct.png\"\n    },\n    {\n      \"id\": \"https://ollama.com/library/qwen3-vl:32b\",\n      \"title\": \"qwen3-vl:32b - Ollama\",\n      \"url\": \"https://ollama.com/library/qwen3-vl:32b\",\n      \"publishedDate\": \"2025-10-29T23:24:00.000Z\",\n      \"author\": \"\",\n      \"text\": \"[qwen3-vl](https://ollama.com/library/qwen3-vl):32b\\n\\n61.9KÂ DownloadsUpdatedÂ 3 days ago\\n\\n## The most powerful vision-language model in the Qwen model family to date.\\n\\nCancel\\n\\nvisioncloud2b4b8b30b32b235b\\n\\nUpdated 4 days ago\\n\\n4 days ago\\n\\nff2e46876908 Â· 21GB Â·\\n\\n[model](https://ollama.com/library/qwen3-vl:32b/blobs/6e416d39200a)\\n\\narchqwen3vl\\n\\nÂ·\\n\\nparameters33.4B\\n\\nÂ·\\n\\nquantizationQ4\\\\_K\\\\_M\\n\\n21GB\\n\\n[license](https://ollama.com/library/qwen3-vl:32b/blobs/7339fa418c9a)\\n\\nApache License\\nVersion 2.0, January 2004\\nhttp://www.apache.org/licenses/\\nTERMS AND CONDITIONS FOR US\\n\\n11kB\\n\\n[params](https://ollama.com/library/qwen3-vl:32b/blobs/f6417cb1e269)\\n\\n{\\n\\\"temperature\\\": 1,\\n\\\"top\\\\_k\\\": 20,\\n\\\"top\\\\_p\\\": 0.95\\n}\\n\\n42B\\n\\n## Readme\\n\\n> Qwen3-VL models require [Ollama 0.12.7](https://github.com/ollama/ollama/releases)\\n\\nQwen3-VL is the most powerful vision-language model in the Qwen family to date.\\n\\nIn this generation, there are improvements to the model in many areas: its understanding and generating text, perceiving and reasoning about visual content, supporting longer context lengths, understanding spatial relationships and dynamic videos, or interacting with AI agents â€” Qwen3-VL shows clear and significant progress in every area.\\n\\n### Models\\n\\n**2B**\\n\\n```\\nollama run qwen3-vl:2b\\n\\n```\\n\\n**4B**\\n\\n```\\nollama run qwen3-vl:4b\\n\\n```\\n\\n**8B**\\n\\n```\\nollama run qwen3-vl:8b\\n\\n```\\n\\n**30B**\\n\\n```\\nollama run qwen3-vl:30b\\n\\n```\\n\\n**32B**\\n\\n```\\nollama run qwen3-vl:32b\\n\\n```\\n\\n**235B**\\n\\n```\\nollama run qwen3-vl:235b\\n\\n```\\n\\n```\\nollama run qwen3-vl:235b-cloud\\n\\n```\\n\\n### Key features\\n\\n**Visual Agent Capabilities**: Qwen3-VL can operate computer and mobile interfaces â€” recognize GUI elements, understand button functions, call tools, and complete tasks. It achieves top global performance on benchmarks like OS World, and using tools significantly improves its performance on fine-grained perception tasks.\\n\\n**Superior Text-Centric Performance**: Qwen3-VL employs early-stage joint pretraining of text and visual modalities, continuously stre\",\n      \"image\": \"https://ollama.com/public/og.png\",\n      \"favicon\": \"https://ollama.com/public/icon-32x32.png\"\n    },\n    {\n      \"id\": \"https://github.com/QwenLM/Qwen3-VL\",\n      \"title\": \"QwenLM/Qwen3-VL - GitHub\",\n      \"url\": \"https://github.com/QwenLM/Qwen3-VL\",\n      \"publishedDate\": \"2024-08-29T08:30:38.000Z\",\n      \"author\": \"QwenLM\",\n      \"text\": \"[Skip to content](https://github.com/github.com#start-of-content)\\n\\nYou signed in with another tab or window. Reload to refresh your session.You signed out in another tab or window. Reload to refresh your session.You switched accounts on another tab or window. Reload to refresh your session.Dismiss alert\\n\\n[QwenLM](https://github.com/QwenLM)/ **[Qwen3-VL](https://github.com/QwenLM/Qwen3-VL)** Public\\n\\n- [Notifications](https://github.com/login?return_to=%2FQwenLM%2FQwen3-VL) You must be signed in to change notification settings\\n- [Fork\\\\\\n1.3k](https://github.com/login?return_to=%2FQwenLM%2FQwen3-VL)\\n- [Star\\\\\\n16k](https://github.com/login?return_to=%2FQwenLM%2FQwen3-VL)\\n\\n\\nQwen3-VL is the multimodal large language model series developed by Qwen team, Alibaba Cloud.\\n\\n### License\\n\\n[Apache-2.0 license](https://github.com/QwenLM/Qwen3-VL/blob/main/LICENSE)\\n\\n[16k\\\\\\nstars](https://github.com/QwenLM/Qwen3-VL/stargazers) [1.3k\\\\\\nforks](https://github.com/QwenLM/Qwen3-VL/forks) [Branches](https://github.com/QwenLM/Qwen3-VL/branches) [Tags](https://github.com/QwenLM/Qwen3-VL/tags) [Activity](https://github.com/QwenLM/Qwen3-VL/activity)\\n\\n[Star](https://github.com/login?return_to=%2FQwenLM%2FQwen3-VL)\\n\\n[Notifications](https://github.com/login?return_to=%2FQwenLM%2FQwen3-VL) You must be signed in to change notification settings\\n\\n# QwenLM/Qwen3-VL\\n\\nmain\\n\\n[Branches](https://github.com/QwenLM/Qwen3-VL/branches) [Tags](https://github.com/QwenLM/Qwen3-VL/tags)\\n\\nGo to file\\n\\nCode\\n\\nOpen more actions menu\\n\\n## Folders and files\\n\\n| Name | Name | Last commit message | Last commit date |\\n| --- | --- | --- | --- |\\n| ## Latest commit ## History [282 Commits](https://github.com/QwenLM/Qwen3-VL/commits/main/) |\\n| [cookbooks](https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks) | [cookbooks](https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks) |\\n| [docker](https://github.com/QwenLM/Qwen3-VL/tree/main/docker) | [docker](https://github.com/QwenLM/Qwen3-VL/tree/main/docker) |\\n| [evaluation/mmmu](https\",\n      \"image\": \"https://opengraph.githubassets.com/886fbf7133fa2349286b517fecbdb0f0ea75afc449bcf412401ae223f8bada69/QwenLM/Qwen3-VL\",\n      \"favicon\": \"https://github.com/fluidicon.png\"\n    },\n    {\n      \"id\": \"https://lmstudio.ai/models/qwen/qwen3-vl-32b\",\n      \"title\": \"qwen/qwen3-vl-32b\",\n      \"url\": \"https://lmstudio.ai/models/qwen/qwen3-vl-32b\",\n      \"publishedDate\": \"2025-10-21T18:36:19.000Z\",\n      \"author\": \"\",\n      \"text\": \"1.6K Downloads\\n\\nModel\\n\\n1.6K Downloads\\n\\nFork\\n\\n0\\n\\nStar\\n\\n1\\n\\n# qwen3-vl-32b\\n\\nPublic\\n\\nFork\\n\\n0\\n\\nStar\\n\\n1\\n\\n[Use Model in LM Studio](https://lmstudio.ai/deeplink?owner=qwen&name=qwen3-vl-32b)\\n\\n[Model](https://lmstudio.ai/models/qwen/qwen3-vl-32b) [Revisions](https://lmstudio.ai/qwen/qwen3-vl-32b/revisions)\\n\\nUse cases\\n\\nVision Input\\n\\nTrained for tool use\\n\\nMinimum system memory\\n\\n20GB\\n\\nTags\\n\\n32B\\n\\nqwen3\\\\_vl\\n\\nLast update\\n\\n[Updated 1 day ago](https://lmstudio.ai/qwen/qwen3-vl-32b/revisions) by [lmmy](https://lmstudio.ai/lmmy)\\n\\nREADME\\n\\nFormattedRaw\\n\\n## Qwen3 VL 32B\\n\\nThe latest generation vision-language model in the Qwen series with comprehensive upgrades to visual perception and spatial reasoning.\\n\\n### Key Features\\n\\n- **Visual Agent**: Operates PC and mobile GUIsâ€”recognizes elements, understands functions, and completes tasks\\n- **Visual Coding**: Generates Draw.io, HTML, CSS, and JavaScript from images and videos\\n- **Advanced Spatial Perception**: Provides 2D/3D grounding for spatial reasoning and embodied AI applications\\n- **Upgraded Recognition**: Recognizes celebrities, anime, products, landmarks, flora, fauna, and more\\n- **Expanded OCR**: Supports 32 languages with robust performance in low light, blur, and tilt conditions\\n- **Pure Text Performance**: Text understanding on par with pure LLMs through seamless text-vision fusion\\n\\n### Architecture Highlights\\n\\n- 33B parameters\\n- Interleaved-MRoPE for enhanced video reasoning\\n- DeepStack for fine-grained detail capture\\n- Text-Timestamp Alignment for precise event localization\\n- Context length: 256,000 tokens\\n- Vision-enabled multimodal model\\n\\n### Performance\\n\\nDelivers superior vision-language performance across diverse tasks including document analysis, visual question answering, and agentic interactions.\\n\\nParameters\\n\\nCustom configuration options included with this model\\n\\nRepeat Penalty\\n\\nDisabled\\n\\nTemperature\\n\\n0.7\\n\\nTop K Sampling\\n\\n20\\n\\nTop P Sampling\\n\\n0.8\\n\\nSources\\n\\nThe underlying model files this model uses\\n\\nBased on\\n\\nðŸ¤— [lmstudio-co\",\n      \"image\": \"https://files.lmstudio.ai/6-model-link-og.png\",\n      \"favicon\": \"https://lmstudio.ai/_next/static/media/android-chrome-192x192.3a60873f.png\"\n    },\n    {\n      \"id\": \"https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct\",\n      \"title\": \"Qwen/Qwen2.5-VL-32B-Instruct - Hugging Face\",\n      \"url\": \"https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct\",\n      \"publishedDate\": \"2025-07-21T16:41:59.000Z\",\n      \"author\": \"\",\n      \"text\": \"# Qwen2.5-VL-32B-Instruct\\n\\n## Latest Updates:\\n\\nIn addition to the original formula, we have further enhanced Qwen2.5-VL-32B's mathematical and problem-solving abilities through reinforcement learning. This has also significantly improved the model's subjective user experience, with response styles adjusted to better align with human preferences. Particularly for objective queries such as mathematics, logical reasoning, and knowledge-based Q&A, the level of detail in responses and the clarity of formatting have been noticeably enhanced.\\n\\n## Introduction\\n\\nIn the past five months since Qwen2-VLâ€™s release, numerous developers have built new models on the Qwen2-VL vision-language models, providing us with valuable feedback. During this period, we focused on building more useful vision-language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5-VL.\\n\\n#### Key Enhancements:\\n\\n- **Understand things visually**: Qwen2.5-VL is not only proficient in recognizing common objects such as flowers, birds, fish, and insects, but it is highly capable of analyzing texts, charts, icons, graphics, and layouts within images.\\n\\n- **Being agentic**: Qwen2.5-VL directly plays as a visual agent that can reason and dynamically direct tools, which is capable of computer use and phone use.\\n\\n- **Understanding long videos and capturing events**: Qwen2.5-VL can comprehend videos of over 1 hour, and this time it has a new ability of cpaturing event by pinpointing the relevant video segments.\\n\\n- **Capable of visual localization in different formats**: Qwen2.5-VL can accurately localize objects in an image by generating bounding boxes or points, and it can provide stable JSON outputs for coordinates and attributes.\\n\\n- **Generating structured outputs**: for data like scans of invoices, forms, tables, etc. Qwen2.5-VL supports structured outputs of their contents, benefiting usages in finance, commerce, etc.\\n\\n\\n#### Model Architecture Updates:\\n\\n- **Dynamic Resolution a\",\n      \"image\": \"https://cdn-thumbnails.huggingface.co/social-thumbnails/models/Qwen/Qwen2.5-VL-32B-Instruct.png\"\n    }\n  ],\n  \"requestTags\": {\n    \"flagsEnabledInPosthog\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exa21,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache\",\n    \"flagsEnabledInRequest\": \"\",\n    \"enabledFlags\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exa21,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache\",\n    \"initialSearchType\": \"auto\",\n    \"didIncludeText\": \"no\",\n    \"didExcludeText\": \"no\",\n    \"didIncludeDomains\": \"no\",\n    \"didExcludeDomains\": \"no\",\n    \"didIncludeUrls\": \"no\",\n    \"didExcludeUrls\": \"no\",\n    \"moderation\": \"no\",\n    \"didUseDateFilters\": \"no\",\n    \"category\": \"none\",\n    \"didUseAutoprompt\": \"no\",\n    \"hasUserLocation\": \"no\",\n    \"didRequestContentText\": true,\n    \"didRequestContentHighlights\": false,\n    \"didRequestContentSummary\": false,\n    \"livecrawl\": \"preferred\",\n    \"requestedLinks\": \"no\",\n    \"willPostRerankNeural\": \"yes\",\n    \"willPostModerate\": false,\n    \"searchTypeBranch\": \"auto\",\n    \"autoBranch\": {\n      \"neuralSearch\": {\n        \"sierra\": {\n          \"query\": \"Qwen VL 32B model capabilities specifications vision language\",\n          \"flags\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache,use_exa21\",\n          \"filters\": {\n            \"moderation\": \"blacklisted\",\n            \"moderationConfig\": {\n              \"llamaguardS1\": false,\n              \"llamaguardS3\": false,\n              \"llamaguardS4\": false,\n              \"llamaguardS12\": false,\n              \"domainBlacklisted\": false,\n              \"domainBlacklistedMedia\": false\n            }\n          },\n          \"queryDateString\": \"2025-11-06\",\n          \"numResultsRequested\": 35,\n          \"engine\": \"default\",\n          \"isTweetCategory\": false,\n          \"sierraTime\": 314.39156299998285,\n          \"numResultsReturned\": 35\n        },\n        \"fetchContents\": {\n          \"contentExpansion\": {\n            \"failedContents\": []\n          },\n          \"contentsResultsCount\": 35,\n          \"filteredResultsCount\": 0\n        }\n      },\n      \"keywordSearch\": {\n        \"query\": \"Qwen VL 32B model capabilities specifications vision language\",\n        \"modifiedQuery\": \"Qwen VL 32B model capabilities specifications vision language\",\n        \"slicedAmount\": 0,\n        \"searchData\": {\n          \"q\": \"Qwen VL 32B model capabilities specifications vision language\",\n          \"num\": 10,\n          \"safe\": \"active\"\n        },\n        \"opts\": {\n          \"snippetMode\": \"snippet\",\n          \"addAnswerBox\": true,\n          \"addKnowledgeGraph\": true,\n          \"addPeopleAlsoAsk\": false,\n          \"allowEmptyAnswerLinks\": false,\n          \"providers\": [\n            {\n              \"provider\": \"serper\",\n              \"startAfterMs\": 0,\n              \"timeoutMs\": 10000\n            }\n          ],\n          \"timeoutMs\": 10000,\n          \"teamId\": \"cm60snyw00011cfcxf2vbwp8n\"\n        },\n        \"serperSource\": \"regular\",\n        \"relatedQueries\": [\n          \"Qwen vl 32b model capabilities specifications vision language download\",\n          \"Qwen vl 32b model capabilities specifications vision language free\",\n          \"Qwen3 vl 32b instruct\",\n          \"Qwen2 5 VL-32B-Instruct\",\n          \"Qwen2 5 VL-32B-Instruct vLLM\",\n          \"Qwen3-32B Instruct\",\n          \"Qwen2 5 vl 32b instruct vram\",\n          \"Qwen2 5 vl 32b instruct github\"\n        ],\n        \"success\": true,\n        \"numResultsRequested\": 5,\n        \"numResultsReturned\": 10,\n        \"fetchContents\": {\n          \"contentExpansion\": {\n            \"failedContents\": [\n              \"https://modelscope.cn/models/Qwen/Qwen3-VL-32B-Instruct-GGUF\"\n            ]\n          },\n          \"contentsResultsCount\": 9,\n          \"filteredResultsCount\": 0\n        }\n      },\n      \"neitherFailed\": true,\n      \"autoJoin\": true,\n      \"autoJoinRerank\": {\n        \"reranker\": {\n          \"withModeration\": false,\n          \"preserveUnranked\": true,\n          \"strategy\": {\n            \"strategy\": \"modal_sranker_qwen_trt\",\n            \"default\": true\n          },\n          \"strategy_decided\": \"modal_sranker_qwen_trt\",\n          \"strategy_reason\": \"explicit_strategy\",\n          \"isQdf\": false\n        }\n      },\n      \"autoJoinResults\": \"35\"\n    },\n    \"decidedAutoType\": \"neural\",\n    \"resolvedSearchType\": \"neural\",\n    \"autoType\": \"neural\",\n    \"decidedResultsLength\": 5,\n    \"totalFilterCount\": \"8\",\n    \"contextExpansionNeeded\": \"not-needed\",\n    \"finalResultsCount\": 5,\n    \"searchTime\": 4681.1,\n    \"path\": \"/search\",\n    \"method\": \"POST\",\n    \"teamId\": \"cm60snyw00011cfcxf2vbwp8n\"\n  },\n  \"effectiveFilters\": {\n    \"includeDomains\": [],\n    \"excludeDomains\": [],\n    \"includeUrls\": [],\n    \"excludeUrls\": [],\n    \"includeText\": [],\n    \"excludeText\": []\n  },\n  \"searchTime\": 4681.1,\n  \"costDollars\": {\n    \"total\": 0.01,\n    \"search\": {\n      \"neural\": 0.005\n    },\n    \"contents\": {\n      \"text\": 0.005\n    }\n  }\n}"}]}
{"parentUuid":"18a8ad58-1ba3-4d08-be2c-8c387c770811","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01EJ7pxYnVkDf6DthGtvxbTs","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"Great! I've got some good information from the search results. Let me synthesize what I found:\n\n1. From the first search, I found a comparison between Claude Haiku 4.5 and Qwen2.5 VL 32B\n2. I also found information about Qwen3-VL-32B which seems to be the latest version\n\nKey points from the searches:\n\n**Claude models:**\n- Claude Sonnet 4.5 - flagship model (that's me!)\n- Claude Haiku 4.5 - faster, cheaper model released Oct 15, 2025\n- Both have strong capabilities, 200K context for Haiku\n\n**Qwen VL 32B:**\n- There are two versions: Qwen2.5-VL-32B and Qwen3-VL-32B\n- 33B parameters (Qwen3-VL)\n- 16.4K context for Qwen2.5-VL, 256K native for Qwen3-VL\n- Open source (available on HuggingFace)\n- Vision-language model with strong visual capabilities\n- Much cheaper (Claude Haiku is 20-22x more expensive according to the search)\n\nLet me provide a comprehensive comparison based on this information.","signature":"Eq0ICkYICRgCKkDQJtXaJmgkwyCZLzeFj/KTSxV32QggUE8t1k70ej4C0/VCv5oO9SwD5pLmyYPrIS6RoUrcG/HxHFfASYlMyWzHEgxnjva00hgU/CRzHLsaDBXCZJLQCsxyg+WBmSIwxrN2RIBR/J4TUacBm4P7RANrgJJHRgUk0rowgYUFcpAeR/frhEnPzQxhyGiHsESgKpQHJ/9Y2i0RNbeQpo/vU+bQsA06cyX89s3hMPZQdkNTOVqXWI9O+7M3VDoPOG8wqi8rW3RaSvs26GYI6QgewNsTPDB2mZwpm/Ln9E41ZjAC5xlqzfrF6jofHqw+kyz6ettZy3I2t7ZPu9BqssXelgYxXVLOyga4388WPmXctZZqO5MYq+/8ujHfIMjTBiTbiTzsET+ccjjSZvXN9d08gfwy3b1CsUeD8ex+a+1HHb7B7rQCFl+5b/cpbzA1EtJjQYGN/0/i8QhEgX6dkA7u3iyGZ87lSf2kkCSOo5rLsVbadrcwJwDn0dUUGz4M/G2FBKJo3E2FiG5z0bHtArCy2MtmAz1dddlsJZ5MWhggaP034143aD3laHD4lyLppqkBd3iBtcH7XBs3rp25fl9J5WcLG3ICCC831KZKBY1jfZsaYd8Q5YPaKB3XtvDV8MUhEhYncGiii9/rDB5aQGsukIiiTXRC+v6f1QiGHTOxPzhDHQoL5loma09fpI1xdPoN3wQqlJImm0+pnn1HLHKDfP/IvzDNjBCvoL85LKcNMAuVMV/mBKj9I/TXbYnoF2/OjGv1XyUanYjP0mpPhAl3HzFzFs9/VpMT2DIzXYN1YLEL+GelJUT8jwT7IrDuMcxiW2FOutMa4ezY9GB3WR06AFroZumsJprUV7uySFKw6LdXB+OtO1wrpBU0boO0aqjraTTaf91Eh3oGpVtXzPiq0/Iv38JRu/7ltXeqY9b0sWYOblDclvK+7oT7VfVGjwumjDYQfl8QT7gv21uGpBsQoIbWmespcSJ2MK8M5aB65j0YhMRzTdbFm8wTog7ffCViM3QDpkSJh7OWsg6UPYe5Ehnrl/w2AbL3VMKxto6OL6wK/Lbcxn9E965muOqWysOAQ6LIf1r9iaoFwAMmJLiSaaelqVlGSX/sdmqaoaaXImNLufD+Go/UxnKJEkUkRhrq1A7LjkM3z0p/JyeYGLztSU2bDJR3zstJnYJCI2s6CYg03Od2dTmxjCYRDpZ78mLA7CCogO5F+idvfPL1Ph3NGg/Pjk353gi311nOJ5P68INz7/8lkJPq7re3oNG+C0EUaVySpxKuoiQmT/rXnDrJ1KAuZVSrVCVKmLh8gcBcXNlAvwsPftIfK9aYHIDIW1U4carxJuE4vT95PXjXHcfFl/JlZEcjUNoovTG18WcbZ6kOmrbLolcF8i/Uxv8KlCFBkIrlpAX0zxgB"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":12,"cache_creation_input_tokens":13815,"cache_read_input_tokens":33336,"cache_creation":{"ephemeral_5m_input_tokens":13815,"ephemeral_1h_input_tokens":0},"output_tokens":2,"service_tier":"standard"}},"requestId":"req_011CUrmFYZww5gWJNJCht4js","type":"assistant","uuid":"f2d7608d-78ef-406f-b01a-d9504ab6ac5c","timestamp":"2025-11-06T14:05:55.421Z"}
{"parentUuid":"f2d7608d-78ef-406f-b01a-d9504ab6ac5c","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01EJ7pxYnVkDf6DthGtvxbTs","type":"message","role":"assistant","content":[{"type":"text","text":"Based on the search results, here's a comparison between Claude models (Sonnet 4.5 and Haiku 4.5) and Qwen VL 32B:\n\n## Key Differences\n\n### **Model Architecture**\n- **Claude Sonnet 4.5** (me): Flagship model, most capable\n- **Claude Haiku 4.5**: Faster, more economical Claude variant (released Oct 15, 2025)\n- **Qwen VL 32B**: Open-source vision-language model (~33B parameters)\n\n### **Context Length**\n- **Claude Haiku 4.5**: 200K tokens input, 64K output\n- **Claude Sonnet 4.5**: Similar large context\n- **Qwen2.5-VL-32B**: 16.4K tokens\n- **Qwen3-VL-32B**: 256K tokens (expandable to 1M)\n\n### **Pricing**\n- **Claude Haiku 4.5**: $1.00/M input tokens, higher for output\n- **Qwen2.5-VL-32B**: ~$0.05/M input tokens (20-22x cheaper)\n- **Qwen3-VL**: Self-hostable (open source) - no API costs\n\n### **Strengths by Model**\n\n**Claude Sonnet 4.5:**\n- Superior reasoning and complex task performance\n- Function calling, structured outputs\n- Strong coding and analysis capabilities\n- Proprietary, commercial-grade reliability\n\n**Claude Haiku 4.5:**\n- Fast response times\n- Cost-effective for production workloads\n- Good for multi-step software tasks\n- Same safety features as Sonnet\n\n**Qwen3-VL-32B:**\n- **Visual agent capabilities**: Can operate PC/mobile GUIs\n- **Visual coding**: Generates HTML/CSS/JS from images\n- **Advanced spatial perception**: 2D/3D grounding\n- **OCR excellence**: 32 languages, robust in challenging conditions\n- **Long video understanding**: 1+ hour videos with second-level indexing\n- **Open source**: Self-hostable, customizable\n\n### **When to Use Each**\n\n- **Claude Sonnet 4.5**: Complex reasoning, critical tasks, best overall quality\n- **Claude Haiku 4.5**: High-volume production, fast responses needed, cost-sensitive\n- **Qwen VL 32B**: Visual tasks, GUI automation, self-hosting requirements, budget constraints, or when you need open-source flexibility\n\nThe main tradeoff is Claude's superior general reasoning vs Qwen's specialized vision capabilities and cost/deployment flexibility."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":12,"cache_creation_input_tokens":13815,"cache_read_input_tokens":33336,"cache_creation":{"ephemeral_5m_input_tokens":13815,"ephemeral_1h_input_tokens":0},"output_tokens":925,"service_tier":"standard"}},"requestId":"req_011CUrmFYZww5gWJNJCht4js","type":"assistant","uuid":"ddc65ed1-fb0c-439e-aa23-5b587a095dab","timestamp":"2025-11-06T14:06:07.535Z"}
{"type":"file-history-snapshot","messageId":"97b5716d-b7f7-40ca-993e-4370084b90f8","snapshot":{"messageId":"97b5716d-b7f7-40ca-993e-4370084b90f8","trackedFileBackups":{},"timestamp":"2025-11-06T14:11:37.352Z"},"isSnapshotUpdate":false}
{"parentUuid":"ddc65ed1-fb0c-439e-aa23-5b587a095dab","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","type":"user","message":{"role":"user","content":"I want to USE my machine for inference running ollama or vllm. I have a choice between a Mac Studio with 128G or AMD macine with 128G. Is Rocm a viable choice."},"uuid":"97b5716d-b7f7-40ca-993e-4370084b90f8","timestamp":"2025-11-06T14:11:37.348Z","thinkingMetadata":{"level":"high","disabled":false,"triggers":[]}}
{"parentUuid":"97b5716d-b7f7-40ca-993e-4370084b90f8","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01YH5753amK7EQUJNFL97zYT","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"The user is asking about hardware choices for running LLM inference locally. They're comparing:\n1. Mac Studio with 128GB (unified memory)\n2. AMD machine with 128GB (presumably RAM + GPU memory via ROCm)\n\nThey want to know if ROCm is viable for running models via ollama or vLLM.\n\nThis is a practical hardware/software question about local inference. I should search for current information about:\n- ROCm viability and compatibility in 2025\n- Mac Studio vs AMD for LLM inference\n- Ollama and vLLM support on both platforms\n- Real-world experiences with ROCm for inference\n\nLet me search for current information about this.","signature":"EpcGCkYICRgCKkCZCW9NACXz/LBIQ2CEc4ytnm/aPxkLE/ORVyiYJOd8muJ2mQx7lOoCkhLIgXDJcaQ4HN/nmmo/cRm3ZZQPjbI5Egyq0exKwhmTlBQTQakaDEFOZ1Qr8LbtjR7qoCIwTJqCyOollhdUux897mks0jJ7x0CLD8Bu59F8k+6w2z8IgUmvcovVNTyElgcRPK7rKv4ErZ17dCwYC3P7uovm0nRgMq3rDiTxnmFv/PeCv1MHMj2xHUMde+OFb0snwUsQctdfhK8C3jj5I5kVkofsHQ/mLQiyWf/pcj5OWvmjNk2+c4VzIAfqRldWWSc6+ubzWwK9iDHQWLL0F3IoTMjEXI2SMX4m1I6Gmqq4iz0CPxW321TpdAFtsueYnBtl8JX2GFampdRVxKydd/lPWsMIf+76t78uxkUfHyeUn9cuUmomHLRGmBW2jZuVznlMP0vaxorYNEFWYfrHaR69hQvtvQkfUKBTjqlZWAkQ/JV4vOWOaLVb+9Smv9b/0Moc5/5La7KV9PJhRQKVv/zfQH6gCwXboEYth29f+/QsRDqQxdziJYp1CSzkhtgHAzT93PGNmi1tHxCVnQJmLX0ZF+KtRwcoB4PtPDcojoYX3EAZrAnsS78AwOKZjXNhiRP5SB2/NXoJ1FIewHLtrv9gIhBewebylev3d6TfsKuQdPQHq61sFlzZnf2qFAGkXMBIxmtAenMqZexk8Pd7ZL6QTVanOCZhhYE9sxmvRIq1Y6NVDAB+X/5L2KlMv3IK3MUkHKs7d+F5AMjAqXUJfTWGLcLlb1MM2KJyqV7hYEdNG0NRIggu89BWzrQOrZ1oER8/qtXKSjkClXWXLfarEAktUNtq3EsEBZe0s9/D7J9ZFtJGLZno5gcZsXHCCHporW4VcL0jVuTH5cZJMa5L48MndrBCKlz6q0SiszHaWzyJOrdcU37KmNrc57RhTHBVrzjgMlSGYTeHIZkYYrnNd2QClq8R7CHCVhaY+Ylt5FR5vrqREB+FOpyfr9AljUb89NTPIsQcKXvb1NaNKZmkgJchQcXDrAUYAQ=="}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":47541,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":47541,"ephemeral_1h_input_tokens":0},"output_tokens":2,"service_tier":"standard"}},"requestId":"req_011CUrmhXVgZs7Bb8rsvHne4","type":"assistant","uuid":"f7eeb18a-6ce1-48d8-9f8f-bef5e695b79b","timestamp":"2025-11-06T14:11:44.401Z"}
{"parentUuid":"f7eeb18a-6ce1-48d8-9f8f-bef5e695b79b","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01YH5753amK7EQUJNFL97zYT","type":"message","role":"assistant","content":[{"type":"text","text":"I'll search for current information about ROCm viability and Mac vs AMD performance for local LLM inference."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":47541,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":47541,"ephemeral_1h_input_tokens":0},"output_tokens":2,"service_tier":"standard"}},"requestId":"req_011CUrmhXVgZs7Bb8rsvHne4","type":"assistant","uuid":"7b0dab02-8f00-4e8b-aa3c-e83b41fefbff","timestamp":"2025-11-06T14:11:45.654Z"}
{"parentUuid":"7b0dab02-8f00-4e8b-aa3c-e83b41fefbff","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01YH5753amK7EQUJNFL97zYT","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01SG7XvHxWurkJjaB16woJiD","name":"mcp__plugin_exa-mcp-server_exa__web_search_exa","input":{"query":"ROCm LLM inference 2025 vLLM ollama AMD viability production","numResults":5}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":47541,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":47541,"ephemeral_1h_input_tokens":0},"output_tokens":2,"service_tier":"standard"}},"requestId":"req_011CUrmhXVgZs7Bb8rsvHne4","type":"assistant","uuid":"e0dbd033-15b0-4abf-9ea1-fb18a0d7f655","timestamp":"2025-11-06T14:11:46.775Z"}
{"parentUuid":"e0dbd033-15b0-4abf-9ea1-fb18a0d7f655","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01YH5753amK7EQUJNFL97zYT","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01QZga158MQCVVcnHzZAd6yx","name":"mcp__plugin_exa-mcp-server_exa__web_search_exa","input":{"query":"Mac Studio M2 Ultra 128GB vs AMD ROCm LLM inference performance comparison","numResults":5}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":47541,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":47541,"ephemeral_1h_input_tokens":0},"output_tokens":2,"service_tier":"standard"}},"requestId":"req_011CUrmhXVgZs7Bb8rsvHne4","type":"assistant","uuid":"a2981d76-7893-4957-95d2-2d5b6db3143f","timestamp":"2025-11-06T14:11:47.774Z"}
{"parentUuid":"a2981d76-7893-4957-95d2-2d5b6db3143f","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01YH5753amK7EQUJNFL97zYT","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01DpxNkiWgByXmKAszEQ5zAV","name":"mcp__plugin_exa-mcp-server_exa__web_search_exa","input":{"query":"ollama vLLM Mac M2 Ultra vs AMD GPU support compatibility 2025","numResults":5}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":47541,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":47541,"ephemeral_1h_input_tokens":0},"output_tokens":483,"service_tier":"standard"}},"requestId":"req_011CUrmhXVgZs7Bb8rsvHne4","type":"assistant","uuid":"30d48f5b-ee12-4c36-ac6f-801008e15327","timestamp":"2025-11-06T14:11:48.489Z"}
{"parentUuid":"30d48f5b-ee12-4c36-ac6f-801008e15327","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01SG7XvHxWurkJjaB16woJiD","type":"tool_result","content":[{"type":"text","text":"{\n  \"requestId\": \"d9e1fccd41eef58eb9be8043c4221e78\",\n  \"resolvedSearchType\": \"neural\",\n  \"results\": [\n    {\n      \"id\": \"https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference-optimization/vllm-optimization.html\",\n      \"title\": \"vLLM V1 performance optimization - AMD ROCm documentation\",\n      \"url\": \"https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference-optimization/vllm-optimization.html\",\n      \"publishedDate\": \"2025-11-05T00:00:00.000Z\",\n      \"author\": \"\",\n      \"text\": \"[Skip to main content](https://rocm.docs.amd.com/rocm.docs.amd.com#main-content)\\n\\nBack to top`Ctrl` + `K`\\n\\n[ROCm Documentation](https://rocm.docs.amd.com/index.html)\\n\\nSearch`Ctrl` + `K`\\n\\n# vLLM V1 performance optimization\\n\\n## Contents\\n\\n# vLLM V1 performance optimization [\\\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\\\#vllm-v1-performance-optimization)\\n\\n2025-10-22\\n\\n31 min read time\\n\\nApplies to Linux and Windows\\n\\nThis guide helps you maximize vLLM throughput and minimize latency on AMD\\nInstinct MI300X, MI325X, MI350X, and MI355X GPUs. Learn how to:\\n\\n- Enable AITER (AI Tensor Engine for ROCm) for speedups on LLM models.\\n\\n- Configure environment variables for optimal HIP, RCCL, and Quick Reduce performance.\\n\\n- Select the right attention backend for your workload (AITER MHA/MLA vs. Triton).\\n\\n- Choose parallelism strategies (tensor, pipeline, data, expert) for multi-GPU deployments.\\n\\n- Apply quantization ( `FP8`/ `FP4`) to reduce memory usage by 2-4Ã— with minimal accuracy loss.\\n\\n- Tune engine arguments (batch size, memory utilization, graph modes) for your use case.\\n\\n- Benchmark and scale across single-node and multi-node configurations.\\n\\n\\n## Performance environment variables [\\\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\\\#performance-environment-variables)\\n\\nThe following variables are generally useful for Instinct MI300X/MI355X GPUs and vLLM:\\n\\n- **HIP and math libraries**\\n\\n - `export HIP_FORCE_DEV_KERNARG=1` â€” improves kernel launch performance by\\n forcing device kernel arguments. This is already set by default in\\n [vLLM ROCm Docker images](https://rocm.docs.amd.com/inference/benchmark-docker/vllm.html). Bare-metal users\\n should set this manually.\\n\\n - `export TORCH_BLAS_PREFER_HIPBLASLT=1` â€” explicitly prefers hipBLASLt\\n over hipBLAS for GEMM operations. By default, PyTorch uses heuristics to\\n choose the best BLAS library. Setting this can improve linear layer\\n performance in some workloads.\\n- **RCCL (collectives for multi-GPU)**\\n\\n - `export NCCL_MIN_NCHANNELS=112` â€” \",\n      \"favicon\": \"https://www.amd.com/content/dam/code/images/favicon/favicon.ico\"\n    },\n    {\n      \"id\": \"https://developers.redhat.com/articles/2025/08/08/ollama-vs-vllm-deep-dive-performance-benchmarking\",\n      \"title\": \"Ollama vs. vLLM: A deep dive into performance benchmarking\",\n      \"url\": \"https://developers.redhat.com/articles/2025/08/08/ollama-vs-vllm-deep-dive-performance-benchmarking\",\n      \"publishedDate\": \"2025-08-08T00:00:00.000Z\",\n      \"author\": \"Harshith Umesh\",\n      \"text\": \"[Skip to main content](https://developers.redhat.com/developers.redhat.com#main-content)\\n\\nSearchSearch\\n\\nAll Red Hat\\n\\n# Ollama vs. vLLM: A deep dive into performance benchmarking\\n\\nAugust 8, 2025\\n\\n[Harshith Umesh](https://developers.redhat.com/author/harshith-umesh)\\n\\nRelated topics:[Artificial intelligence](https://developers.redhat.com/topics/ai-ml) [Open source](https://developers.redhat.com/topics/open-source-communities)Related products:[Red Hat AI](https://developers.redhat.com/taxonomy/term/37288)\\n\\nTable of contents:\\n\\nKey takeaways\\n\\n- Ollama and vLLM serve different purposes, and that's a good thing for the AI community: Ollama is ideal for local development and prototyping, while vLLM is built for high-performance production deployments.\\n- vLLM outperforms Ollama at scale: vLLM delivers significantly higher throughput (achieving a peak of **793 TPS** compared to Ollama's **41 TPS**) and lower P99 latency ( **80 ms** vs. **673 ms** at peak throughput). vLLM delivers higher throughput and lower latency across all concurrency levels(1-256 concurrent users), even when Ollama is tuned for parallelism.\\n- Ollama prioritizes simplicity, vLLM prioritizes scalability: Ollama keeps things lightweight for single users, while vLLM dynamically scales to handle large, concurrent workloads efficiently.\\n\\nIn [Ollama or vLLM? How to choose the right LLM serving tool for your use case](https://developers.redhat.com/articles/2025/07/08/ollama-or-vllm-how-choose-right-llm-serving-tool-your-use-case), we introduced Ollama as a good tool for local development and [vLLM](https://www.redhat.com/en/topics/ai/what-is-vllm) as the go-to solution for high-performance production serving. We argued that while Ollama prioritizes ease of use for individual developers, vLLM is engineered for the scalability and throughput that enterprise applications demand.\\n\\nNow, it's time to move from theory to practice. In this follow-up article, we'll put these two inference engines to the test in a head-to-\",\n      \"image\": \"https://developers.redhat.com/sites/default/files/styles/share/public/Inference-vLLM-1920x1080.png?itok=x_9NzXpT\",\n      \"favicon\": \"https://developers.redhat.com/themes/custom/rhdp_fe/favicons/favicon-32x32.png\"\n    },\n    {\n      \"id\": \"https://rocm.blogs.amd.com/software-tools-optimization/vllmv1-rocm-llm/README.html\",\n      \"title\": \"vLLM V1 Meets AMD Instinct GPUs: A New Era for LLM Inference ...\",\n      \"url\": \"https://rocm.blogs.amd.com/software-tools-optimization/vllmv1-rocm-llm/README.html\",\n      \"publishedDate\": \"2025-07-07T00:00:00.000Z\",\n      \"author\": \"Seungrok Jung, Hyukjoon Lee, Andy Luo\",\n      \"text\": \"[Skip to main content](https://rocm.blogs.amd.com/rocm.blogs.amd.com#main-content)\\nBack to top\\n\\n`Ctrl` + `K`\\n\\n[ROCm blogs](https://rocm.blogs.amd.com/)\\n\\n# vLLM V1 Meets AMD Instinct GPUs: A New Era for LLM Inference Performance\\n\\n## Contents\\n\\n# vLLM V1 Meets AMD Instinct GPUs: A New Era for LLM Inference Performance [\\\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\\\#vllm-v1-meets-amd-instinct-gpus-a-new-era-for-llm-inference-performance)\\n\\nJuly 07, 2025 by [Seungrok Jung](https://rocm.blogs.amd.com/authors/seungrok-jung.html), Hyukjoon Lee, [Andy Luo](https://rocm.blogs.amd.com/authors/andy-luo.html).\\n\\n3 min read. \\\\| 732 total words.\\n\\n[Software tools & optimizations](https://rocm.blogs.amd.com/blog/category/software-tools-optimizations.html)\\n\\n[AI/ML](https://rocm.blogs.amd.com/blog/tag/aiml.html)\\n\\n[AI](https://rocm.blogs.amd.com/ai.html)\\n\\n[vLLM](https://github.com/vllm-project/vllm) has been a successful LLM\\ninference and serving engine that excels at providing innovative\\nfeatures to users and developers. Earlier this year, the vLLM community\\n[introduced](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html)\\na major upgrade of its core engine and architecture to\\n[vLLM](https://docs.vllm.ai/en/latest/usage/v1_guide.html) V1 (V1),\\nwhich enhances the flexibility and scalability of the engine while\\nretaining its core features. For simplicity, weâ€™ll refer to vLLM V0 as\\nâ€œv0â€ and vLLM V1 as â€œV1â€ throughout this post. To align with the vLLM\\ncommunityâ€™s continuous innovation, the AMD ROCmâ„¢ software\\nteam and open-source ROCm developers have enabled the fully optimized\\nvLLM V1 engine on AMD GPUs.\\n\\nIn this blog, we explore key improvements introduced in vLLM V1 for AMD\\nGPUs, as well as the main benefits users can expect from migrating to\\nthis new version.\\n\\n## Overview of Architectural Improvements in vLLM V1 [\\\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\\\#overview-of-architectural-improvements-in-vllm-v1)\\n\\nThe new V1 engine has multiple improvements that include:\\n\\n1. Optimi\",\n      \"image\": \"https://rocm.blogs.amd.com/_images/vllmv1-rocm-llm.webp\",\n      \"favicon\": \"https://www.amd.com/content/dam/code/images/favicon/favicon.ico\"\n    },\n    {\n      \"id\": \"https://rocm.blogs.amd.com/software-tools-optimization/vllm-0.9.x-rocm/README.html\",\n      \"title\": \"Accelerated LLM Inference on AMD Instinctâ„¢ GPUs with vLLM 0.9.x and ROCm\",\n      \"url\": \"https://rocm.blogs.amd.com/software-tools-optimization/vllm-0.9.x-rocm/README.html\",\n      \"publishedDate\": \"2025-06-28T00:00:00.000Z\",\n      \"author\": \"Hongxia Yang, Peng Sun, Tun Jian Tan, Pin Siang Tan, Anshul Gupta\",\n      \"text\": \"[Skip to main content](https://rocm.blogs.amd.com/rocm.blogs.amd.com#main-content)\\nBack to top\\n\\n`Ctrl` + `K`\\n\\n[ROCm blogs](https://rocm.blogs.amd.com/)\\n\\n# Accelerated LLM Inference on AMD Instinctâ„¢ GPUs with vLLM 0.9.x and ROCm\\n\\n## Contents\\n\\n# Accelerated LLM Inference on AMD Instinctâ„¢ GPUs with vLLM 0.9.x and ROCm [\\\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\\\#accelerated-llm-inference-on-amd-instinct-gpus-with-vllm-0-9-x-and-rocm)\\n\\nJune 28, 2025 by [Hongxia Yang](https://rocm.blogs.amd.com/authors/hongxia-yang.html), Peng Sun, Tun Jian Tan, Pin Siang Tan, [Anshul Gupta](https://rocm.blogs.amd.com/authors/anshul-gupta.html).\\n\\n8 min read. \\\\| 1923 total words.\\n\\n[Software tools & optimizations](https://rocm.blogs.amd.com/blog/category/software-tools-optimizations.html)\\n\\n[LLM](https://rocm.blogs.amd.com/blog/tag/llm.html), [GenAI](https://rocm.blogs.amd.com/blog/tag/genai.html), [Performance](https://rocm.blogs.amd.com/blog/tag/performance.html)\\n\\n[AI](https://rocm.blogs.amd.com/ai.html)\\n\\nAMD is pleased to announce the release of vLLM 0.9.x, delivering significant advances in LLM inference performance through ROCmâ„¢ software and AITER integration. This release\\nprovides a variety of powerful optimizations and exciting new\\ncapabilities to the AMD ROCm software ecosystem as shown in Figure 1,\\nbelow. Whether you are a developer or a researcher, this release is\\ndesigned to help you unlock new levels of performance and explore wider\\nmodel support on AMD Instinctâ„¢ GPUs.\\n\\nFor anyone deploying LLMs in real-world scenarios, every millisecond of\\nlatency whether itâ€™s time to first token (TTFT), time per output token (TPOT),\\nor overall throughput plays a crucial role in providing a seamless\\nuser experience and operational efficiency. In this blog, we will discuss how vLLM 0.9.x, with its deep integration with ROCm software, offers a range of powerful enhancements designed to meet these performance-critical needs.\\n\\nFigure 1. Key Enhancements in vLLM 0.9.x for AMD ROCm [#](https:/\",\n      \"image\": \"https://rocm.blogs.amd.com/_images/tt.webp\",\n      \"favicon\": \"https://www.amd.com/content/dam/code/images/favicon/favicon.ico\"\n    },\n    {\n      \"id\": \"https://northflank.com/blog/vllm-vs-ollama-and-how-to-run-them\",\n      \"title\": \"vLLM vs Ollama: Key differences, performance, and how to run them\",\n      \"url\": \"https://northflank.com/blog/vllm-vs-ollama-and-how-to-run-them\",\n      \"publishedDate\": \"2025-09-12T12:45:00.000Z\",\n      \"author\": \"\",\n      \"text\": \"[Guides](https://northflank.com/guides) [Changelog](https://northflank.com/changelog) [Blog](https://northflank.com/blog)\\n\\n[â† Back to Blog](https://northflank.com/blog)\\n\\nBy [Daniel Adeboye](https://northflank.com/author/daniel-adeboye)\\n\\nPublished 12th September 2025\\n\\n# vLLM vs Ollama: Key differences, performance, and how to run them\\n\\n[AI](https://northflank.com/blog/tag/ai)\\n\\nLarge language models are no longer just research toys. They power customer support, code assistants, knowledge retrieval, and even production systems that handle millions of requests a day.\\n\\nBut if you have ever tried to run one yourself, you know that serving these models is not trivial. You need to balance latency, throughput, memory, cost, and developer experience. That is why two open-source projects, vLLM and Ollama, have quickly become popular choices for various types of teams.\\n\\nAt first glance, they may look similar. Both let you run and interact with large language models locally or in the cloud. But under the hood, they approach the problem from different angles. Understanding these differences can save you a lot of time and money when choosing the right tool for your stack.\\n\\nIn this article, we will compare vLLM and Ollama side by side, look at their strengths and trade-offs, and explore where each shines. By the end, you will not only know which one suits your needs but also how to deploy either on [Northflank](https://northflank.com/) to get them running in production quickly.\\n\\n## TL;DR: vllm vs ollama at a glance\\n\\nIf you are short on time, here is a high-level snapshot of how they compare.\\n\\n| Feature | vLLM | Ollama |\\n| --- | --- | --- |\\n| Focus | High-performance LLM inference | Developer-friendly local model runner |\\n| Architecture | PagedAttention + optimized GPU scheduling | Lightweight runtime + simple model packaging |\\n| Performance | Industry-leading throughput and low latency | Optimized for quick setup and ease of use |\\n| Supported models | Hugging Face models | Curated \",\n      \"image\": \"https://assets.northflank.com/what_is_gpu_as_a_service_1_f184fabce4.png\"\n    }\n  ],\n  \"requestTags\": {\n    \"flagsEnabledInPosthog\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exa21,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache\",\n    \"flagsEnabledInRequest\": \"\",\n    \"enabledFlags\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exa21,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache\",\n    \"initialSearchType\": \"auto\",\n    \"didIncludeText\": \"no\",\n    \"didExcludeText\": \"no\",\n    \"didIncludeDomains\": \"no\",\n    \"didExcludeDomains\": \"no\",\n    \"didIncludeUrls\": \"no\",\n    \"didExcludeUrls\": \"no\",\n    \"moderation\": \"no\",\n    \"didUseDateFilters\": \"no\",\n    \"category\": \"none\",\n    \"didUseAutoprompt\": \"no\",\n    \"hasUserLocation\": \"no\",\n    \"didRequestContentText\": true,\n    \"didRequestContentHighlights\": false,\n    \"didRequestContentSummary\": false,\n    \"livecrawl\": \"preferred\",\n    \"requestedLinks\": \"no\",\n    \"willPostRerankNeural\": \"yes\",\n    \"willPostModerate\": false,\n    \"searchTypeBranch\": \"auto\",\n    \"autoBranch\": {\n      \"neuralSearch\": {\n        \"sierra\": {\n          \"query\": \"ROCm LLM inference 2025 vLLM ollama AMD viability production\",\n          \"flags\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache,use_exa21\",\n          \"filters\": {\n            \"moderation\": \"blacklisted\",\n            \"moderationConfig\": {\n              \"llamaguardS1\": false,\n              \"llamaguardS3\": false,\n              \"llamaguardS4\": false,\n              \"llamaguardS12\": false,\n              \"domainBlacklisted\": false,\n              \"domainBlacklistedMedia\": false\n            }\n          },\n          \"queryDateString\": \"2025-11-06\",\n          \"numResultsRequested\": 35,\n          \"engine\": \"default\",\n          \"isTweetCategory\": false,\n          \"sierraTime\": 414.0378339999588,\n          \"numResultsReturned\": 35\n        },\n        \"fetchContents\": {\n          \"contentExpansion\": {\n            \"failedContents\": []\n          },\n          \"contentsResultsCount\": 35,\n          \"filteredResultsCount\": 0\n        }\n      },\n      \"keywordSearch\": {\n        \"query\": \"ROCm LLM inference 2025 vLLM ollama AMD viability production\",\n        \"modifiedQuery\": \"ROCm LLM inference 2025 vLLM ollama AMD viability production\",\n        \"slicedAmount\": 0,\n        \"searchData\": {\n          \"q\": \"ROCm LLM inference 2025 vLLM ollama AMD viability production\",\n          \"num\": 10,\n          \"safe\": \"active\"\n        },\n        \"opts\": {\n          \"snippetMode\": \"snippet\",\n          \"addAnswerBox\": true,\n          \"addKnowledgeGraph\": true,\n          \"addPeopleAlsoAsk\": false,\n          \"allowEmptyAnswerLinks\": false,\n          \"providers\": [\n            {\n              \"provider\": \"serper\",\n              \"startAfterMs\": 0,\n              \"timeoutMs\": 10000\n            }\n          ],\n          \"timeoutMs\": 10000,\n          \"teamId\": \"cm60snyw00011cfcxf2vbwp8n\"\n        },\n        \"serperSource\": \"regular\",\n        \"peopleAlsoAsk\": [\n          \"Which is better, vLLM or Ollama?\",\n          \"Why is vLLM faster?\"\n        ],\n        \"relatedQueries\": [\n          \"Rocm/vLLM GitHub\",\n          \"Rocm/vllm-dev\",\n          \"Rocm/vllm Docker\",\n          \"Ollama vs vLLM performance\",\n          \"Ollama vs vLLM vs SGLang\",\n          \"vLLM vs Ollama\",\n          \"Ollama vs vLLM vs llama Cpp\",\n          \"Ollama GitHub\"\n        ],\n        \"success\": true,\n        \"numResultsRequested\": 5,\n        \"numResultsReturned\": 10,\n        \"fetchContents\": {\n          \"contentExpansion\": {\n            \"failedContents\": [\n              \"https://www.reddit.com/r/ROCm/comments/1l1qo19/vllm_090_is_here_unleashing_huge_performance_on/\",\n              \"https://www.reddit.com/r/LocalLLaMA/comments/1nufu17/amd_tested_20_local_models_for_coding_only_2/\",\n              \"https://www.youtube.com/watch?v=LyrY0O6G8kI\"\n            ]\n          },\n          \"contentsResultsCount\": 7,\n          \"filteredResultsCount\": 0\n        }\n      },\n      \"neitherFailed\": true,\n      \"autoJoin\": true,\n      \"autoJoinRerank\": {\n        \"reranker\": {\n          \"withModeration\": false,\n          \"preserveUnranked\": true,\n          \"strategy\": {\n            \"strategy\": \"modal_sranker_qwen_trt\",\n            \"default\": true\n          },\n          \"strategy_decided\": \"modal_sranker_qwen_trt\",\n          \"strategy_reason\": \"explicit_strategy\",\n          \"isQdf\": false\n        }\n      },\n      \"autoJoinResults\": \"35\"\n    },\n    \"decidedAutoType\": \"neural\",\n    \"resolvedSearchType\": \"neural\",\n    \"autoType\": \"neural\",\n    \"decidedResultsLength\": 5,\n    \"totalFilterCount\": \"9\",\n    \"contextExpansionNeeded\": \"not-needed\",\n    \"finalResultsCount\": 5,\n    \"searchTime\": 4919.9,\n    \"path\": \"/search\",\n    \"method\": \"POST\",\n    \"teamId\": \"cm60snyw00011cfcxf2vbwp8n\"\n  },\n  \"effectiveFilters\": {\n    \"includeDomains\": [],\n    \"excludeDomains\": [],\n    \"includeUrls\": [],\n    \"excludeUrls\": [],\n    \"includeText\": [],\n    \"excludeText\": []\n  },\n  \"searchTime\": 4919.9,\n  \"costDollars\": {\n    \"total\": 0.01,\n    \"search\": {\n      \"neural\": 0.005\n    },\n    \"contents\": {\n      \"text\": 0.005\n    }\n  }\n}"}]}]},"uuid":"e584c465-2e65-411c-99bb-4d126d6d4733","timestamp":"2025-11-06T14:11:54.540Z","toolUseResult":[{"type":"text","text":"{\n  \"requestId\": \"d9e1fccd41eef58eb9be8043c4221e78\",\n  \"resolvedSearchType\": \"neural\",\n  \"results\": [\n    {\n      \"id\": \"https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference-optimization/vllm-optimization.html\",\n      \"title\": \"vLLM V1 performance optimization - AMD ROCm documentation\",\n      \"url\": \"https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference-optimization/vllm-optimization.html\",\n      \"publishedDate\": \"2025-11-05T00:00:00.000Z\",\n      \"author\": \"\",\n      \"text\": \"[Skip to main content](https://rocm.docs.amd.com/rocm.docs.amd.com#main-content)\\n\\nBack to top`Ctrl` + `K`\\n\\n[ROCm Documentation](https://rocm.docs.amd.com/index.html)\\n\\nSearch`Ctrl` + `K`\\n\\n# vLLM V1 performance optimization\\n\\n## Contents\\n\\n# vLLM V1 performance optimization [\\\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\\\#vllm-v1-performance-optimization)\\n\\n2025-10-22\\n\\n31 min read time\\n\\nApplies to Linux and Windows\\n\\nThis guide helps you maximize vLLM throughput and minimize latency on AMD\\nInstinct MI300X, MI325X, MI350X, and MI355X GPUs. Learn how to:\\n\\n- Enable AITER (AI Tensor Engine for ROCm) for speedups on LLM models.\\n\\n- Configure environment variables for optimal HIP, RCCL, and Quick Reduce performance.\\n\\n- Select the right attention backend for your workload (AITER MHA/MLA vs. Triton).\\n\\n- Choose parallelism strategies (tensor, pipeline, data, expert) for multi-GPU deployments.\\n\\n- Apply quantization ( `FP8`/ `FP4`) to reduce memory usage by 2-4Ã— with minimal accuracy loss.\\n\\n- Tune engine arguments (batch size, memory utilization, graph modes) for your use case.\\n\\n- Benchmark and scale across single-node and multi-node configurations.\\n\\n\\n## Performance environment variables [\\\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\\\#performance-environment-variables)\\n\\nThe following variables are generally useful for Instinct MI300X/MI355X GPUs and vLLM:\\n\\n- **HIP and math libraries**\\n\\n - `export HIP_FORCE_DEV_KERNARG=1` â€” improves kernel launch performance by\\n forcing device kernel arguments. This is already set by default in\\n [vLLM ROCm Docker images](https://rocm.docs.amd.com/inference/benchmark-docker/vllm.html). Bare-metal users\\n should set this manually.\\n\\n - `export TORCH_BLAS_PREFER_HIPBLASLT=1` â€” explicitly prefers hipBLASLt\\n over hipBLAS for GEMM operations. By default, PyTorch uses heuristics to\\n choose the best BLAS library. Setting this can improve linear layer\\n performance in some workloads.\\n- **RCCL (collectives for multi-GPU)**\\n\\n - `export NCCL_MIN_NCHANNELS=112` â€” \",\n      \"favicon\": \"https://www.amd.com/content/dam/code/images/favicon/favicon.ico\"\n    },\n    {\n      \"id\": \"https://developers.redhat.com/articles/2025/08/08/ollama-vs-vllm-deep-dive-performance-benchmarking\",\n      \"title\": \"Ollama vs. vLLM: A deep dive into performance benchmarking\",\n      \"url\": \"https://developers.redhat.com/articles/2025/08/08/ollama-vs-vllm-deep-dive-performance-benchmarking\",\n      \"publishedDate\": \"2025-08-08T00:00:00.000Z\",\n      \"author\": \"Harshith Umesh\",\n      \"text\": \"[Skip to main content](https://developers.redhat.com/developers.redhat.com#main-content)\\n\\nSearchSearch\\n\\nAll Red Hat\\n\\n# Ollama vs. vLLM: A deep dive into performance benchmarking\\n\\nAugust 8, 2025\\n\\n[Harshith Umesh](https://developers.redhat.com/author/harshith-umesh)\\n\\nRelated topics:[Artificial intelligence](https://developers.redhat.com/topics/ai-ml) [Open source](https://developers.redhat.com/topics/open-source-communities)Related products:[Red Hat AI](https://developers.redhat.com/taxonomy/term/37288)\\n\\nTable of contents:\\n\\nKey takeaways\\n\\n- Ollama and vLLM serve different purposes, and that's a good thing for the AI community: Ollama is ideal for local development and prototyping, while vLLM is built for high-performance production deployments.\\n- vLLM outperforms Ollama at scale: vLLM delivers significantly higher throughput (achieving a peak of **793 TPS** compared to Ollama's **41 TPS**) and lower P99 latency ( **80 ms** vs. **673 ms** at peak throughput). vLLM delivers higher throughput and lower latency across all concurrency levels(1-256 concurrent users), even when Ollama is tuned for parallelism.\\n- Ollama prioritizes simplicity, vLLM prioritizes scalability: Ollama keeps things lightweight for single users, while vLLM dynamically scales to handle large, concurrent workloads efficiently.\\n\\nIn [Ollama or vLLM? How to choose the right LLM serving tool for your use case](https://developers.redhat.com/articles/2025/07/08/ollama-or-vllm-how-choose-right-llm-serving-tool-your-use-case), we introduced Ollama as a good tool for local development and [vLLM](https://www.redhat.com/en/topics/ai/what-is-vllm) as the go-to solution for high-performance production serving. We argued that while Ollama prioritizes ease of use for individual developers, vLLM is engineered for the scalability and throughput that enterprise applications demand.\\n\\nNow, it's time to move from theory to practice. In this follow-up article, we'll put these two inference engines to the test in a head-to-\",\n      \"image\": \"https://developers.redhat.com/sites/default/files/styles/share/public/Inference-vLLM-1920x1080.png?itok=x_9NzXpT\",\n      \"favicon\": \"https://developers.redhat.com/themes/custom/rhdp_fe/favicons/favicon-32x32.png\"\n    },\n    {\n      \"id\": \"https://rocm.blogs.amd.com/software-tools-optimization/vllmv1-rocm-llm/README.html\",\n      \"title\": \"vLLM V1 Meets AMD Instinct GPUs: A New Era for LLM Inference ...\",\n      \"url\": \"https://rocm.blogs.amd.com/software-tools-optimization/vllmv1-rocm-llm/README.html\",\n      \"publishedDate\": \"2025-07-07T00:00:00.000Z\",\n      \"author\": \"Seungrok Jung, Hyukjoon Lee, Andy Luo\",\n      \"text\": \"[Skip to main content](https://rocm.blogs.amd.com/rocm.blogs.amd.com#main-content)\\nBack to top\\n\\n`Ctrl` + `K`\\n\\n[ROCm blogs](https://rocm.blogs.amd.com/)\\n\\n# vLLM V1 Meets AMD Instinct GPUs: A New Era for LLM Inference Performance\\n\\n## Contents\\n\\n# vLLM V1 Meets AMD Instinct GPUs: A New Era for LLM Inference Performance [\\\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\\\#vllm-v1-meets-amd-instinct-gpus-a-new-era-for-llm-inference-performance)\\n\\nJuly 07, 2025 by [Seungrok Jung](https://rocm.blogs.amd.com/authors/seungrok-jung.html), Hyukjoon Lee, [Andy Luo](https://rocm.blogs.amd.com/authors/andy-luo.html).\\n\\n3 min read. \\\\| 732 total words.\\n\\n[Software tools & optimizations](https://rocm.blogs.amd.com/blog/category/software-tools-optimizations.html)\\n\\n[AI/ML](https://rocm.blogs.amd.com/blog/tag/aiml.html)\\n\\n[AI](https://rocm.blogs.amd.com/ai.html)\\n\\n[vLLM](https://github.com/vllm-project/vllm) has been a successful LLM\\ninference and serving engine that excels at providing innovative\\nfeatures to users and developers. Earlier this year, the vLLM community\\n[introduced](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html)\\na major upgrade of its core engine and architecture to\\n[vLLM](https://docs.vllm.ai/en/latest/usage/v1_guide.html) V1 (V1),\\nwhich enhances the flexibility and scalability of the engine while\\nretaining its core features. For simplicity, weâ€™ll refer to vLLM V0 as\\nâ€œv0â€ and vLLM V1 as â€œV1â€ throughout this post. To align with the vLLM\\ncommunityâ€™s continuous innovation, the AMD ROCmâ„¢ software\\nteam and open-source ROCm developers have enabled the fully optimized\\nvLLM V1 engine on AMD GPUs.\\n\\nIn this blog, we explore key improvements introduced in vLLM V1 for AMD\\nGPUs, as well as the main benefits users can expect from migrating to\\nthis new version.\\n\\n## Overview of Architectural Improvements in vLLM V1 [\\\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\\\#overview-of-architectural-improvements-in-vllm-v1)\\n\\nThe new V1 engine has multiple improvements that include:\\n\\n1. Optimi\",\n      \"image\": \"https://rocm.blogs.amd.com/_images/vllmv1-rocm-llm.webp\",\n      \"favicon\": \"https://www.amd.com/content/dam/code/images/favicon/favicon.ico\"\n    },\n    {\n      \"id\": \"https://rocm.blogs.amd.com/software-tools-optimization/vllm-0.9.x-rocm/README.html\",\n      \"title\": \"Accelerated LLM Inference on AMD Instinctâ„¢ GPUs with vLLM 0.9.x and ROCm\",\n      \"url\": \"https://rocm.blogs.amd.com/software-tools-optimization/vllm-0.9.x-rocm/README.html\",\n      \"publishedDate\": \"2025-06-28T00:00:00.000Z\",\n      \"author\": \"Hongxia Yang, Peng Sun, Tun Jian Tan, Pin Siang Tan, Anshul Gupta\",\n      \"text\": \"[Skip to main content](https://rocm.blogs.amd.com/rocm.blogs.amd.com#main-content)\\nBack to top\\n\\n`Ctrl` + `K`\\n\\n[ROCm blogs](https://rocm.blogs.amd.com/)\\n\\n# Accelerated LLM Inference on AMD Instinctâ„¢ GPUs with vLLM 0.9.x and ROCm\\n\\n## Contents\\n\\n# Accelerated LLM Inference on AMD Instinctâ„¢ GPUs with vLLM 0.9.x and ROCm [\\\\#](https://rocm.blogs.amd.com/rocm.blogs.amd.com\\\\#accelerated-llm-inference-on-amd-instinct-gpus-with-vllm-0-9-x-and-rocm)\\n\\nJune 28, 2025 by [Hongxia Yang](https://rocm.blogs.amd.com/authors/hongxia-yang.html), Peng Sun, Tun Jian Tan, Pin Siang Tan, [Anshul Gupta](https://rocm.blogs.amd.com/authors/anshul-gupta.html).\\n\\n8 min read. \\\\| 1923 total words.\\n\\n[Software tools & optimizations](https://rocm.blogs.amd.com/blog/category/software-tools-optimizations.html)\\n\\n[LLM](https://rocm.blogs.amd.com/blog/tag/llm.html), [GenAI](https://rocm.blogs.amd.com/blog/tag/genai.html), [Performance](https://rocm.blogs.amd.com/blog/tag/performance.html)\\n\\n[AI](https://rocm.blogs.amd.com/ai.html)\\n\\nAMD is pleased to announce the release of vLLM 0.9.x, delivering significant advances in LLM inference performance through ROCmâ„¢ software and AITER integration. This release\\nprovides a variety of powerful optimizations and exciting new\\ncapabilities to the AMD ROCm software ecosystem as shown in Figure 1,\\nbelow. Whether you are a developer or a researcher, this release is\\ndesigned to help you unlock new levels of performance and explore wider\\nmodel support on AMD Instinctâ„¢ GPUs.\\n\\nFor anyone deploying LLMs in real-world scenarios, every millisecond of\\nlatency whether itâ€™s time to first token (TTFT), time per output token (TPOT),\\nor overall throughput plays a crucial role in providing a seamless\\nuser experience and operational efficiency. In this blog, we will discuss how vLLM 0.9.x, with its deep integration with ROCm software, offers a range of powerful enhancements designed to meet these performance-critical needs.\\n\\nFigure 1. Key Enhancements in vLLM 0.9.x for AMD ROCm [#](https:/\",\n      \"image\": \"https://rocm.blogs.amd.com/_images/tt.webp\",\n      \"favicon\": \"https://www.amd.com/content/dam/code/images/favicon/favicon.ico\"\n    },\n    {\n      \"id\": \"https://northflank.com/blog/vllm-vs-ollama-and-how-to-run-them\",\n      \"title\": \"vLLM vs Ollama: Key differences, performance, and how to run them\",\n      \"url\": \"https://northflank.com/blog/vllm-vs-ollama-and-how-to-run-them\",\n      \"publishedDate\": \"2025-09-12T12:45:00.000Z\",\n      \"author\": \"\",\n      \"text\": \"[Guides](https://northflank.com/guides) [Changelog](https://northflank.com/changelog) [Blog](https://northflank.com/blog)\\n\\n[â† Back to Blog](https://northflank.com/blog)\\n\\nBy [Daniel Adeboye](https://northflank.com/author/daniel-adeboye)\\n\\nPublished 12th September 2025\\n\\n# vLLM vs Ollama: Key differences, performance, and how to run them\\n\\n[AI](https://northflank.com/blog/tag/ai)\\n\\nLarge language models are no longer just research toys. They power customer support, code assistants, knowledge retrieval, and even production systems that handle millions of requests a day.\\n\\nBut if you have ever tried to run one yourself, you know that serving these models is not trivial. You need to balance latency, throughput, memory, cost, and developer experience. That is why two open-source projects, vLLM and Ollama, have quickly become popular choices for various types of teams.\\n\\nAt first glance, they may look similar. Both let you run and interact with large language models locally or in the cloud. But under the hood, they approach the problem from different angles. Understanding these differences can save you a lot of time and money when choosing the right tool for your stack.\\n\\nIn this article, we will compare vLLM and Ollama side by side, look at their strengths and trade-offs, and explore where each shines. By the end, you will not only know which one suits your needs but also how to deploy either on [Northflank](https://northflank.com/) to get them running in production quickly.\\n\\n## TL;DR: vllm vs ollama at a glance\\n\\nIf you are short on time, here is a high-level snapshot of how they compare.\\n\\n| Feature | vLLM | Ollama |\\n| --- | --- | --- |\\n| Focus | High-performance LLM inference | Developer-friendly local model runner |\\n| Architecture | PagedAttention + optimized GPU scheduling | Lightweight runtime + simple model packaging |\\n| Performance | Industry-leading throughput and low latency | Optimized for quick setup and ease of use |\\n| Supported models | Hugging Face models | Curated \",\n      \"image\": \"https://assets.northflank.com/what_is_gpu_as_a_service_1_f184fabce4.png\"\n    }\n  ],\n  \"requestTags\": {\n    \"flagsEnabledInPosthog\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exa21,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache\",\n    \"flagsEnabledInRequest\": \"\",\n    \"enabledFlags\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exa21,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache\",\n    \"initialSearchType\": \"auto\",\n    \"didIncludeText\": \"no\",\n    \"didExcludeText\": \"no\",\n    \"didIncludeDomains\": \"no\",\n    \"didExcludeDomains\": \"no\",\n    \"didIncludeUrls\": \"no\",\n    \"didExcludeUrls\": \"no\",\n    \"moderation\": \"no\",\n    \"didUseDateFilters\": \"no\",\n    \"category\": \"none\",\n    \"didUseAutoprompt\": \"no\",\n    \"hasUserLocation\": \"no\",\n    \"didRequestContentText\": true,\n    \"didRequestContentHighlights\": false,\n    \"didRequestContentSummary\": false,\n    \"livecrawl\": \"preferred\",\n    \"requestedLinks\": \"no\",\n    \"willPostRerankNeural\": \"yes\",\n    \"willPostModerate\": false,\n    \"searchTypeBranch\": \"auto\",\n    \"autoBranch\": {\n      \"neuralSearch\": {\n        \"sierra\": {\n          \"query\": \"ROCm LLM inference 2025 vLLM ollama AMD viability production\",\n          \"flags\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache,use_exa21\",\n          \"filters\": {\n            \"moderation\": \"blacklisted\",\n            \"moderationConfig\": {\n              \"llamaguardS1\": false,\n              \"llamaguardS3\": false,\n              \"llamaguardS4\": false,\n              \"llamaguardS12\": false,\n              \"domainBlacklisted\": false,\n              \"domainBlacklistedMedia\": false\n            }\n          },\n          \"queryDateString\": \"2025-11-06\",\n          \"numResultsRequested\": 35,\n          \"engine\": \"default\",\n          \"isTweetCategory\": false,\n          \"sierraTime\": 414.0378339999588,\n          \"numResultsReturned\": 35\n        },\n        \"fetchContents\": {\n          \"contentExpansion\": {\n            \"failedContents\": []\n          },\n          \"contentsResultsCount\": 35,\n          \"filteredResultsCount\": 0\n        }\n      },\n      \"keywordSearch\": {\n        \"query\": \"ROCm LLM inference 2025 vLLM ollama AMD viability production\",\n        \"modifiedQuery\": \"ROCm LLM inference 2025 vLLM ollama AMD viability production\",\n        \"slicedAmount\": 0,\n        \"searchData\": {\n          \"q\": \"ROCm LLM inference 2025 vLLM ollama AMD viability production\",\n          \"num\": 10,\n          \"safe\": \"active\"\n        },\n        \"opts\": {\n          \"snippetMode\": \"snippet\",\n          \"addAnswerBox\": true,\n          \"addKnowledgeGraph\": true,\n          \"addPeopleAlsoAsk\": false,\n          \"allowEmptyAnswerLinks\": false,\n          \"providers\": [\n            {\n              \"provider\": \"serper\",\n              \"startAfterMs\": 0,\n              \"timeoutMs\": 10000\n            }\n          ],\n          \"timeoutMs\": 10000,\n          \"teamId\": \"cm60snyw00011cfcxf2vbwp8n\"\n        },\n        \"serperSource\": \"regular\",\n        \"peopleAlsoAsk\": [\n          \"Which is better, vLLM or Ollama?\",\n          \"Why is vLLM faster?\"\n        ],\n        \"relatedQueries\": [\n          \"Rocm/vLLM GitHub\",\n          \"Rocm/vllm-dev\",\n          \"Rocm/vllm Docker\",\n          \"Ollama vs vLLM performance\",\n          \"Ollama vs vLLM vs SGLang\",\n          \"vLLM vs Ollama\",\n          \"Ollama vs vLLM vs llama Cpp\",\n          \"Ollama GitHub\"\n        ],\n        \"success\": true,\n        \"numResultsRequested\": 5,\n        \"numResultsReturned\": 10,\n        \"fetchContents\": {\n          \"contentExpansion\": {\n            \"failedContents\": [\n              \"https://www.reddit.com/r/ROCm/comments/1l1qo19/vllm_090_is_here_unleashing_huge_performance_on/\",\n              \"https://www.reddit.com/r/LocalLLaMA/comments/1nufu17/amd_tested_20_local_models_for_coding_only_2/\",\n              \"https://www.youtube.com/watch?v=LyrY0O6G8kI\"\n            ]\n          },\n          \"contentsResultsCount\": 7,\n          \"filteredResultsCount\": 0\n        }\n      },\n      \"neitherFailed\": true,\n      \"autoJoin\": true,\n      \"autoJoinRerank\": {\n        \"reranker\": {\n          \"withModeration\": false,\n          \"preserveUnranked\": true,\n          \"strategy\": {\n            \"strategy\": \"modal_sranker_qwen_trt\",\n            \"default\": true\n          },\n          \"strategy_decided\": \"modal_sranker_qwen_trt\",\n          \"strategy_reason\": \"explicit_strategy\",\n          \"isQdf\": false\n        }\n      },\n      \"autoJoinResults\": \"35\"\n    },\n    \"decidedAutoType\": \"neural\",\n    \"resolvedSearchType\": \"neural\",\n    \"autoType\": \"neural\",\n    \"decidedResultsLength\": 5,\n    \"totalFilterCount\": \"9\",\n    \"contextExpansionNeeded\": \"not-needed\",\n    \"finalResultsCount\": 5,\n    \"searchTime\": 4919.9,\n    \"path\": \"/search\",\n    \"method\": \"POST\",\n    \"teamId\": \"cm60snyw00011cfcxf2vbwp8n\"\n  },\n  \"effectiveFilters\": {\n    \"includeDomains\": [],\n    \"excludeDomains\": [],\n    \"includeUrls\": [],\n    \"excludeUrls\": [],\n    \"includeText\": [],\n    \"excludeText\": []\n  },\n  \"searchTime\": 4919.9,\n  \"costDollars\": {\n    \"total\": 0.01,\n    \"search\": {\n      \"neural\": 0.005\n    },\n    \"contents\": {\n      \"text\": 0.005\n    }\n  }\n}"}]}
{"parentUuid":"e584c465-2e65-411c-99bb-4d126d6d4733","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01QZga158MQCVVcnHzZAd6yx","type":"tool_result","content":[{"type":"text","text":"{\n  \"requestId\": \"7e521af51cbb388353782863752fca75\",\n  \"resolvedSearchType\": \"neural\",\n  \"results\": [\n    {\n      \"id\": \"https://www.amd.com/en/developer/resources/rocm-hub/dev-ai/performance-results.html\",\n      \"title\": \"Performance Results with AMD ROCmâ„¢ Software\",\n      \"url\": \"https://www.amd.com/en/developer/resources/rocm-hub/dev-ai/performance-results.html\",\n      \"publishedDate\": \"2025-10-31T16:21:22.000Z\",\n      \"author\": \"\",\n      \"text\": \"[Skip to main content](https://www.amd.com/www.amd.com)\\n\\n[AMD Website Accessibility Statement](https://www.amd.com/en/site-notifications/accessibility-statement.html)\\n\\n# Performance Results with AMD ROCmâ„¢ Software\\n\\nThis page summarizes performance measurements on AMD Instinctâ„¢ GPUs for popular AI models.\\n\\nThe data in the following tables is a reference point to help users evaluate observed performance. It should not be considered as the peak performance that AMD GPUs and ROCmâ„¢ software can deliver.\\n\\n1. AI Inference\\n2. AI Training\\n\\n## AI Inference\\n\\n#### Throughput Measurements\\n\\nThe table below shows performance data where a local inference client is fed requests at an infinite rate and shows the throughput client-server scenario under maximum load.\\n\\nThis result is based on the Docker container (rocm/vllm:rocm7.0.0\\\\_vllm\\\\_0.10.2\\\\_20251006), which was released on October 6, 2025.\\n\\n| | | | | | | | |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n| **Model** | **Precision** | **TP Size** | **Input** | **Output** | **Num Prompts** | **Max Num Seqs** | **Throughput (tokens/s)** |\\n| Llama 3.1 70B (amd/Llama-3.1-70B-Instruct-FP8-KV) | FP8 | 8 | 128 | 2048 | 3200 | 3200 | 13212.5 |\\n| 128 | 4096 | 1500 | 1500 | 11312.8 |\\n| 500 | 2000 | 2000 | 2000 | 11376.7 |\\n| 2048 | 2048 | 1500 | 1500 | 7252.1 |\\n| Llama 3.1 405B (amd/Llama-3.1-405B-Instruct-FP8-KV) | FP8 | 8 | 128 | 2048 | 1500 | 1500 | 4201.7 |\\n| 128 | 4096 | 1500 | 1500 | 3176.3 |\\n| 500 | 2000 | 2000 | 2000 | 2992.0 |\\n| 2048 | 2048 | 500 | 500 | 2153.7 |\\n\\nTP stands for Tensor Parallelism.\\n\\nServer: Dual AMD EPYC 9554 64-core processor-based production server with 8x AMD MI300X (192GB HBM3 750W) GPUs, 1 NUMA node per socket, System BIOS 1.8,Â UbuntuÂ® 22.04, amdgpu driver 6.8.5\\n\\nReproduce these results on your system by following the instructions in [measuring inference performance with vLLM on the AMD GPUs user guide](https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference/vllm-benchmark.html?model=pyt_vllm_llama-\",\n      \"image\": \"https://www.amd.com/content/dam/amd/en/images/logos/products/2058250-amd-rocm.jpg\",\n      \"favicon\": \"https://www.amd.com/content/dam/code/images/favicon/favicon.ico\"\n    },\n    {\n      \"id\": \"https://rocm.docs.amd.com/en/docs-7.0.1/how-to/rocm-for-ai/inference/benchmark-docker/previous-versions/vllm-0.8.3-20250415.html\",\n      \"title\": \"vLLM inference performance testing â€” ROCm Documentation\",\n      \"url\": \"https://rocm.docs.amd.com/en/docs-7.0.1/how-to/rocm-for-ai/inference/benchmark-docker/previous-versions/vllm-0.8.3-20250415.html\",\n      \"publishedDate\": \"2025-09-11T00:00:00.000Z\",\n      \"author\": \"\",\n      \"text\": \"[Skip to main content](https://rocm.docs.amd.com/rocm.docs.amd.com#main-content)\\n\\nBack to top`Ctrl` + `K`\\n\\n[ROCm Documentation](https://rocm.docs.amd.com/index.html)\\n\\nSearch`Ctrl` + `K`\\n\\n# vLLM inference performance testing\\n\\n## Contents\\n\\n# vLLM inference performance testing [\\\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\\\#vllm-inference-performance-testing)\\n\\n2025-09-11\\n\\n71 min read time\\n\\nApplies to Linux\\n\\nThe [ROCm vLLM Docker](https://hub.docker.com/layers/rocm/vllm/rocm6.3.1_instinct_vllm0.8.3_20250415/images/sha256-ad9062dea3483d59dedb17c67f7c49f30eebd6eb37c3fac0a171fb19696cc845) image offers\\na prebuilt, optimized environment for validating large language model (LLM)\\ninference performance on AMD Instinctâ„¢ MI300X series accelerators. This ROCm vLLM\\nDocker image integrates vLLM and PyTorch tailored specifically for MI300X series\\naccelerators and includes the following components:\\n\\n- [ROCm 6.3.1](https://github.com/ROCm/ROCm)\\n\\n- [vLLM 0.8.3](https://docs.vllm.ai/en/latest)\\n\\n- [PyTorch 2.7.0 (dev nightly)](https://github.com/pytorch/pytorch)\\n\\n- [hipBLASLt 0.13](https://github.com/ROCm/hipBLASLt)\\n\\n\\nWith this Docker image, you can quickly test the [expected\\\\\\ninference performance numbers](https://rocm.docs.amd.com/rocm.docs.amd.com#vllm-benchmark-performance-measurements-v083) for\\nMI300X series accelerators.\\n\\n## Supported models [\\\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\\\#supported-models)\\n\\nModel\\n\\nLlama\\n\\nMistral\\n\\nQwen\\n\\nDBRX\\n\\nGemma\\n\\nCohere\\n\\nDeepSeek\\n\\nModel variant\\n\\nLlama 3.1 8B\\n\\nLlama 3.1 70B\\n\\nLlama 3.1 405B\\n\\nLlama 3.2 11B Vision\\n\\nLlama 2 7B\\n\\nLlama 2 70B\\n\\nLlama 3.1 8B FP8\\n\\nLlama 3.1 70B FP8\\n\\nLlama 3.1 405B FP8\\n\\nMixtral MoE 8x7B\\n\\nMixtral MoE 8x22B\\n\\nMistral 7B\\n\\nMixtral MoE 8x7B FP8\\n\\nMixtral MoE 8x22B FP8\\n\\nMistral 7B FP8\\n\\nQwen2 7B\\n\\nQwen2 72B\\n\\nQwQ-32B\\n\\nDBRX Instruct\\n\\nDBRX Instruct FP8\\n\\nGemma 2 27B\\n\\nC4AI Command R+ 08-2024\\n\\nC4AI Command R+ 08-2024 FP8\\n\\nDeepSeek MoE 16B\\n\\nNote\\n\\nSee the [Llama 3.1 8B model card on Hugging Face](https://huggingface.co/meta-llama/Llama-3.1\",\n      \"favicon\": \"https://www.amd.com/content/dam/code/images/favicon/favicon.ico\"\n    },\n    {\n      \"id\": \"https://rocm.docs.amd.com/en/docs-6.3.3/how-to/rocm-for-ai/inference/vllm-benchmark.html\",\n      \"title\": \"LLM inference performance testing on AMD Instinct MI300X â€” ROCm Documentation\",\n      \"url\": \"https://rocm.docs.amd.com/en/docs-6.3.3/how-to/rocm-for-ai/inference/vllm-benchmark.html\",\n      \"publishedDate\": \"2025-06-13T00:00:00.000Z\",\n      \"author\": \"\",\n      \"text\": \"[Skip to main content](https://rocm.docs.amd.com/rocm.docs.amd.com#main-content)\\n\\nBack to top\\n\\n`Ctrl` + `K`\\n\\n[ROCm Documentation](https://rocm.docs.amd.com/index.html)\\n\\n# LLM inference performance testing on AMD Instinct MI300X\\n\\n## Contents\\n\\n# LLM inference performance testing on AMD Instinct MI300X [\\\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\\\#llm-inference-performance-testing-on-amd-instinct-mi300x)\\n\\n2025-06-13\\n\\n74 min read time\\n\\nApplies to Linux\\n\\nThe [ROCm vLLM Docker](https://hub.docker.com/layers/rocm/vllm/rocm6.3.1_instinct_vllm0.7.3_20250325/images/sha256-25245924f61750b19be6dcd8e787e46088a496c1fe17ee9b9e397f3d84d35640) image offers\\na prebuilt, optimized environment for validating large language model (LLM)\\ninference performance on AMD Instinctâ„¢ MI300X series accelerator. This ROCm vLLM\\nDocker image integrates vLLM and PyTorch tailored specifically for MI300X series\\naccelerators and includes the following components:\\n\\n- [ROCm 6.3.1](https://github.com/ROCm/ROCm)\\n\\n- [vLLM 0.7.3](https://docs.vllm.ai/en/latest)\\n\\n- [PyTorch 2.7.0 (dev nightly)](https://github.com/pytorch/pytorch)\\n\\n- [hipBLASLt 0.13](https://github.com/ROCm/hipBLASLt)\\n\\n\\nWith this Docker image, you can quickly test the [expected\\\\\\ninference performance numbers](https://rocm.docs.amd.com/rocm.docs.amd.com#vllm-benchmark-performance-measurements) for\\nMI300X series accelerators.\\n\\n## Available models [\\\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\\\#available-models)\\n\\nModel\\n\\nLlama\\n\\nMistral\\n\\nQwen\\n\\nJAIS\\n\\nDBRX\\n\\nGemma\\n\\nCohere\\n\\nDeepSeek\\n\\nModel variant\\n\\nLlama 3.1 8B\\n\\nLlama 3.1 70B\\n\\nLlama 3.1 405B\\n\\nLlama 3.2 11B Vision\\n\\nLlama 2 7B\\n\\nLlama 2 70B\\n\\nLlama 3.1 8B FP8\\n\\nLlama 3.1 70B FP8\\n\\nLlama 3.1 405B FP8\\n\\nMixtral MoE 8x7B\\n\\nMixtral MoE 8x22B\\n\\nMistral 7B\\n\\nMixtral MoE 8x7B FP8\\n\\nMixtral MoE 8x22B FP8\\n\\nMistral 7B FP8\\n\\nQwen2 7B\\n\\nQwen2 72B\\n\\nJAIS 13B\\n\\nJAIS 30B\\n\\nDBRX Instruct\\n\\nDBRX Instruct FP8\\n\\nGemma 2 27B\\n\\nC4AI Command R+ 08-2024\\n\\nC4AI Command R+ 08-2024 FP8\\n\\nDeepSeek MoE 16B\\n\\nNote\\n\\nSee the [Llama 3.1 8B model card o\",\n      \"favicon\": \"https://www.amd.com/content/dam/code/images/favicon/favicon.ico\"\n    },\n    {\n      \"id\": \"https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference/benchmark-docker/vllm.html\",\n      \"title\": \"vLLM inference performance testing â€” ROCm Documentation\",\n      \"url\": \"https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference/benchmark-docker/vllm.html\",\n      \"publishedDate\": \"2025-11-05T00:00:00.000Z\",\n      \"author\": \"\",\n      \"text\": \"[Skip to main content](https://rocm.docs.amd.com/rocm.docs.amd.com#main-content)\\n\\nBack to top`Ctrl` + `K`\\n\\n[ROCm Documentation](https://rocm.docs.amd.com/index.html)\\n\\nSearch`Ctrl` + `K`\\n\\n# vLLM inference performance testing\\n\\n## Contents\\n\\n# vLLM inference performance testing [\\\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\\\#vllm-inference-performance-testing)\\n\\n2025-10-16\\n\\n96 min read time\\n\\nApplies to Linux\\n\\nThe [ROCm vLLM Docker](https://hub.docker.com/layers/rocm/vllm/rocm7.0.0_vllm_0.10.2_20251006/images/sha256-94fd001964e1cf55c3224a445b1fb5be31a7dac302315255db8422d813edd7f5) image offers a\\nprebuilt, optimized environment for validating large language model (LLM)\\ninference performance on AMD Instinctâ„¢ MI355X, MI350X, MI325X and MI300X\\nGPUs. This ROCm vLLM Docker image integrates vLLM and PyTorch tailored\\nspecifically for AMD data center GPUs and includes the following components:\\n\\nrocm/vllm:rocm7.0.0\\\\_vllm\\\\_0.10.2\\\\_20251006\\n\\n| Software component | Version |\\n| --- | --- |\\n| ROCm | 7.0.0 |\\n| vLLM | 0.10.2 (0.11.0rc2.dev160+g790d22168.rocm700) |\\n| PyTorch | 2.9.0a0+git1c57644 |\\n| hipBLASLt | 1.0.0 |\\n\\nWith this Docker image, you can quickly test the [expected\\\\\\ninference performance numbers](https://rocm.docs.amd.com/rocm.docs.amd.com#vllm-benchmark-performance-measurements-930) for\\nAMD Instinct GPUs.\\n\\n## Whatâ€™s new [\\\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\\\#what-s-new)\\n\\nThe following is summary of notable changes since the [previous ROCm/vLLM Docker release](https://rocm.docs.amd.com/previous-versions/vllm-history.html).\\n\\n- Added support for AMD Instinct MI355X and MI350X GPUs.\\n\\n- Added support and benchmarking instructions for the following models. See [Supported models](https://rocm.docs.amd.com/rocm.docs.amd.com#vllm-benchmark-supported-models-930).\\n\\n - Llama 4 Scout and Maverick\\n\\n - DeepSeek R1 0528 FP8\\n\\n - MXFP4 models (MI355X and MI350X only): Llama 3.3 70B MXFP4 and Llama 3.1 405B MXFP4\\n\\n - GPT OSS 20B and 120B\\n\\n - Qwen 3 32B, 30B-A3B, and 235B-A22B\\n- Rem\",\n      \"favicon\": \"https://www.amd.com/content/dam/code/images/favicon/favicon.ico\"\n    },\n    {\n      \"id\": \"https://www.youtube.com/watch?v=2Hgk46_E_1c\",\n      \"title\": \"Mac Studio M3 Ultra Vs M2 Ultra - OMG! These Results are CRAZY!\",\n      \"url\": \"https://www.youtube.com/watch?v=2Hgk46_E_1c\",\n      \"publishedDate\": \"2025-03-12T00:00:00.000Z\",\n      \"author\": \"Matt Talks Tech\",\n      \"text\": \"Is the Mac Studio M3 ULTRA worth getting for LLM with DeepSeek v3 or Developers with code and coding with JavaScript, C# and Python? Today lets Benchmark the difference to the M2 Ultra.\\\\n\\\\nMake sure to check out my other Apple News videos to give you the latest apple leaks and of course the latest Release Date and Price Details\\\\n\\\\nðŸŽ¶GET NEW AIRPODS HERE (Amazon Links)\\\\n---------------------------------------------------\\\\nAirPods Max USB-C: https://amzn.to/3YU5mAo\\\\nAirPods Pro 2: https://amzn.to/3OmWAGf\\\\nAirPods 4 ACN: https://amzn.to/3CD9Ow3\\\\nAirPods 4: https://amzn.to/490nYmZ\\\\n\\\\nðŸ’» GET THE NEW M4 Max, M4 Pro, M4 MACS HERE (Amazon Links)\\\\n---------------------------------------------------\\\\nM4 14â€ MacBook Pro: https://amzn.to/3YYkFbn\\\\nM4 Pro/Max 14\\\\\\\" MacBook Pro: https://amzn.to/3YW64x0\\\\nM4 Pro/Max 16\\\\\\\" MacBook Pro: https://amzn.to/3V2PSZy\\\\nM3 13\\\\\\\" MacBook Pro: https://amzn.to/40TX3qW\\\\nM3 15\\\\\\\" MacBook Air: https://amzn.to/4fW1LIS\\\\nM2 13\\\\\\\" MacBook Air https://amzn.to/410WjA9\\\\nM4/M4 Pro Mac Mini: https://amzn.to/3KviSDi\\\\nM2 Max/Ultra Mac Studio: https://amzn.to/4eMU1rE\\\\nM4 24\\\\\\\" iMac; https://amzn.to/3Z9bkxA\\\\n\\\\nðŸ“±GET THE NEW IPHONES HERE (Amazon Links)\\\\n---------------------------------------------------\\\\niPhone 16 Pro Max: https://amzn.to/3Z9bkxA\\\\niPhone 16 Pro: https://amzn.to/416R1Dc\\\\niPhone 16: https://amzn.to/3CACsO8\\\\niPhone 16 Plus: https://amzn.to/4g2vj7B\\\\niPhone 15: https://amzn.to/3CHrFBQ\\\\niPhone 14: https://amzn.to/4fzqx1I\\\\n\\\\nðŸ“±GET THE NEW IPADS HERE (Amazon Links)\\\\n---------------------------------------------------\\\\niPad Pro M4 13 (2024): https://amzn.to/3On12EY\\\\niPad Pro M4 11 (2024): https://amzn.to/4fzqx1I\\\\niPad Air M2 11â€: https://amzn.to/4eLCdNN\\\\niPad Air M2 13â€: https://amzn.to/4g3rGi5\\\\niPad 10th Generation: https://amzn.to/4eHzrsW\\\\niPad 9th Generation: https://amzn.to/3zVbZqa\\\\niPad Mini: https://amzn.to/3V1mLG8\\\\n\\\\nM2 Ultra Vs M2 Ultra Developers and LLM Benchmarks Comparison 0:00\\\\nGeekbench CPU \\\\u0026 GPU 1:24\\\\nJavaScript, SpeedOmeter 2:09\\\\nC# Compil\",\n      \"favicon\": \"https://www.youtube.com/s/desktop/652cfe42/img/favicon_32x32.png\"\n    }\n  ],\n  \"requestTags\": {\n    \"flagsEnabledInPosthog\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exa21,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache\",\n    \"flagsEnabledInRequest\": \"\",\n    \"enabledFlags\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exa21,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache\",\n    \"initialSearchType\": \"auto\",\n    \"didIncludeText\": \"no\",\n    \"didExcludeText\": \"no\",\n    \"didIncludeDomains\": \"no\",\n    \"didExcludeDomains\": \"no\",\n    \"didIncludeUrls\": \"no\",\n    \"didExcludeUrls\": \"no\",\n    \"moderation\": \"no\",\n    \"didUseDateFilters\": \"no\",\n    \"category\": \"none\",\n    \"didUseAutoprompt\": \"no\",\n    \"hasUserLocation\": \"no\",\n    \"didRequestContentText\": true,\n    \"didRequestContentHighlights\": false,\n    \"didRequestContentSummary\": false,\n    \"livecrawl\": \"preferred\",\n    \"requestedLinks\": \"no\",\n    \"willPostRerankNeural\": \"yes\",\n    \"willPostModerate\": false,\n    \"searchTypeBranch\": \"auto\",\n    \"autoBranch\": {\n      \"neuralSearch\": {\n        \"sierra\": {\n          \"query\": \"Mac Studio M2 Ultra 128GB vs AMD ROCm LLM inference performance comparison\",\n          \"flags\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache,use_exa21\",\n          \"filters\": {\n            \"moderation\": \"blacklisted\",\n            \"moderationConfig\": {\n              \"llamaguardS1\": false,\n              \"llamaguardS3\": false,\n              \"llamaguardS4\": false,\n              \"llamaguardS12\": false,\n              \"domainBlacklisted\": false,\n              \"domainBlacklistedMedia\": false\n            }\n          },\n          \"queryDateString\": \"2025-11-06\",\n          \"numResultsRequested\": 35,\n          \"engine\": \"default\",\n          \"isTweetCategory\": false,\n          \"sierraTime\": 333.87447799998336,\n          \"numResultsReturned\": 35\n        },\n        \"fetchContents\": {\n          \"contentExpansion\": {\n            \"failedContents\": []\n          },\n          \"contentsResultsCount\": 35,\n          \"filteredResultsCount\": 0\n        }\n      },\n      \"keywordSearch\": {\n        \"query\": \"Mac Studio M2 Ultra 128GB vs AMD ROCm LLM inference performance comparison\",\n        \"modifiedQuery\": \"Mac Studio M2 Ultra 128GB vs AMD ROCm LLM inference performance comparison\",\n        \"slicedAmount\": 0,\n        \"searchData\": {\n          \"q\": \"Mac Studio M2 Ultra 128GB vs AMD ROCm LLM inference performance comparison\",\n          \"num\": 10,\n          \"safe\": \"active\"\n        },\n        \"opts\": {\n          \"snippetMode\": \"snippet\",\n          \"addAnswerBox\": true,\n          \"addKnowledgeGraph\": true,\n          \"addPeopleAlsoAsk\": false,\n          \"allowEmptyAnswerLinks\": false,\n          \"providers\": [\n            {\n              \"provider\": \"serper\",\n              \"startAfterMs\": 0,\n              \"timeoutMs\": 10000\n            }\n          ],\n          \"timeoutMs\": 10000,\n          \"teamId\": \"cm60snyw00011cfcxf2vbwp8n\"\n        },\n        \"serperSource\": \"regular\",\n        \"peopleAlsoAsk\": [\n          \"What is the best Mac for inference?\",\n          \"Is the Mac Studio energy efficient?\",\n          \"What operating system does Mac Studio use?\"\n        ],\n        \"relatedQueries\": [\n          \"Mac Studio LLM benchmark\",\n          \"M3 Ultra LLM benchmark\",\n          \"M3 max llm benchmark\",\n          \"LLM inference benchmark\",\n          \"M2 Ultra LLM\",\n          \"NVIDIA LLM benchmark\",\n          \"Mac Studio inference speed\",\n          \"Apple Silicon LLM performance\"\n        ],\n        \"success\": true,\n        \"numResultsRequested\": 5,\n        \"numResultsReturned\": 10,\n        \"fetchContents\": {\n          \"contentExpansion\": {\n            \"failedContents\": [\n              \"https://www.reddit.com/r/LocalLLM/comments/1l702x2/mac_studio_for_llms_m4_max_64gb_40c_gpu_vs_m2/\",\n              \"https://www.reddit.com/r/ROCm/comments/1lhjzx0/benchmark_lm_studio_vulkan_vs_rocm/\",\n              \"https://www.reddit.com/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/\"\n            ]\n          },\n          \"contentsResultsCount\": 7,\n          \"filteredResultsCount\": 0\n        }\n      },\n      \"neitherFailed\": true,\n      \"autoJoin\": true,\n      \"autoJoinRerank\": {\n        \"reranker\": {\n          \"withModeration\": false,\n          \"preserveUnranked\": true,\n          \"strategy\": {\n            \"strategy\": \"modal_sranker_qwen_trt\",\n            \"default\": true\n          },\n          \"strategy_decided\": \"modal_sranker_qwen_trt\",\n          \"strategy_reason\": \"explicit_strategy\",\n          \"isQdf\": false\n        }\n      },\n      \"autoJoinResults\": \"35\"\n    },\n    \"decidedAutoType\": \"neural\",\n    \"resolvedSearchType\": \"neural\",\n    \"autoType\": \"neural\",\n    \"decidedResultsLength\": 5,\n    \"totalFilterCount\": \"12\",\n    \"contextExpansionNeeded\": \"not-needed\",\n    \"finalResultsCount\": 5,\n    \"searchTime\": 4545.7,\n    \"path\": \"/search\",\n    \"method\": \"POST\",\n    \"teamId\": \"cm60snyw00011cfcxf2vbwp8n\"\n  },\n  \"effectiveFilters\": {\n    \"includeDomains\": [],\n    \"excludeDomains\": [],\n    \"includeUrls\": [],\n    \"excludeUrls\": [],\n    \"includeText\": [],\n    \"excludeText\": []\n  },\n  \"searchTime\": 4545.7,\n  \"costDollars\": {\n    \"total\": 0.01,\n    \"search\": {\n      \"neural\": 0.005\n    },\n    \"contents\": {\n      \"text\": 0.005\n    }\n  }\n}"}]}]},"uuid":"b7765547-c44d-4d72-a1ab-1be93367429a","timestamp":"2025-11-06T14:11:59.880Z","toolUseResult":[{"type":"text","text":"{\n  \"requestId\": \"7e521af51cbb388353782863752fca75\",\n  \"resolvedSearchType\": \"neural\",\n  \"results\": [\n    {\n      \"id\": \"https://www.amd.com/en/developer/resources/rocm-hub/dev-ai/performance-results.html\",\n      \"title\": \"Performance Results with AMD ROCmâ„¢ Software\",\n      \"url\": \"https://www.amd.com/en/developer/resources/rocm-hub/dev-ai/performance-results.html\",\n      \"publishedDate\": \"2025-10-31T16:21:22.000Z\",\n      \"author\": \"\",\n      \"text\": \"[Skip to main content](https://www.amd.com/www.amd.com)\\n\\n[AMD Website Accessibility Statement](https://www.amd.com/en/site-notifications/accessibility-statement.html)\\n\\n# Performance Results with AMD ROCmâ„¢ Software\\n\\nThis page summarizes performance measurements on AMD Instinctâ„¢ GPUs for popular AI models.\\n\\nThe data in the following tables is a reference point to help users evaluate observed performance. It should not be considered as the peak performance that AMD GPUs and ROCmâ„¢ software can deliver.\\n\\n1. AI Inference\\n2. AI Training\\n\\n## AI Inference\\n\\n#### Throughput Measurements\\n\\nThe table below shows performance data where a local inference client is fed requests at an infinite rate and shows the throughput client-server scenario under maximum load.\\n\\nThis result is based on the Docker container (rocm/vllm:rocm7.0.0\\\\_vllm\\\\_0.10.2\\\\_20251006), which was released on October 6, 2025.\\n\\n| | | | | | | | |\\n| --- | --- | --- | --- | --- | --- | --- | --- |\\n| **Model** | **Precision** | **TP Size** | **Input** | **Output** | **Num Prompts** | **Max Num Seqs** | **Throughput (tokens/s)** |\\n| Llama 3.1 70B (amd/Llama-3.1-70B-Instruct-FP8-KV) | FP8 | 8 | 128 | 2048 | 3200 | 3200 | 13212.5 |\\n| 128 | 4096 | 1500 | 1500 | 11312.8 |\\n| 500 | 2000 | 2000 | 2000 | 11376.7 |\\n| 2048 | 2048 | 1500 | 1500 | 7252.1 |\\n| Llama 3.1 405B (amd/Llama-3.1-405B-Instruct-FP8-KV) | FP8 | 8 | 128 | 2048 | 1500 | 1500 | 4201.7 |\\n| 128 | 4096 | 1500 | 1500 | 3176.3 |\\n| 500 | 2000 | 2000 | 2000 | 2992.0 |\\n| 2048 | 2048 | 500 | 500 | 2153.7 |\\n\\nTP stands for Tensor Parallelism.\\n\\nServer: Dual AMD EPYC 9554 64-core processor-based production server with 8x AMD MI300X (192GB HBM3 750W) GPUs, 1 NUMA node per socket, System BIOS 1.8,Â UbuntuÂ® 22.04, amdgpu driver 6.8.5\\n\\nReproduce these results on your system by following the instructions in [measuring inference performance with vLLM on the AMD GPUs user guide](https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference/vllm-benchmark.html?model=pyt_vllm_llama-\",\n      \"image\": \"https://www.amd.com/content/dam/amd/en/images/logos/products/2058250-amd-rocm.jpg\",\n      \"favicon\": \"https://www.amd.com/content/dam/code/images/favicon/favicon.ico\"\n    },\n    {\n      \"id\": \"https://rocm.docs.amd.com/en/docs-7.0.1/how-to/rocm-for-ai/inference/benchmark-docker/previous-versions/vllm-0.8.3-20250415.html\",\n      \"title\": \"vLLM inference performance testing â€” ROCm Documentation\",\n      \"url\": \"https://rocm.docs.amd.com/en/docs-7.0.1/how-to/rocm-for-ai/inference/benchmark-docker/previous-versions/vllm-0.8.3-20250415.html\",\n      \"publishedDate\": \"2025-09-11T00:00:00.000Z\",\n      \"author\": \"\",\n      \"text\": \"[Skip to main content](https://rocm.docs.amd.com/rocm.docs.amd.com#main-content)\\n\\nBack to top`Ctrl` + `K`\\n\\n[ROCm Documentation](https://rocm.docs.amd.com/index.html)\\n\\nSearch`Ctrl` + `K`\\n\\n# vLLM inference performance testing\\n\\n## Contents\\n\\n# vLLM inference performance testing [\\\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\\\#vllm-inference-performance-testing)\\n\\n2025-09-11\\n\\n71 min read time\\n\\nApplies to Linux\\n\\nThe [ROCm vLLM Docker](https://hub.docker.com/layers/rocm/vllm/rocm6.3.1_instinct_vllm0.8.3_20250415/images/sha256-ad9062dea3483d59dedb17c67f7c49f30eebd6eb37c3fac0a171fb19696cc845) image offers\\na prebuilt, optimized environment for validating large language model (LLM)\\ninference performance on AMD Instinctâ„¢ MI300X series accelerators. This ROCm vLLM\\nDocker image integrates vLLM and PyTorch tailored specifically for MI300X series\\naccelerators and includes the following components:\\n\\n- [ROCm 6.3.1](https://github.com/ROCm/ROCm)\\n\\n- [vLLM 0.8.3](https://docs.vllm.ai/en/latest)\\n\\n- [PyTorch 2.7.0 (dev nightly)](https://github.com/pytorch/pytorch)\\n\\n- [hipBLASLt 0.13](https://github.com/ROCm/hipBLASLt)\\n\\n\\nWith this Docker image, you can quickly test the [expected\\\\\\ninference performance numbers](https://rocm.docs.amd.com/rocm.docs.amd.com#vllm-benchmark-performance-measurements-v083) for\\nMI300X series accelerators.\\n\\n## Supported models [\\\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\\\#supported-models)\\n\\nModel\\n\\nLlama\\n\\nMistral\\n\\nQwen\\n\\nDBRX\\n\\nGemma\\n\\nCohere\\n\\nDeepSeek\\n\\nModel variant\\n\\nLlama 3.1 8B\\n\\nLlama 3.1 70B\\n\\nLlama 3.1 405B\\n\\nLlama 3.2 11B Vision\\n\\nLlama 2 7B\\n\\nLlama 2 70B\\n\\nLlama 3.1 8B FP8\\n\\nLlama 3.1 70B FP8\\n\\nLlama 3.1 405B FP8\\n\\nMixtral MoE 8x7B\\n\\nMixtral MoE 8x22B\\n\\nMistral 7B\\n\\nMixtral MoE 8x7B FP8\\n\\nMixtral MoE 8x22B FP8\\n\\nMistral 7B FP8\\n\\nQwen2 7B\\n\\nQwen2 72B\\n\\nQwQ-32B\\n\\nDBRX Instruct\\n\\nDBRX Instruct FP8\\n\\nGemma 2 27B\\n\\nC4AI Command R+ 08-2024\\n\\nC4AI Command R+ 08-2024 FP8\\n\\nDeepSeek MoE 16B\\n\\nNote\\n\\nSee the [Llama 3.1 8B model card on Hugging Face](https://huggingface.co/meta-llama/Llama-3.1\",\n      \"favicon\": \"https://www.amd.com/content/dam/code/images/favicon/favicon.ico\"\n    },\n    {\n      \"id\": \"https://rocm.docs.amd.com/en/docs-6.3.3/how-to/rocm-for-ai/inference/vllm-benchmark.html\",\n      \"title\": \"LLM inference performance testing on AMD Instinct MI300X â€” ROCm Documentation\",\n      \"url\": \"https://rocm.docs.amd.com/en/docs-6.3.3/how-to/rocm-for-ai/inference/vllm-benchmark.html\",\n      \"publishedDate\": \"2025-06-13T00:00:00.000Z\",\n      \"author\": \"\",\n      \"text\": \"[Skip to main content](https://rocm.docs.amd.com/rocm.docs.amd.com#main-content)\\n\\nBack to top\\n\\n`Ctrl` + `K`\\n\\n[ROCm Documentation](https://rocm.docs.amd.com/index.html)\\n\\n# LLM inference performance testing on AMD Instinct MI300X\\n\\n## Contents\\n\\n# LLM inference performance testing on AMD Instinct MI300X [\\\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\\\#llm-inference-performance-testing-on-amd-instinct-mi300x)\\n\\n2025-06-13\\n\\n74 min read time\\n\\nApplies to Linux\\n\\nThe [ROCm vLLM Docker](https://hub.docker.com/layers/rocm/vllm/rocm6.3.1_instinct_vllm0.7.3_20250325/images/sha256-25245924f61750b19be6dcd8e787e46088a496c1fe17ee9b9e397f3d84d35640) image offers\\na prebuilt, optimized environment for validating large language model (LLM)\\ninference performance on AMD Instinctâ„¢ MI300X series accelerator. This ROCm vLLM\\nDocker image integrates vLLM and PyTorch tailored specifically for MI300X series\\naccelerators and includes the following components:\\n\\n- [ROCm 6.3.1](https://github.com/ROCm/ROCm)\\n\\n- [vLLM 0.7.3](https://docs.vllm.ai/en/latest)\\n\\n- [PyTorch 2.7.0 (dev nightly)](https://github.com/pytorch/pytorch)\\n\\n- [hipBLASLt 0.13](https://github.com/ROCm/hipBLASLt)\\n\\n\\nWith this Docker image, you can quickly test the [expected\\\\\\ninference performance numbers](https://rocm.docs.amd.com/rocm.docs.amd.com#vllm-benchmark-performance-measurements) for\\nMI300X series accelerators.\\n\\n## Available models [\\\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\\\#available-models)\\n\\nModel\\n\\nLlama\\n\\nMistral\\n\\nQwen\\n\\nJAIS\\n\\nDBRX\\n\\nGemma\\n\\nCohere\\n\\nDeepSeek\\n\\nModel variant\\n\\nLlama 3.1 8B\\n\\nLlama 3.1 70B\\n\\nLlama 3.1 405B\\n\\nLlama 3.2 11B Vision\\n\\nLlama 2 7B\\n\\nLlama 2 70B\\n\\nLlama 3.1 8B FP8\\n\\nLlama 3.1 70B FP8\\n\\nLlama 3.1 405B FP8\\n\\nMixtral MoE 8x7B\\n\\nMixtral MoE 8x22B\\n\\nMistral 7B\\n\\nMixtral MoE 8x7B FP8\\n\\nMixtral MoE 8x22B FP8\\n\\nMistral 7B FP8\\n\\nQwen2 7B\\n\\nQwen2 72B\\n\\nJAIS 13B\\n\\nJAIS 30B\\n\\nDBRX Instruct\\n\\nDBRX Instruct FP8\\n\\nGemma 2 27B\\n\\nC4AI Command R+ 08-2024\\n\\nC4AI Command R+ 08-2024 FP8\\n\\nDeepSeek MoE 16B\\n\\nNote\\n\\nSee the [Llama 3.1 8B model card o\",\n      \"favicon\": \"https://www.amd.com/content/dam/code/images/favicon/favicon.ico\"\n    },\n    {\n      \"id\": \"https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference/benchmark-docker/vllm.html\",\n      \"title\": \"vLLM inference performance testing â€” ROCm Documentation\",\n      \"url\": \"https://rocm.docs.amd.com/en/latest/how-to/rocm-for-ai/inference/benchmark-docker/vllm.html\",\n      \"publishedDate\": \"2025-11-05T00:00:00.000Z\",\n      \"author\": \"\",\n      \"text\": \"[Skip to main content](https://rocm.docs.amd.com/rocm.docs.amd.com#main-content)\\n\\nBack to top`Ctrl` + `K`\\n\\n[ROCm Documentation](https://rocm.docs.amd.com/index.html)\\n\\nSearch`Ctrl` + `K`\\n\\n# vLLM inference performance testing\\n\\n## Contents\\n\\n# vLLM inference performance testing [\\\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\\\#vllm-inference-performance-testing)\\n\\n2025-10-16\\n\\n96 min read time\\n\\nApplies to Linux\\n\\nThe [ROCm vLLM Docker](https://hub.docker.com/layers/rocm/vllm/rocm7.0.0_vllm_0.10.2_20251006/images/sha256-94fd001964e1cf55c3224a445b1fb5be31a7dac302315255db8422d813edd7f5) image offers a\\nprebuilt, optimized environment for validating large language model (LLM)\\ninference performance on AMD Instinctâ„¢ MI355X, MI350X, MI325X and MI300X\\nGPUs. This ROCm vLLM Docker image integrates vLLM and PyTorch tailored\\nspecifically for AMD data center GPUs and includes the following components:\\n\\nrocm/vllm:rocm7.0.0\\\\_vllm\\\\_0.10.2\\\\_20251006\\n\\n| Software component | Version |\\n| --- | --- |\\n| ROCm | 7.0.0 |\\n| vLLM | 0.10.2 (0.11.0rc2.dev160+g790d22168.rocm700) |\\n| PyTorch | 2.9.0a0+git1c57644 |\\n| hipBLASLt | 1.0.0 |\\n\\nWith this Docker image, you can quickly test the [expected\\\\\\ninference performance numbers](https://rocm.docs.amd.com/rocm.docs.amd.com#vllm-benchmark-performance-measurements-930) for\\nAMD Instinct GPUs.\\n\\n## Whatâ€™s new [\\\\#](https://rocm.docs.amd.com/rocm.docs.amd.com\\\\#what-s-new)\\n\\nThe following is summary of notable changes since the [previous ROCm/vLLM Docker release](https://rocm.docs.amd.com/previous-versions/vllm-history.html).\\n\\n- Added support for AMD Instinct MI355X and MI350X GPUs.\\n\\n- Added support and benchmarking instructions for the following models. See [Supported models](https://rocm.docs.amd.com/rocm.docs.amd.com#vllm-benchmark-supported-models-930).\\n\\n - Llama 4 Scout and Maverick\\n\\n - DeepSeek R1 0528 FP8\\n\\n - MXFP4 models (MI355X and MI350X only): Llama 3.3 70B MXFP4 and Llama 3.1 405B MXFP4\\n\\n - GPT OSS 20B and 120B\\n\\n - Qwen 3 32B, 30B-A3B, and 235B-A22B\\n- Rem\",\n      \"favicon\": \"https://www.amd.com/content/dam/code/images/favicon/favicon.ico\"\n    },\n    {\n      \"id\": \"https://www.youtube.com/watch?v=2Hgk46_E_1c\",\n      \"title\": \"Mac Studio M3 Ultra Vs M2 Ultra - OMG! These Results are CRAZY!\",\n      \"url\": \"https://www.youtube.com/watch?v=2Hgk46_E_1c\",\n      \"publishedDate\": \"2025-03-12T00:00:00.000Z\",\n      \"author\": \"Matt Talks Tech\",\n      \"text\": \"Is the Mac Studio M3 ULTRA worth getting for LLM with DeepSeek v3 or Developers with code and coding with JavaScript, C# and Python? Today lets Benchmark the difference to the M2 Ultra.\\\\n\\\\nMake sure to check out my other Apple News videos to give you the latest apple leaks and of course the latest Release Date and Price Details\\\\n\\\\nðŸŽ¶GET NEW AIRPODS HERE (Amazon Links)\\\\n---------------------------------------------------\\\\nAirPods Max USB-C: https://amzn.to/3YU5mAo\\\\nAirPods Pro 2: https://amzn.to/3OmWAGf\\\\nAirPods 4 ACN: https://amzn.to/3CD9Ow3\\\\nAirPods 4: https://amzn.to/490nYmZ\\\\n\\\\nðŸ’» GET THE NEW M4 Max, M4 Pro, M4 MACS HERE (Amazon Links)\\\\n---------------------------------------------------\\\\nM4 14â€ MacBook Pro: https://amzn.to/3YYkFbn\\\\nM4 Pro/Max 14\\\\\\\" MacBook Pro: https://amzn.to/3YW64x0\\\\nM4 Pro/Max 16\\\\\\\" MacBook Pro: https://amzn.to/3V2PSZy\\\\nM3 13\\\\\\\" MacBook Pro: https://amzn.to/40TX3qW\\\\nM3 15\\\\\\\" MacBook Air: https://amzn.to/4fW1LIS\\\\nM2 13\\\\\\\" MacBook Air https://amzn.to/410WjA9\\\\nM4/M4 Pro Mac Mini: https://amzn.to/3KviSDi\\\\nM2 Max/Ultra Mac Studio: https://amzn.to/4eMU1rE\\\\nM4 24\\\\\\\" iMac; https://amzn.to/3Z9bkxA\\\\n\\\\nðŸ“±GET THE NEW IPHONES HERE (Amazon Links)\\\\n---------------------------------------------------\\\\niPhone 16 Pro Max: https://amzn.to/3Z9bkxA\\\\niPhone 16 Pro: https://amzn.to/416R1Dc\\\\niPhone 16: https://amzn.to/3CACsO8\\\\niPhone 16 Plus: https://amzn.to/4g2vj7B\\\\niPhone 15: https://amzn.to/3CHrFBQ\\\\niPhone 14: https://amzn.to/4fzqx1I\\\\n\\\\nðŸ“±GET THE NEW IPADS HERE (Amazon Links)\\\\n---------------------------------------------------\\\\niPad Pro M4 13 (2024): https://amzn.to/3On12EY\\\\niPad Pro M4 11 (2024): https://amzn.to/4fzqx1I\\\\niPad Air M2 11â€: https://amzn.to/4eLCdNN\\\\niPad Air M2 13â€: https://amzn.to/4g3rGi5\\\\niPad 10th Generation: https://amzn.to/4eHzrsW\\\\niPad 9th Generation: https://amzn.to/3zVbZqa\\\\niPad Mini: https://amzn.to/3V1mLG8\\\\n\\\\nM2 Ultra Vs M2 Ultra Developers and LLM Benchmarks Comparison 0:00\\\\nGeekbench CPU \\\\u0026 GPU 1:24\\\\nJavaScript, SpeedOmeter 2:09\\\\nC# Compil\",\n      \"favicon\": \"https://www.youtube.com/s/desktop/652cfe42/img/favicon_32x32.png\"\n    }\n  ],\n  \"requestTags\": {\n    \"flagsEnabledInPosthog\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exa21,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache\",\n    \"flagsEnabledInRequest\": \"\",\n    \"enabledFlags\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exa21,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache\",\n    \"initialSearchType\": \"auto\",\n    \"didIncludeText\": \"no\",\n    \"didExcludeText\": \"no\",\n    \"didIncludeDomains\": \"no\",\n    \"didExcludeDomains\": \"no\",\n    \"didIncludeUrls\": \"no\",\n    \"didExcludeUrls\": \"no\",\n    \"moderation\": \"no\",\n    \"didUseDateFilters\": \"no\",\n    \"category\": \"none\",\n    \"didUseAutoprompt\": \"no\",\n    \"hasUserLocation\": \"no\",\n    \"didRequestContentText\": true,\n    \"didRequestContentHighlights\": false,\n    \"didRequestContentSummary\": false,\n    \"livecrawl\": \"preferred\",\n    \"requestedLinks\": \"no\",\n    \"willPostRerankNeural\": \"yes\",\n    \"willPostModerate\": false,\n    \"searchTypeBranch\": \"auto\",\n    \"autoBranch\": {\n      \"neuralSearch\": {\n        \"sierra\": {\n          \"query\": \"Mac Studio M2 Ultra 128GB vs AMD ROCm LLM inference performance comparison\",\n          \"flags\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache,use_exa21\",\n          \"filters\": {\n            \"moderation\": \"blacklisted\",\n            \"moderationConfig\": {\n              \"llamaguardS1\": false,\n              \"llamaguardS3\": false,\n              \"llamaguardS4\": false,\n              \"llamaguardS12\": false,\n              \"domainBlacklisted\": false,\n              \"domainBlacklistedMedia\": false\n            }\n          },\n          \"queryDateString\": \"2025-11-06\",\n          \"numResultsRequested\": 35,\n          \"engine\": \"default\",\n          \"isTweetCategory\": false,\n          \"sierraTime\": 333.87447799998336,\n          \"numResultsReturned\": 35\n        },\n        \"fetchContents\": {\n          \"contentExpansion\": {\n            \"failedContents\": []\n          },\n          \"contentsResultsCount\": 35,\n          \"filteredResultsCount\": 0\n        }\n      },\n      \"keywordSearch\": {\n        \"query\": \"Mac Studio M2 Ultra 128GB vs AMD ROCm LLM inference performance comparison\",\n        \"modifiedQuery\": \"Mac Studio M2 Ultra 128GB vs AMD ROCm LLM inference performance comparison\",\n        \"slicedAmount\": 0,\n        \"searchData\": {\n          \"q\": \"Mac Studio M2 Ultra 128GB vs AMD ROCm LLM inference performance comparison\",\n          \"num\": 10,\n          \"safe\": \"active\"\n        },\n        \"opts\": {\n          \"snippetMode\": \"snippet\",\n          \"addAnswerBox\": true,\n          \"addKnowledgeGraph\": true,\n          \"addPeopleAlsoAsk\": false,\n          \"allowEmptyAnswerLinks\": false,\n          \"providers\": [\n            {\n              \"provider\": \"serper\",\n              \"startAfterMs\": 0,\n              \"timeoutMs\": 10000\n            }\n          ],\n          \"timeoutMs\": 10000,\n          \"teamId\": \"cm60snyw00011cfcxf2vbwp8n\"\n        },\n        \"serperSource\": \"regular\",\n        \"peopleAlsoAsk\": [\n          \"What is the best Mac for inference?\",\n          \"Is the Mac Studio energy efficient?\",\n          \"What operating system does Mac Studio use?\"\n        ],\n        \"relatedQueries\": [\n          \"Mac Studio LLM benchmark\",\n          \"M3 Ultra LLM benchmark\",\n          \"M3 max llm benchmark\",\n          \"LLM inference benchmark\",\n          \"M2 Ultra LLM\",\n          \"NVIDIA LLM benchmark\",\n          \"Mac Studio inference speed\",\n          \"Apple Silicon LLM performance\"\n        ],\n        \"success\": true,\n        \"numResultsRequested\": 5,\n        \"numResultsReturned\": 10,\n        \"fetchContents\": {\n          \"contentExpansion\": {\n            \"failedContents\": [\n              \"https://www.reddit.com/r/LocalLLM/comments/1l702x2/mac_studio_for_llms_m4_max_64gb_40c_gpu_vs_m2/\",\n              \"https://www.reddit.com/r/ROCm/comments/1lhjzx0/benchmark_lm_studio_vulkan_vs_rocm/\",\n              \"https://www.reddit.com/r/LocalLLaMA/comments/1ltv847/understanding_tradeoffs_between_m4_max_studio_vs/\"\n            ]\n          },\n          \"contentsResultsCount\": 7,\n          \"filteredResultsCount\": 0\n        }\n      },\n      \"neitherFailed\": true,\n      \"autoJoin\": true,\n      \"autoJoinRerank\": {\n        \"reranker\": {\n          \"withModeration\": false,\n          \"preserveUnranked\": true,\n          \"strategy\": {\n            \"strategy\": \"modal_sranker_qwen_trt\",\n            \"default\": true\n          },\n          \"strategy_decided\": \"modal_sranker_qwen_trt\",\n          \"strategy_reason\": \"explicit_strategy\",\n          \"isQdf\": false\n        }\n      },\n      \"autoJoinResults\": \"35\"\n    },\n    \"decidedAutoType\": \"neural\",\n    \"resolvedSearchType\": \"neural\",\n    \"autoType\": \"neural\",\n    \"decidedResultsLength\": 5,\n    \"totalFilterCount\": \"12\",\n    \"contextExpansionNeeded\": \"not-needed\",\n    \"finalResultsCount\": 5,\n    \"searchTime\": 4545.7,\n    \"path\": \"/search\",\n    \"method\": \"POST\",\n    \"teamId\": \"cm60snyw00011cfcxf2vbwp8n\"\n  },\n  \"effectiveFilters\": {\n    \"includeDomains\": [],\n    \"excludeDomains\": [],\n    \"includeUrls\": [],\n    \"excludeUrls\": [],\n    \"includeText\": [],\n    \"excludeText\": []\n  },\n  \"searchTime\": 4545.7,\n  \"costDollars\": {\n    \"total\": 0.01,\n    \"search\": {\n      \"neural\": 0.005\n    },\n    \"contents\": {\n      \"text\": 0.005\n    }\n  }\n}"}]}
{"parentUuid":"b7765547-c44d-4d72-a1ab-1be93367429a","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01DpxNkiWgByXmKAszEQ5zAV","type":"tool_result","content":[{"type":"text","text":"{\n  \"requestId\": \"01a3c4eac6241f62a56772c5b00703a6\",\n  \"autoDate\": \"2025-01-01T00:00:00.000Z\",\n  \"resolvedSearchType\": \"neural\",\n  \"results\": [\n    {\n      \"id\": \"https://robert-mcdermott.medium.com/performance-vs-practicality-a-comparison-of-vllm-and-ollama-104acad250fd\",\n      \"title\": \"Performance vs Practicality: A Comparison of vLLM and Ollama\",\n      \"url\": \"https://robert-mcdermott.medium.com/performance-vs-practicality-a-comparison-of-vllm-and-ollama-104acad250fd\",\n      \"publishedDate\": \"2025-05-27T06:52:28.000Z\",\n      \"author\": \"Robert McDermott\",\n      \"text\": \"[Sitemap](https://robert-mcdermott.medium.com/sitemap/sitemap.xml)\\n\\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F104acad250fd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Frobert-mcdermott.medium.com%2Fperformance-vs-practicality-a-comparison-of-vllm-and-ollama-104acad250fd&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n\\n[Search](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Frobert-mcdermott.medium.com%2Fperformance-vs-practicality-a-comparison-of-vllm-and-ollama-104acad250fd&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[Mastodon](https://me.dm/@robertmcdermott)\\n\\nPress enter or click to view image in full size\\n\\n# Performance vs Practicality: A Comparison of vLLM and Ollama\\n\\n## vLLM is designed for high-throughput scenarios, while Ollama emphasizes day-to-day simplicity and good-enough performance for most use cases.\\n\\n[Robert McDermott](https://robert-mcdermott.medium.com/?source=post_page---byline--104acad250fd---------------------------------------)\\n\\n14 min read\\n\\nÂ·\\n\\nMay 27, 2025\\n\\n--\\n\\n3\\n\\nListen\\n\\nShare\\n\\n## Background\\n\\nIâ€™m a fan of [**Ollama**](https://ollama.com/) because itâ€™s surprisingly user-friendly. It runs on Linux, Windows, and macOS, and its Docker-style CLI ( `pull`, `run`, `list`, `ps`, `push\",\n      \"image\": \"https://miro.medium.com/v2/resize:fit:1200/1*iLeDGyws7UHmEz84t8sJyQ.png\"\n    },\n    {\n      \"id\": \"https://blog.alphabravo.io/ollama-vs-vllm-the-definitive-guide-to-local-llm-frameworks-in-2025/\",\n      \"title\": \"Ollama vs. vLLM: The Definitive Guide to Local LLM Frameworks in ...\",\n      \"url\": \"https://blog.alphabravo.io/ollama-vs-vllm-the-definitive-guide-to-local-llm-frameworks-in-2025/\",\n      \"publishedDate\": \"2025-04-30T20:55:43.000Z\",\n      \"author\": \"AB Engineering\",\n      \"text\": \"As we navigate through the ever-evolving landscape of artificial intelligence, two heavyweight contenders have emerged in the realm of local large language model (LLM) deployment: Ollama and vLLM. Both frameworks aim to bring powerful AI capabilities to your local infrastructure, but they take distinctly different approaches that make each better suited for particular use cases. This comprehensive comparison will help you understand which tool might be right for your specific needs, whether you're a curious developer, an enterprise architect, or a defense contractor working on classified projects.\\n\\n## The Local LLM Revolution: Setting the Stage\\n\\nBefore diving into the comparison, let's take a moment to understand why local LLM frameworks like Ollama and vLLM have become increasingly important.\\n\\nLarge language models are essentially massive neural networks trained on vast amounts of text data. These models-think GPT-4, Llama, or Gemma-require significant computational resources to run. Traditionally, accessing these models meant sending your data to cloud providers, which raises concerns about privacy, data sovereignty, and operational continuity in disconnected environments.\\n\\nLocal LLM frameworks solve this problem by allowing you to run these powerful models on your own hardware. It's like having your own personal AI assistant that doesn't need to phone home to a third party with your sensitive information. This capability is especially critical for government, defense, healthcare, and financial sectors where data privacy isn't just nice to have-it's non-negotiable.\\n\\n## Ollama: Your Friendly Neighborhood LLM Manager\\n\\n### What Is Ollama, Exactly?\\n\\nOllama is an open-source framework designed with simplicity at its core. It allows you to download, run, and manage large language models on your own hardware with minimal fuss. If setting up AI models were like cooking, Ollama would be your meal kit delivery service-everything comes pre-measured and ready to go.\\n\\nThe name\",\n      \"image\": \"https://blog.alphabravo.io/content/images/size/w1200/2025/04/20250428_1244_Local-LLM-Frameworks-2025.png\",\n      \"favicon\": \"https://blog.alphabravo.io/content/images/size/w256h256/2025/04/ablogo-black.png\"\n    },\n    {\n      \"id\": \"https://www.designveloper.com/blog/vllm-tutorial/\",\n      \"title\": \"vLLM Tutorial for Beginner: What It Is and How to Use It\",\n      \"url\": \"https://www.designveloper.com/blog/vllm-tutorial/\",\n      \"publishedDate\": \"2025-09-03T06:31:51.000Z\",\n      \"author\": \"admin\",\n      \"text\": \"[Get a quote](https://www.designveloper.com/estimation/)\\n\\nServices\\n\\n[Web App Development](https://www.designveloper.com/services/web-application-development-services/) [Mobile App Development](https://www.designveloper.com/services/mobile-app-development-services/) [Software Development](https://www.designveloper.com/services/software-development-services/) [Cyber Security Consultant](https://www.designveloper.com/services/cyber-security-consultant-services/) [AI Development Services](https://www.designveloper.com/services/ai-development-services/) [VOIP App Development](https://www.designveloper.com/services/voip-application-development-services/)\\n\\n[Web App Development When it comes to web application development services, Designveloper is the industry leader.](https://www.designveloper.com/services/web-application-development-services/)\\n\\n[Mobile App Development We have been able to motivate and assist in the implementation of innovation at some of the most recognizable firms in the world.](https://www.designveloper.com/services/mobile-app-development-services/)\\n\\n[Software Development Designveloperâ€™s team of skilled and driven software developers will create long-term value with our software development services.](https://www.designveloper.com/services/software-development-services/)\\n\\n[Cyber Security Consultant Empower your digital defense with expert Cyber Security Consultant Services. Proactive solutions tailored to your needs. Contact us today for a secure tomorrow!](https://www.designveloper.com/services/cyber-security-consultant-services/)\\n\\n[AI Development Services Discover unparalleled AI development services. Crafted with precision and innovation, our solutions empower your business for success. Elevate your technology game today!](https://www.designveloper.com/services/ai-development-services/)\\n\\n[VOIP App Development Unlock efficiency with our top-notch VoIP application development services with Designveloper. Elevate your communication strategy today!](htt\",\n      \"image\": \"https://www.designveloper.com/wp-content/uploads/2025/09/designveloper-blog-13-68b7e09a2959d.webp\",\n      \"favicon\": \"https://www.designveloper.com/wp-content/themes/designveloper-light-theme/images/logo-favicon.png\"\n    },\n    {\n      \"id\": \"https://www.amd.com/en/developer/resources/technical-articles/running-llms-locally-on-amd-gpus-with-ollama.html\",\n      \"title\": \"Running LLMs Locally on AMD GPUs with Ollama\",\n      \"url\": \"https://www.amd.com/en/developer/resources/technical-articles/running-llms-locally-on-amd-gpus-with-ollama.html\",\n      \"publishedDate\": \"2025-04-25T10:20:38.000Z\",\n      \"author\": \"AMD\",\n      \"text\": \"[Skip to main content](https://www.amd.com/www.amd.com)\\n\\n[AMD Website Accessibility Statement](https://www.amd.com/en/site-notifications/accessibility-statement.html)\\n\\n# Running LLMs Locally on AMD GPUs with Ollama\\n\\nSep 26, 2024\\n\\nRunning large language models (LLMs) locally on AMD systems has become more accessible, thanks to Ollama. This guide will focus on the latest Llama 3.2 model, published by Meta on Sep 25th 2024, Meta's Llama 3.2 goes small and multimodal with 1B, 3B, 11B and 90B models. Hereâ€™s how you can run these models on various AMD hardware configurations and a step-by-step installation guide for Ollama on both Linux and Windows Operating Systems on Radeon GPUs.\\n\\n## Supported AMD GPUs\\n\\nOllama supports a range of AMD GPUs, enabling their product on both newer and older models. Here you can find the list of supported GPUs by Ollama:Â [https://github.com/ollama/ollama/blob/main/docs/gpu.md#amd-radeon](https://github.com/ollama/ollama/blob/main/docs/gpu.md#amd-radeon)\\n\\n## Installation and Setup Guide for Ollama\\n\\n**Linux**\\n\\n- System Requirements:\\n - Ubuntu 22.04.4\\n - AMD GPUs with latest AMD ROCm â„¢ software installed\\n- Install ROCm 6.1.3 from below instructions\\n - [https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native\\\\_linux/install-radeon.html](https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-radeon.html)\\n- Install Ollama via single command\\n - `curl -fsSLÂ https://ollama.com/install.shÂ | sh`\\n\\n- Download and run llama-3.2 model:\\n - `ollama run llama3.2`\\n\\n**Windows**\\n\\n- System Requirements:\\n - Windows 10 or Higher\\n - Supported AMD GPUs with driver installed\\n- For Windows installation you can simply download and install Ollama from here:\\n[https://ollama.com/download](https://ollama.com/download) Once installed, simply open PowerShell and run: - `Ollama run llama3.2`\\n\\nAs simple as that, you are ready to chat with your local LLM.\\n\\nYou can find the list of all available models from Ollama hereÂ [https://ollama.\",\n      \"image\": \"https://www.amd.com/content/dam/amd/en/images/blogs/designs/projects/technical-blogs/3234460-running-llms-locally-on-amd-gpus-with-ollama/3234460-running-llms-locally-on-amd-gpus-with-ollama.jpg\",\n      \"favicon\": \"https://www.amd.com/content/dam/code/images/favicon/favicon.ico\"\n    },\n    {\n      \"id\": \"https://medium.com/@namansharma_13002/7-fastest-open-source-llms-you-can-run-locally-in-2025-524be87c2064\",\n      \"title\": \"7 Fastest Open Source LLMs You Can Run Locally in 2025 - Medium\",\n      \"url\": \"https://medium.com/@namansharma_13002/7-fastest-open-source-llms-you-can-run-locally-in-2025-524be87c2064\",\n      \"publishedDate\": \"2025-07-07T08:11:10.000Z\",\n      \"author\": \"Naman Sharma\",\n      \"text\": \"[Sitemap](https://medium.com/sitemap/sitemap.xml)\\n\\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F524be87c2064&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40namansharma_13002%2F7-fastest-open-source-llms-you-can-run-locally-in-2025-524be87c2064&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n\\n[Search](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40namansharma_13002%2F7-fastest-open-source-llms-you-can-run-locally-in-2025-524be87c2064&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n# **7 Fastest Open Source LLMs You Can Run Locally in 2025**\\n\\n[Naman Sharma](https://medium.com/@namansharma_13002?source=post_page---byline--524be87c2064---------------------------------------)\\n\\n12 min read\\n\\nÂ·\\n\\nJul 7, 2025\\n\\n--\\n\\nListen\\n\\nShare\\n\\nPress enter or click to view image in full size\\n\\nWith the ever-evolving generative AI, more developers and startups have found it necessary to **operate** [**large language models**](https://www.ibm.com/think/topics/large-language-models) **(LLMs) locally** rather than solely relying on cloud APIs such as OpenAIâ€™s GPT or Anthropicâ€™s Claude. For decisions like **data privacy, lower latency, or cost reduction in the long run**, it has been proven that in 2025, go\",\n      \"image\": \"https://miro.medium.com/v2/resize:fit:1200/1*UHaqqX6nBk6XHDkaTmkOcw.png\"\n    }\n  ],\n  \"requestTags\": {\n    \"flagsEnabledInPosthog\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exa21,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache\",\n    \"flagsEnabledInRequest\": \"\",\n    \"enabledFlags\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exa21,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache\",\n    \"initialSearchType\": \"auto\",\n    \"didIncludeText\": \"no\",\n    \"didExcludeText\": \"no\",\n    \"didIncludeDomains\": \"no\",\n    \"didExcludeDomains\": \"no\",\n    \"didIncludeUrls\": \"no\",\n    \"didExcludeUrls\": \"no\",\n    \"moderation\": \"no\",\n    \"didUseDateFilters\": \"no\",\n    \"category\": \"none\",\n    \"didUseAutoprompt\": \"no\",\n    \"hasUserLocation\": \"no\",\n    \"didRequestContentText\": true,\n    \"didRequestContentHighlights\": false,\n    \"didRequestContentSummary\": false,\n    \"livecrawl\": \"preferred\",\n    \"requestedLinks\": \"no\",\n    \"willPostRerankNeural\": \"yes\",\n    \"willPostModerate\": false,\n    \"searchTypeBranch\": \"auto\",\n    \"autoBranch\": {\n      \"neuralSearch\": {\n        \"sierra\": {\n          \"query\": \"ollama vLLM Mac M2 Ultra vs AMD GPU support compatibility 2025\",\n          \"flags\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache,use_exa21\",\n          \"filters\": {\n            \"moderation\": \"blacklisted\",\n            \"moderationConfig\": {\n              \"llamaguardS1\": false,\n              \"llamaguardS3\": false,\n              \"llamaguardS4\": false,\n              \"llamaguardS12\": false,\n              \"domainBlacklisted\": false,\n              \"domainBlacklistedMedia\": false\n            }\n          },\n          \"queryDateString\": \"2025-11-06\",\n          \"numResultsRequested\": 35,\n          \"engine\": \"default\",\n          \"isTweetCategory\": false,\n          \"sierraTime\": 469.364636000013,\n          \"numResultsReturned\": 35\n        },\n        \"fetchContents\": {\n          \"contentExpansion\": {\n            \"failedContents\": []\n          },\n          \"contentsResultsCount\": 35,\n          \"filteredResultsCount\": 0\n        }\n      },\n      \"keywordSearch\": {\n        \"query\": \"ollama vLLM Mac M2 Ultra vs AMD GPU support compatibility 2025\",\n        \"modifiedQuery\": \"ollama vLLM Mac M2 Ultra vs AMD GPU support compatibility 2025\",\n        \"slicedAmount\": 0,\n        \"searchData\": {\n          \"q\": \"ollama vLLM Mac M2 Ultra vs AMD GPU support compatibility 2025\",\n          \"num\": 10,\n          \"safe\": \"active\"\n        },\n        \"opts\": {\n          \"snippetMode\": \"snippet\",\n          \"addAnswerBox\": true,\n          \"addKnowledgeGraph\": true,\n          \"addPeopleAlsoAsk\": false,\n          \"allowEmptyAnswerLinks\": false,\n          \"providers\": [\n            {\n              \"provider\": \"serper\",\n              \"startAfterMs\": 0,\n              \"timeoutMs\": 10000\n            }\n          ],\n          \"timeoutMs\": 10000,\n          \"teamId\": \"cm60snyw00011cfcxf2vbwp8n\"\n        },\n        \"serperSource\": \"regular\",\n        \"peopleAlsoAsk\": [\n          \"Will Ollama work with AMD GPU?\",\n          \"Which GPU does Ollama prefer?\",\n          \"What is the best model for llama 2025?\",\n          \"Why is Ollama slower than vLLM?\"\n        ],\n        \"relatedQueries\": [\n          \"Ollama GPU list\",\n          \"Ollama GPU requirements\",\n          \"Ollama GPU support list\",\n          \"Best GPU for Ollama\",\n          \"Ollama macOS AMD GPU\",\n          \"vLLM vs Ollama performance\",\n          \"Ollama Mac GPU support\",\n          \"Ollama GPU support windows\"\n        ],\n        \"success\": true,\n        \"numResultsRequested\": 5,\n        \"numResultsReturned\": 10,\n        \"fetchContents\": {\n          \"contentExpansion\": {\n            \"failedContents\": [\n              \"https://www.reddit.com/r/selfhosted/comments/1g0rpfp/what_gpu_do_i_need_to_run_ai_ollama/\"\n            ]\n          },\n          \"contentsResultsCount\": 9,\n          \"filteredResultsCount\": 0\n        }\n      },\n      \"neitherFailed\": true,\n      \"autoJoin\": true,\n      \"autoJoinRerank\": {\n        \"reranker\": {\n          \"withModeration\": false,\n          \"preserveUnranked\": true,\n          \"strategy\": {\n            \"strategy\": \"modal_sranker_qwen_trt\",\n            \"default\": true\n          },\n          \"strategy_decided\": \"modal_sranker_qwen_trt\",\n          \"strategy_reason\": \"explicit_strategy\",\n          \"isQdf\": false\n        }\n      },\n      \"autoJoinResults\": \"35\"\n    },\n    \"decidedAutoType\": \"neural\",\n    \"resolvedSearchType\": \"neural\",\n    \"autoType\": \"neural\",\n    \"decidedResultsLength\": 5,\n    \"totalFilterCount\": \"11\",\n    \"contextExpansionNeeded\": \"not-needed\",\n    \"finalResultsCount\": 5,\n    \"searchTime\": 4862.1,\n    \"path\": \"/search\",\n    \"method\": \"POST\",\n    \"teamId\": \"cm60snyw00011cfcxf2vbwp8n\"\n  },\n  \"effectiveFilters\": {\n    \"includeDomains\": [],\n    \"excludeDomains\": [],\n    \"includeUrls\": [],\n    \"excludeUrls\": [],\n    \"includeText\": [],\n    \"excludeText\": []\n  },\n  \"searchTime\": 4862.1,\n  \"costDollars\": {\n    \"total\": 0.01,\n    \"search\": {\n      \"neural\": 0.005\n    },\n    \"contents\": {\n      \"text\": 0.005\n    }\n  }\n}"}]}]},"uuid":"9bbd2f33-d7c2-49bd-980b-8982acabe982","timestamp":"2025-11-06T14:12:05.509Z","toolUseResult":[{"type":"text","text":"{\n  \"requestId\": \"01a3c4eac6241f62a56772c5b00703a6\",\n  \"autoDate\": \"2025-01-01T00:00:00.000Z\",\n  \"resolvedSearchType\": \"neural\",\n  \"results\": [\n    {\n      \"id\": \"https://robert-mcdermott.medium.com/performance-vs-practicality-a-comparison-of-vllm-and-ollama-104acad250fd\",\n      \"title\": \"Performance vs Practicality: A Comparison of vLLM and Ollama\",\n      \"url\": \"https://robert-mcdermott.medium.com/performance-vs-practicality-a-comparison-of-vllm-and-ollama-104acad250fd\",\n      \"publishedDate\": \"2025-05-27T06:52:28.000Z\",\n      \"author\": \"Robert McDermott\",\n      \"text\": \"[Sitemap](https://robert-mcdermott.medium.com/sitemap/sitemap.xml)\\n\\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F104acad250fd&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Frobert-mcdermott.medium.com%2Fperformance-vs-practicality-a-comparison-of-vllm-and-ollama-104acad250fd&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n\\n[Search](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Frobert-mcdermott.medium.com%2Fperformance-vs-practicality-a-comparison-of-vllm-and-ollama-104acad250fd&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[Mastodon](https://me.dm/@robertmcdermott)\\n\\nPress enter or click to view image in full size\\n\\n# Performance vs Practicality: A Comparison of vLLM and Ollama\\n\\n## vLLM is designed for high-throughput scenarios, while Ollama emphasizes day-to-day simplicity and good-enough performance for most use cases.\\n\\n[Robert McDermott](https://robert-mcdermott.medium.com/?source=post_page---byline--104acad250fd---------------------------------------)\\n\\n14 min read\\n\\nÂ·\\n\\nMay 27, 2025\\n\\n--\\n\\n3\\n\\nListen\\n\\nShare\\n\\n## Background\\n\\nIâ€™m a fan of [**Ollama**](https://ollama.com/) because itâ€™s surprisingly user-friendly. It runs on Linux, Windows, and macOS, and its Docker-style CLI ( `pull`, `run`, `list`, `ps`, `push\",\n      \"image\": \"https://miro.medium.com/v2/resize:fit:1200/1*iLeDGyws7UHmEz84t8sJyQ.png\"\n    },\n    {\n      \"id\": \"https://blog.alphabravo.io/ollama-vs-vllm-the-definitive-guide-to-local-llm-frameworks-in-2025/\",\n      \"title\": \"Ollama vs. vLLM: The Definitive Guide to Local LLM Frameworks in ...\",\n      \"url\": \"https://blog.alphabravo.io/ollama-vs-vllm-the-definitive-guide-to-local-llm-frameworks-in-2025/\",\n      \"publishedDate\": \"2025-04-30T20:55:43.000Z\",\n      \"author\": \"AB Engineering\",\n      \"text\": \"As we navigate through the ever-evolving landscape of artificial intelligence, two heavyweight contenders have emerged in the realm of local large language model (LLM) deployment: Ollama and vLLM. Both frameworks aim to bring powerful AI capabilities to your local infrastructure, but they take distinctly different approaches that make each better suited for particular use cases. This comprehensive comparison will help you understand which tool might be right for your specific needs, whether you're a curious developer, an enterprise architect, or a defense contractor working on classified projects.\\n\\n## The Local LLM Revolution: Setting the Stage\\n\\nBefore diving into the comparison, let's take a moment to understand why local LLM frameworks like Ollama and vLLM have become increasingly important.\\n\\nLarge language models are essentially massive neural networks trained on vast amounts of text data. These models-think GPT-4, Llama, or Gemma-require significant computational resources to run. Traditionally, accessing these models meant sending your data to cloud providers, which raises concerns about privacy, data sovereignty, and operational continuity in disconnected environments.\\n\\nLocal LLM frameworks solve this problem by allowing you to run these powerful models on your own hardware. It's like having your own personal AI assistant that doesn't need to phone home to a third party with your sensitive information. This capability is especially critical for government, defense, healthcare, and financial sectors where data privacy isn't just nice to have-it's non-negotiable.\\n\\n## Ollama: Your Friendly Neighborhood LLM Manager\\n\\n### What Is Ollama, Exactly?\\n\\nOllama is an open-source framework designed with simplicity at its core. It allows you to download, run, and manage large language models on your own hardware with minimal fuss. If setting up AI models were like cooking, Ollama would be your meal kit delivery service-everything comes pre-measured and ready to go.\\n\\nThe name\",\n      \"image\": \"https://blog.alphabravo.io/content/images/size/w1200/2025/04/20250428_1244_Local-LLM-Frameworks-2025.png\",\n      \"favicon\": \"https://blog.alphabravo.io/content/images/size/w256h256/2025/04/ablogo-black.png\"\n    },\n    {\n      \"id\": \"https://www.designveloper.com/blog/vllm-tutorial/\",\n      \"title\": \"vLLM Tutorial for Beginner: What It Is and How to Use It\",\n      \"url\": \"https://www.designveloper.com/blog/vllm-tutorial/\",\n      \"publishedDate\": \"2025-09-03T06:31:51.000Z\",\n      \"author\": \"admin\",\n      \"text\": \"[Get a quote](https://www.designveloper.com/estimation/)\\n\\nServices\\n\\n[Web App Development](https://www.designveloper.com/services/web-application-development-services/) [Mobile App Development](https://www.designveloper.com/services/mobile-app-development-services/) [Software Development](https://www.designveloper.com/services/software-development-services/) [Cyber Security Consultant](https://www.designveloper.com/services/cyber-security-consultant-services/) [AI Development Services](https://www.designveloper.com/services/ai-development-services/) [VOIP App Development](https://www.designveloper.com/services/voip-application-development-services/)\\n\\n[Web App Development When it comes to web application development services, Designveloper is the industry leader.](https://www.designveloper.com/services/web-application-development-services/)\\n\\n[Mobile App Development We have been able to motivate and assist in the implementation of innovation at some of the most recognizable firms in the world.](https://www.designveloper.com/services/mobile-app-development-services/)\\n\\n[Software Development Designveloperâ€™s team of skilled and driven software developers will create long-term value with our software development services.](https://www.designveloper.com/services/software-development-services/)\\n\\n[Cyber Security Consultant Empower your digital defense with expert Cyber Security Consultant Services. Proactive solutions tailored to your needs. Contact us today for a secure tomorrow!](https://www.designveloper.com/services/cyber-security-consultant-services/)\\n\\n[AI Development Services Discover unparalleled AI development services. Crafted with precision and innovation, our solutions empower your business for success. Elevate your technology game today!](https://www.designveloper.com/services/ai-development-services/)\\n\\n[VOIP App Development Unlock efficiency with our top-notch VoIP application development services with Designveloper. Elevate your communication strategy today!](htt\",\n      \"image\": \"https://www.designveloper.com/wp-content/uploads/2025/09/designveloper-blog-13-68b7e09a2959d.webp\",\n      \"favicon\": \"https://www.designveloper.com/wp-content/themes/designveloper-light-theme/images/logo-favicon.png\"\n    },\n    {\n      \"id\": \"https://www.amd.com/en/developer/resources/technical-articles/running-llms-locally-on-amd-gpus-with-ollama.html\",\n      \"title\": \"Running LLMs Locally on AMD GPUs with Ollama\",\n      \"url\": \"https://www.amd.com/en/developer/resources/technical-articles/running-llms-locally-on-amd-gpus-with-ollama.html\",\n      \"publishedDate\": \"2025-04-25T10:20:38.000Z\",\n      \"author\": \"AMD\",\n      \"text\": \"[Skip to main content](https://www.amd.com/www.amd.com)\\n\\n[AMD Website Accessibility Statement](https://www.amd.com/en/site-notifications/accessibility-statement.html)\\n\\n# Running LLMs Locally on AMD GPUs with Ollama\\n\\nSep 26, 2024\\n\\nRunning large language models (LLMs) locally on AMD systems has become more accessible, thanks to Ollama. This guide will focus on the latest Llama 3.2 model, published by Meta on Sep 25th 2024, Meta's Llama 3.2 goes small and multimodal with 1B, 3B, 11B and 90B models. Hereâ€™s how you can run these models on various AMD hardware configurations and a step-by-step installation guide for Ollama on both Linux and Windows Operating Systems on Radeon GPUs.\\n\\n## Supported AMD GPUs\\n\\nOllama supports a range of AMD GPUs, enabling their product on both newer and older models. Here you can find the list of supported GPUs by Ollama:Â [https://github.com/ollama/ollama/blob/main/docs/gpu.md#amd-radeon](https://github.com/ollama/ollama/blob/main/docs/gpu.md#amd-radeon)\\n\\n## Installation and Setup Guide for Ollama\\n\\n**Linux**\\n\\n- System Requirements:\\n - Ubuntu 22.04.4\\n - AMD GPUs with latest AMD ROCm â„¢ software installed\\n- Install ROCm 6.1.3 from below instructions\\n - [https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native\\\\_linux/install-radeon.html](https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-radeon.html)\\n- Install Ollama via single command\\n - `curl -fsSLÂ https://ollama.com/install.shÂ | sh`\\n\\n- Download and run llama-3.2 model:\\n - `ollama run llama3.2`\\n\\n**Windows**\\n\\n- System Requirements:\\n - Windows 10 or Higher\\n - Supported AMD GPUs with driver installed\\n- For Windows installation you can simply download and install Ollama from here:\\n[https://ollama.com/download](https://ollama.com/download) Once installed, simply open PowerShell and run: - `Ollama run llama3.2`\\n\\nAs simple as that, you are ready to chat with your local LLM.\\n\\nYou can find the list of all available models from Ollama hereÂ [https://ollama.\",\n      \"image\": \"https://www.amd.com/content/dam/amd/en/images/blogs/designs/projects/technical-blogs/3234460-running-llms-locally-on-amd-gpus-with-ollama/3234460-running-llms-locally-on-amd-gpus-with-ollama.jpg\",\n      \"favicon\": \"https://www.amd.com/content/dam/code/images/favicon/favicon.ico\"\n    },\n    {\n      \"id\": \"https://medium.com/@namansharma_13002/7-fastest-open-source-llms-you-can-run-locally-in-2025-524be87c2064\",\n      \"title\": \"7 Fastest Open Source LLMs You Can Run Locally in 2025 - Medium\",\n      \"url\": \"https://medium.com/@namansharma_13002/7-fastest-open-source-llms-you-can-run-locally-in-2025-524be87c2064\",\n      \"publishedDate\": \"2025-07-07T08:11:10.000Z\",\n      \"author\": \"Naman Sharma\",\n      \"text\": \"[Sitemap](https://medium.com/sitemap/sitemap.xml)\\n\\n[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F524be87c2064&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&%7Estage=mobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40namansharma_13002%2F7-fastest-open-source-llms-you-can-run-locally-in-2025-524be87c2064&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\\n\\n[Search](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\\n\\nSign up\\n\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40namansharma_13002%2F7-fastest-open-source-llms-you-can-run-locally-in-2025-524be87c2064&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\\n\\n# **7 Fastest Open Source LLMs You Can Run Locally in 2025**\\n\\n[Naman Sharma](https://medium.com/@namansharma_13002?source=post_page---byline--524be87c2064---------------------------------------)\\n\\n12 min read\\n\\nÂ·\\n\\nJul 7, 2025\\n\\n--\\n\\nListen\\n\\nShare\\n\\nPress enter or click to view image in full size\\n\\nWith the ever-evolving generative AI, more developers and startups have found it necessary to **operate** [**large language models**](https://www.ibm.com/think/topics/large-language-models) **(LLMs) locally** rather than solely relying on cloud APIs such as OpenAIâ€™s GPT or Anthropicâ€™s Claude. For decisions like **data privacy, lower latency, or cost reduction in the long run**, it has been proven that in 2025, go\",\n      \"image\": \"https://miro.medium.com/v2/resize:fit:1200/1*UHaqqX6nBk6XHDkaTmkOcw.png\"\n    }\n  ],\n  \"requestTags\": {\n    \"flagsEnabledInPosthog\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exa21,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache\",\n    \"flagsEnabledInRequest\": \"\",\n    \"enabledFlags\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exa21,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache\",\n    \"initialSearchType\": \"auto\",\n    \"didIncludeText\": \"no\",\n    \"didExcludeText\": \"no\",\n    \"didIncludeDomains\": \"no\",\n    \"didExcludeDomains\": \"no\",\n    \"didIncludeUrls\": \"no\",\n    \"didExcludeUrls\": \"no\",\n    \"moderation\": \"no\",\n    \"didUseDateFilters\": \"no\",\n    \"category\": \"none\",\n    \"didUseAutoprompt\": \"no\",\n    \"hasUserLocation\": \"no\",\n    \"didRequestContentText\": true,\n    \"didRequestContentHighlights\": false,\n    \"didRequestContentSummary\": false,\n    \"livecrawl\": \"preferred\",\n    \"requestedLinks\": \"no\",\n    \"willPostRerankNeural\": \"yes\",\n    \"willPostModerate\": false,\n    \"searchTypeBranch\": \"auto\",\n    \"autoBranch\": {\n      \"neuralSearch\": {\n        \"sierra\": {\n          \"query\": \"ollama vLLM Mac M2 Ultra vs AMD GPU support compatibility 2025\",\n          \"flags\": \"experiment_auto_join,fallback_keyword_to_neural,fallback_neural_to_keyword,hybrid_for_auto,lang_filter,langfuse_search_logging,llm_gateway,markdown_contents,neosnippets,race_providers_for_query_cache,redirect_classifier_active,redirect_classifier_shadow,reranker_with_date,research_enabled,snippets_strategy:default,sranker_qwen_medium_trt,sranker_qwen_medium_trt_for_auto,sranker_qwen_medium_trt_for_fast,sranker_qwen_medium_trt_for_hybrid,type_hybrid_merge_chicken_index_at_35,use_dynamo_db_with_query_cache,use_exaperson_v1_latest,use_kronos_answer_active,use_kronos_answer_shadow,use_modal_embeddings,use_moderated_index,use_new_atlas_keyword,use_query_cache_sparse_for_fast,use_query_cache_sparse_for_hybrid,use_s3e_race,use_zilliz_for_query_cache,use_exa21\",\n          \"filters\": {\n            \"moderation\": \"blacklisted\",\n            \"moderationConfig\": {\n              \"llamaguardS1\": false,\n              \"llamaguardS3\": false,\n              \"llamaguardS4\": false,\n              \"llamaguardS12\": false,\n              \"domainBlacklisted\": false,\n              \"domainBlacklistedMedia\": false\n            }\n          },\n          \"queryDateString\": \"2025-11-06\",\n          \"numResultsRequested\": 35,\n          \"engine\": \"default\",\n          \"isTweetCategory\": false,\n          \"sierraTime\": 469.364636000013,\n          \"numResultsReturned\": 35\n        },\n        \"fetchContents\": {\n          \"contentExpansion\": {\n            \"failedContents\": []\n          },\n          \"contentsResultsCount\": 35,\n          \"filteredResultsCount\": 0\n        }\n      },\n      \"keywordSearch\": {\n        \"query\": \"ollama vLLM Mac M2 Ultra vs AMD GPU support compatibility 2025\",\n        \"modifiedQuery\": \"ollama vLLM Mac M2 Ultra vs AMD GPU support compatibility 2025\",\n        \"slicedAmount\": 0,\n        \"searchData\": {\n          \"q\": \"ollama vLLM Mac M2 Ultra vs AMD GPU support compatibility 2025\",\n          \"num\": 10,\n          \"safe\": \"active\"\n        },\n        \"opts\": {\n          \"snippetMode\": \"snippet\",\n          \"addAnswerBox\": true,\n          \"addKnowledgeGraph\": true,\n          \"addPeopleAlsoAsk\": false,\n          \"allowEmptyAnswerLinks\": false,\n          \"providers\": [\n            {\n              \"provider\": \"serper\",\n              \"startAfterMs\": 0,\n              \"timeoutMs\": 10000\n            }\n          ],\n          \"timeoutMs\": 10000,\n          \"teamId\": \"cm60snyw00011cfcxf2vbwp8n\"\n        },\n        \"serperSource\": \"regular\",\n        \"peopleAlsoAsk\": [\n          \"Will Ollama work with AMD GPU?\",\n          \"Which GPU does Ollama prefer?\",\n          \"What is the best model for llama 2025?\",\n          \"Why is Ollama slower than vLLM?\"\n        ],\n        \"relatedQueries\": [\n          \"Ollama GPU list\",\n          \"Ollama GPU requirements\",\n          \"Ollama GPU support list\",\n          \"Best GPU for Ollama\",\n          \"Ollama macOS AMD GPU\",\n          \"vLLM vs Ollama performance\",\n          \"Ollama Mac GPU support\",\n          \"Ollama GPU support windows\"\n        ],\n        \"success\": true,\n        \"numResultsRequested\": 5,\n        \"numResultsReturned\": 10,\n        \"fetchContents\": {\n          \"contentExpansion\": {\n            \"failedContents\": [\n              \"https://www.reddit.com/r/selfhosted/comments/1g0rpfp/what_gpu_do_i_need_to_run_ai_ollama/\"\n            ]\n          },\n          \"contentsResultsCount\": 9,\n          \"filteredResultsCount\": 0\n        }\n      },\n      \"neitherFailed\": true,\n      \"autoJoin\": true,\n      \"autoJoinRerank\": {\n        \"reranker\": {\n          \"withModeration\": false,\n          \"preserveUnranked\": true,\n          \"strategy\": {\n            \"strategy\": \"modal_sranker_qwen_trt\",\n            \"default\": true\n          },\n          \"strategy_decided\": \"modal_sranker_qwen_trt\",\n          \"strategy_reason\": \"explicit_strategy\",\n          \"isQdf\": false\n        }\n      },\n      \"autoJoinResults\": \"35\"\n    },\n    \"decidedAutoType\": \"neural\",\n    \"resolvedSearchType\": \"neural\",\n    \"autoType\": \"neural\",\n    \"decidedResultsLength\": 5,\n    \"totalFilterCount\": \"11\",\n    \"contextExpansionNeeded\": \"not-needed\",\n    \"finalResultsCount\": 5,\n    \"searchTime\": 4862.1,\n    \"path\": \"/search\",\n    \"method\": \"POST\",\n    \"teamId\": \"cm60snyw00011cfcxf2vbwp8n\"\n  },\n  \"effectiveFilters\": {\n    \"includeDomains\": [],\n    \"excludeDomains\": [],\n    \"includeUrls\": [],\n    \"excludeUrls\": [],\n    \"includeText\": [],\n    \"excludeText\": []\n  },\n  \"searchTime\": 4862.1,\n  \"costDollars\": {\n    \"total\": 0.01,\n    \"search\": {\n      \"neural\": 0.005\n    },\n    \"contents\": {\n      \"text\": 0.005\n    }\n  }\n}"}]}
{"parentUuid":"9bbd2f33-d7c2-49bd-980b-8982acabe982","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_018iBupg1oi48n5qiXYEZv2m","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"The user is asking about whether ROCm is viable for LLM inference, and comparing Mac Studio with 128GB vs AMD machine with 128GB. They want to run ollama or vLLM locally.\n\nLet me synthesize the search results to provide a clear answer:\n\nFrom the search results:\n\n1. **ROCm viability for production LLM inference (2025)**:\n   - vLLM has excellent ROCm support with AMD Instinct GPUs (MI300X, MI325X, MI350X, MI355X)\n   - vLLM V1 with ROCm delivers significant performance improvements\n   - AMD has optimized ROCm for LLM inference with features like AITER (AI Tensor Engine for ROCm)\n   - Performance benchmarks show impressive throughput (e.g., 13,212 tokens/s for Llama 3.1 70B FP8)\n   - Ollama supports AMD GPUs via ROCm on Linux and Windows\n\n2. **vLLM vs Ollama**:\n   - vLLM: High-performance, production-grade (793 TPS vs Ollama's 41 TPS at peak)\n   - Ollama: Developer-friendly, simpler setup, good for prototyping\n   - vLLM has lower P99 latency (80ms vs 673ms) at scale\n\n3. **Mac Studio support**:\n   - Ollama has excellent Mac support (Linux, Windows, macOS)\n   - Mac Studio with M2 Ultra and 128GB unified memory is well-supported\n   - Apple Silicon has good LLM performance with unified memory architecture\n\n4. **Key considerations**:\n   - ROCm is production-viable BUT requires AMD Instinct GPUs (MI300X series) for best results\n   - Consumer AMD GPUs (Radeon) have ROCm support but with limitations\n   - Mac Studio offers simpler setup and excellent Ollama support\n   \nI'll highlight the additional complexity of vLLM's ROCm implementation, which demands more technical expertise but compensates with superior performance metrics.","signature":"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"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":22544,"cache_read_input_tokens":47541,"cache_creation":{"ephemeral_5m_input_tokens":22544,"ephemeral_1h_input_tokens":0},"output_tokens":2,"service_tier":"standard"}},"requestId":"req_011CUrmjbSNJC48LZmZ3CzBY","type":"assistant","uuid":"48811a34-93f7-4036-bca2-5b7885ff5aff","timestamp":"2025-11-06T14:12:22.200Z"}
{"parentUuid":"48811a34-93f7-4036-bca2-5b7885ff5aff","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_018iBupg1oi48n5qiXYEZv2m","type":"message","role":"assistant","content":[{"type":"text","text":"Based on the current state of LLM inference in 2025, here's my assessment:\n\n## ROCm Viability: YES, but with caveats\n\n**ROCm is production-ready for LLM inference**, especially with:\n- **vLLM**: Fully optimized, excellent performance (13,212 tokens/s on Llama 3.1 70B FP8)\n- **AMD Instinct GPUs** (MI300X/MI325X/MI350X series): Top-tier support\n- **Consumer AMD GPUs** (Radeon): Supported but less optimized\n\n### Performance Comparison (vLLM benchmarks):\n- **vLLM**: 793 TPS peak throughput, 80ms P99 latency\n- **Ollama**: 41 TPS peak throughput, 673ms P99 latency\n\n## Mac Studio vs AMD: The Verdict\n\n### **Mac Studio 128GB (M2 Ultra)** âœ… RECOMMENDED\n**Pros:**\n- **Ollama works flawlessly** - native macOS support\n- Unified memory architecture (128GB accessible by GPU)\n- Zero configuration hassle\n- Lower power consumption\n- Reliable, consistent performance\n- Great for development and prototyping\n\n**Cons:**\n- vLLM doesn't support Mac (Metal backend limited)\n- Lower peak throughput than AMD Instinct GPUs\n\n### **AMD Machine 128GB + ROCm** âš ï¸ DEPENDS\n**Critical question: Which AMD GPU?**\n\n**If AMD Instinct MI300X series:**\n- **YES** - Production-grade, best raw performance\n- Full vLLM optimization\n- Complex setup but worth it for production\n\n**If Consumer Radeon GPU:**\n- **MAYBE** - ROCm support exists but:\n  - More driver issues\n  - Less optimization\n  - Configuration complexity\n  - Community support varies by model\n\n## My Recommendation\n\n**For your use case (local inference with 128GB):**\n\n### Choose **Mac Studio** if:\n- You want it to \"just work\"\n- Using Ollama (simpler, developer-friendly)\n- Development/research/personal use\n- Value reliability over peak performance\n\n### Choose **AMD + ROCm** if:\n- You have an **Instinct MI300X** (not consumer GPU)\n- Need maximum throughput for production\n- Willing to invest time in setup/optimization\n- Using vLLM for high-concurrency scenarios\n\n### âš ï¸ Important Notes:\n1. **Consumer AMD GPUs (RX 7900 XTX, etc.) with ROCm**: Technically works, but expect driver headaches and suboptimal performance. Not recommended unless you enjoy troubleshooting.\n\n2. **128GB RAM on AMD system**: Make sure you also have a high-end GPU (24GB+ VRAM) - the system RAM alone won't help much.\n\n3. **vLLM on Mac**: Not officially supported, use Ollama instead.\n\n## Bottom Line\n**Mac Studio 128GB is the safer, more practical choice for 99% of use cases.** AMD + ROCm only makes sense if you have enterprise-grade Instinct GPUs or specific production requirements that justify the complexity."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":8,"cache_creation_input_tokens":22544,"cache_read_input_tokens":47541,"cache_creation":{"ephemeral_5m_input_tokens":22544,"ephemeral_1h_input_tokens":0},"output_tokens":2,"service_tier":"standard"}},"requestId":"req_011CUrmjbSNJC48LZmZ3CzBY","type":"assistant","uuid":"b5af6106-0c44-455e-9763-5c4fda4e305b","timestamp":"2025-11-06T14:12:37.514Z"}
{"type":"file-history-snapshot","messageId":"49ec805e-9e19-4d65-a612-895a32fdb362","snapshot":{"messageId":"49ec805e-9e19-4d65-a612-895a32fdb362","trackedFileBackups":{},"timestamp":"2025-11-07T07:19:12.609Z"},"isSnapshotUpdate":false}
{"parentUuid":"b5af6106-0c44-455e-9763-5c4fda4e305b","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","type":"user","message":{"role":"user","content":"Caveat: The messages below were generated by the user while running local commands. DO NOT respond to these messages or otherwise consider them in your response unless the user explicitly asks you to."},"isMeta":true,"uuid":"c1badab7-f856-4393-96b1-154d3cf7fc7a","timestamp":"2025-11-07T07:19:12.594Z"}
{"parentUuid":"c1badab7-f856-4393-96b1-154d3cf7fc7a","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","type":"user","message":{"role":"user","content":"<command-name>/exit</command-name>\n            <command-message>exit</command-message>\n            <command-args></command-args>"},"uuid":"49ec805e-9e19-4d65-a612-895a32fdb362","timestamp":"2025-11-07T07:19:12.592Z"}
{"parentUuid":"49ec805e-9e19-4d65-a612-895a32fdb362","isSidechain":false,"userType":"external","cwd":"/Users/levi/data","sessionId":"a2be89f8-7f5d-4ab2-8d64-bd7a59c171f7","version":"2.0.32","gitBranch":"","type":"user","message":{"role":"user","content":"<local-command-stdout>Goodbye!</local-command-stdout>"},"uuid":"cde3f377-a7f1-4d86-8e25-e98b2c616b18","timestamp":"2025-11-07T07:19:12.592Z"}
